{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Vendor:  Continuum Analytics, Inc.\n",
      "Package: mkl\n",
      "Message: trial mode expires in 27 days\n",
      "Vendor:  Continuum Analytics, Inc.\n",
      "Package: mkl\n",
      "Message: trial mode expires in 27 days\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "debug = True # global var to control debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import theano\n",
    "\n",
    "import theano.tensor as TT\n",
    "\n",
    "from theano.tensor.shared_randomstreams import RandomStreams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# These are taken from https://github.com/mila-udem/blocks\n",
    "# \n",
    "\n",
    "class Constant():\n",
    "    \"\"\"Initialize parameters to a constant.\n",
    "    The constant may be a scalar or a :class:`~numpy.ndarray` of any shape\n",
    "    that is broadcastable with the requested parameter arrays.\n",
    "    Parameters\n",
    "    ----------\n",
    "    constant : :class:`~numpy.ndarray`\n",
    "        The initialization value to use. Must be a scalar or an ndarray (or\n",
    "        compatible object, such as a nested list) that has a shape that is\n",
    "        broadcastable with any shape requested by `initialize`.\n",
    "    \"\"\"\n",
    "    def __init__(self, constant):\n",
    "        self._constant = numpy.asarray(constant)\n",
    "\n",
    "    def generate(self, rng, shape):\n",
    "        dest = numpy.empty(shape, dtype=np.float32)\n",
    "        dest[...] = self._constant\n",
    "        return dest\n",
    "\n",
    "\n",
    "class IsotropicGaussian():\n",
    "    \"\"\"Initialize parameters from an isotropic Gaussian distribution.\n",
    "    Parameters\n",
    "    ----------\n",
    "    std : float, optional\n",
    "        The standard deviation of the Gaussian distribution. Defaults to 1.\n",
    "    mean : float, optional\n",
    "        The mean of the Gaussian distribution. Defaults to 0\n",
    "    Notes\n",
    "    -----\n",
    "    Be careful: the standard deviation goes first and the mean goes\n",
    "    second!\n",
    "    \"\"\"\n",
    "    def __init__(self, std=1, mean=0):\n",
    "        self._mean = mean\n",
    "        self._std = std\n",
    "\n",
    "    def generate(self, rng, shape):\n",
    "        m = rng.normal(self._mean, self._std, size=shape)\n",
    "        return m.astype(np.float32)\n",
    "\n",
    "\n",
    "class Uniform():\n",
    "    \"\"\"Initialize parameters from a uniform distribution.\n",
    "    Parameters\n",
    "    ----------\n",
    "    mean : float, optional\n",
    "        The mean of the uniform distribution (i.e. the center of mass for\n",
    "        the density function); Defaults to 0.\n",
    "    width : float, optional\n",
    "        One way of specifying the range of the uniform distribution. The\n",
    "        support will be [mean - width/2, mean + width/2]. **Exactly one**\n",
    "        of `width` or `std` must be specified.\n",
    "    std : float, optional\n",
    "        An alternative method of specifying the range of the uniform\n",
    "        distribution. Chooses the width of the uniform such that random\n",
    "        variates will have a desired standard deviation. **Exactly one** of\n",
    "        `width` or `std` must be specified.\n",
    "    \"\"\"\n",
    "    def __init__(self, mean=0., width=None, std=None):\n",
    "        if (width is not None) == (std is not None):\n",
    "            raise ValueError(\"must specify width or std, \"\n",
    "                             \"but not both\")\n",
    "        if std is not None:\n",
    "            # Variance of a uniform is 1/12 * width^2\n",
    "            self._width = numpy.sqrt(12) * std\n",
    "        else:\n",
    "            self._width = width\n",
    "        self._mean = mean\n",
    "\n",
    "    def generate(self, rng, shape):\n",
    "        w = self._width / 2\n",
    "        m = rng.uniform(self._mean - w, self._mean + w, size=shape)\n",
    "        return m.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def default(var, val):\n",
    "    if var is None:\n",
    "        return val\n",
    "    else:\n",
    "        return var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM implementation in Theano\n",
    "\n",
    "Theano fully supports recurrent neural networks. One typically needs only to provide an implementation of a single step of the recurrency.\n",
    "\n",
    "Please read about the scan function which is used to implement the loops: http://deeplearning.net/software/theano/library/scan.html.\n",
    "\n",
    "**Attention**: through the code we will assume that the 0-th axis refers to time and that the 1-st axis refers to individual examples inside a minibatch. (This way in a C-major memory layout individual time steps occupy contiguous regions in memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"\n",
    "    Taken from https://github.com/vitruvianscience/OpenDeep/blob/master/opendeep/utils/activation.py\n",
    "    \n",
    "    See the Theano documentation.\n",
    "    Returns the row-wise softmax function of x.\n",
    "    In the case of 3D input, it returns the scan of softmax applied over the second two dimensions\n",
    "    (loops over first dimension).\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : 2D or 3D tensor\n",
    "        Symbolic 2D or 3D Tensor (or compatible).\n",
    "    Returns\n",
    "    -------\n",
    "    2D or 3D tensor\n",
    "        Row-wise softmax: softmax_{ij}(x) = exp(x_{ij})/sum_k(exp(x_{ik})) applied to `x`. Returns same shape as input.\n",
    "    \"\"\"\n",
    "    if x.ndim == 3:\n",
    "        cost, _ = theano.scan(fn=TT.nnet.softmax, sequences=x)\n",
    "        return cost\n",
    "    return TT.nnet.softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Layer(object):\n",
    "    def __init__(self, rng=None):\n",
    "        if rng is None:\n",
    "            rng = numpy.random\n",
    "        self.rng = rng\n",
    "        self._parameters = []\n",
    "    \n",
    "    @property\n",
    "    def parameters(self):\n",
    "        return self._parameters\n",
    "    \n",
    "    def add_param(self, name, shape, initializer, dtype='float32'):\n",
    "        param = theano.shared(numpy.zeros(\n",
    "            shape, dtype=dtype), name=name)\n",
    "        param.tag.initializer = initializer\n",
    "        self._parameters.append(param)\n",
    "        setattr(self, name, param)\n",
    "        \n",
    "    def initialize(self):\n",
    "        for p in self.parameters:\n",
    "            p.set_value(p.tag.initializer.generate(self.rng, \n",
    "                                                   p.get_value().shape))\n",
    "\n",
    "\n",
    "class RecurrentLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(RecurrentLayer, self).__init__(**kwargs)\n",
    "        self.initial_states = []\n",
    "    \n",
    "    def apply(self, X, **kwargs):\n",
    "        batch_size = X.shape[1]\n",
    "        outputs_info = []\n",
    "        for h in self.initial_states:\n",
    "            h0 = TT.repeat(h, batch_size, axis=0)\n",
    "            outputs_info.append(dict(initial=h0))\n",
    "        \n",
    "        #\n",
    "        # Scan in theano takes a function which performs a single step of the\n",
    "        # recurrent computation. Subclasses just need to provide the\n",
    "        # self.transition function.\n",
    "        #\n",
    "        scan_result, scan_updates = theano.scan(\n",
    "            self.transition,\n",
    "            sequences=X,\n",
    "            outputs_info=outputs_info,\n",
    "            **kwargs\n",
    "            )\n",
    "        # Note: this in general will not be the case and we will need to\n",
    "        # make sure that the updates are given to theano.function\n",
    "        assert not scan_updates\n",
    "        return scan_result\n",
    "\n",
    "\n",
    "class MergeInputHiddens(Layer):\n",
    "    \"\"\"\n",
    "    Merge two sequences - inputs and hidden states and produce an output.\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, in_dim, hidden_dim, out_dim,\n",
    "                 weight_init=None, bias_init=None, \n",
    "                 **kwargs):\n",
    "        super(MergeInputHiddens, self).__init__(**kwargs)\n",
    "        weight_init = default(\n",
    "            weight_init, IsotropicGaussian(1.0/sqrt(in_dim)))\n",
    "        bias_init = default(\n",
    "            weight_init, Constant(0.))\n",
    "        \n",
    "        # Input to output\n",
    "        self.add_param('Wxo', (in_dim, out_dim), \n",
    "                       weight_init)\n",
    "        \n",
    "        # Hidden to output\n",
    "        self.add_param('Who', (hidden_dim, out_dim), \n",
    "                       weight_init)\n",
    "        \n",
    "        # Output bias\n",
    "        self.add_param('Bo', (out_dim,), \n",
    "                       bias_init)\n",
    "        \n",
    "    def apply(self, X, H):\n",
    "        # Get the shape\n",
    "        nsteps, bs, nin = X.shape\n",
    "        nhid = H.shape[2]\n",
    "        \n",
    "        # Note - we flatten the steps and batch size\n",
    "        # as the computation of outputs can be performed in \n",
    "        # parallel for all time steps.\n",
    "        \n",
    "        O = (X.reshape((nsteps*bs, nin)).dot(self.Wxo) + \n",
    "             H.reshape((nsteps*bs, nhid)).dot(self.Who) +\n",
    "             self.Bo)\n",
    "        return O.reshape((nsteps, bs, O.shape[1]))\n",
    "    \n",
    "class Chain(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Chain, self).__init__(**kwargs)\n",
    "        self.children = []\n",
    "        \n",
    "    @property\n",
    "    def parameters(self):\n",
    "        ret = list(self._parameters)\n",
    "        for c in self.children:\n",
    "            ret.extend(c.parameters)\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Trainer(object):\n",
    "    \"\"\"\n",
    "    This class computes the updates to parameters using the RMSProp learning rule and adding weight decay.\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, net_loss, parameters, inputs):\n",
    "        self.RMSProp_dec_rate = \\\n",
    "            theano.shared(np.array(0.9, dtype='float32'))\n",
    "        self.RMSProp_epsilon = \\\n",
    "            theano.shared(np.array(1e-5, dtype='float32'))\n",
    "        self.lrate = \\\n",
    "            theano.shared(np.array(1e-2, dtype='float32'))\n",
    "        self.max_grad_norm = \\\n",
    "            theano.shared(np.array(1., dtype='float32'))\n",
    "        self.wdec = \\\n",
    "            theano.shared(np.array(0., dtype='float32'))\n",
    "\n",
    "        theano.config.compute_test_value='off' # Turn off for gradient computation\n",
    "        \n",
    "        wdec_loss = 0\n",
    "        for p in parameters:\n",
    "            if p.name[0]=='W':\n",
    "                wdec_loss = wdec_loss + (p**2).sum()*self.wdec\n",
    "        \n",
    "        grads = theano.grad(net_loss + wdec_loss, parameters)\n",
    "        updates = []\n",
    "\n",
    "        grad_norm = 0.\n",
    "\n",
    "        for g in grads:\n",
    "            grad_norm = grad_norm + (g**2).sum()\n",
    "        \n",
    "\n",
    "        for g,p in zip(grads, parameters):\n",
    "            step = g\n",
    "            step = g / TT.maximum(1.0, grad_norm/self.max_grad_norm)\n",
    "            if 1:\n",
    "                g2 = theano.shared(p.get_value() * 0.,\n",
    "                                   name=p.name + '_g2')\n",
    "                g2_new = (self.RMSProp_dec_rate * g2 + \n",
    "                          (1.0 - self.RMSProp_dec_rate) * g**2)\n",
    "                updates.append((g2, g2_new))\n",
    "                step = step / TT.sqrt(g2_new + self.RMSProp_epsilon)\n",
    "\n",
    "            step = self.lrate * step\n",
    "            updates.append((p, p - step))\n",
    "\n",
    "        self.train_function = theano.function(inputs, \n",
    "                                              [net_loss, net_loss + wdec_loss, grad_norm], \n",
    "                                              updates=updates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LSTM_RNN(RecurrentLayer):\n",
    "    \"\"\"\n",
    "    Implementation follows Alex Graves, Abdel-rahman Mohamed and Geoffrey Hinton\n",
    "    \"SPEECH RECOGNITION WITH DEEP RECURRENT NEURAL NETWORKS\"\n",
    "    http://www.cs.toronto.edu/~fritz/absps/RNN13.pdf\n",
    "    \"\"\"\n",
    "    def __init__(self, in_dim, hidden_dim,\n",
    "                 hidden_activation=TT.tanh,\n",
    "                 rec_weight_init=None,\n",
    "                 weight_init=None, bias_init=None, \n",
    "                 forget_bias_init=None, \n",
    "                 **kwargs):\n",
    "        super(LSTM_RNN, self).__init__(**kwargs)\n",
    "        rec_weight_init = default(\n",
    "            rec_weight_init, IsotropicGaussian(1.0/sqrt(hidden_dim)))\n",
    "        weight_init = default(\n",
    "            weight_init, IsotropicGaussian(1.0/sqrt(in_dim)))\n",
    "        bias_init = default(\n",
    "            weight_init, Constant(0.))\n",
    "        forget_bias_init = default(\n",
    "            weight_init, Constant(1.))\n",
    "        self.hidden_activation = hidden_activation\n",
    "        \n",
    "        #\n",
    "        # Gates\n",
    "        #\n",
    "        \n",
    "        for gate in 'ifo':\n",
    "            self.add_param('Wx' + gate, (in_dim, hidden_dim), \n",
    "                           weight_init)\n",
    "            self.add_param('Wh' + gate, (hidden_dim, hidden_dim), \n",
    "                           weight_init)\n",
    "            # Note: a cell is only connected to its own gates\n",
    "            # Wc... are diagonal - so we allocate only a vector\n",
    "            # for them\n",
    "            self.add_param('Wc' + gate, (hidden_dim,), \n",
    "                           weight_init)\n",
    "            self.add_param('B' + gate, (hidden_dim,), \n",
    "                           bias_init)\n",
    "        \n",
    "        #\n",
    "        # Note - forget gate bias has a different initializer, because\n",
    "        # we often want to initialize it to 1\n",
    "        #\n",
    "        self.Bf.tag.initializer = forget_bias_init\n",
    "        \n",
    "        # Cell\n",
    "        self.add_param('Wxc', (in_dim, hidden_dim), \n",
    "                       weight_init)\n",
    "        self.add_param('Whc', (hidden_dim, hidden_dim), \n",
    "                       weight_init)\n",
    "        self.add_param('Bc', (hidden_dim,), \n",
    "                       bias_init)\n",
    "        \n",
    "        # Initial states\n",
    "        self.add_param('h0', (1, hidden_dim), \n",
    "                       bias_init)\n",
    "        self.initial_states.append(self.h0)\n",
    "        self.add_param('c0', (1, hidden_dim), \n",
    "                       bias_init)\n",
    "        self.initial_states.append(self.c0)\n",
    "        \n",
    "        \n",
    "    def transition(self, x, h, c):\n",
    "        \"\"\"\n",
    "        One step of LSTM transition.\n",
    "        \n",
    "        x is the previous input\n",
    "        h is the previous hidden state\n",
    "        c is the previous memory cell content\n",
    "        \"\"\"\n",
    "        \n",
    "        #\n",
    "        # Please note:\n",
    "        # The implementation below is not speed-optimal\n",
    "        # usually, it pays off to group similar matrix multiplications\n",
    "        # by grouping gates.\n",
    "        #\n",
    "        # Also, input-related computations should be moved out of scan since\n",
    "        # they can be done for all steps in parallel.\n",
    "        #\n",
    "        \n",
    "        \n",
    "        s = TT.nnet.sigmoid\n",
    "\n",
    "        # Note: for cells we do element0wise multiplication which \n",
    "        # is equvalent to a matrix multiplication with a diagonal matrix!\n",
    "        i = s(x.dot(self.Wxi) + h.dot(self.Whi) + c*self.Wci + self.Bi)\n",
    "        f = s(x.dot(self.Wxf) + h.dot(self.Whf) + c*self.Wcf + self.Bf)\n",
    "        \n",
    "        c_new = f*c + i*self.hidden_activation(x.dot(self.Wxc) + h.dot(self.Whc) + self.Bc)\n",
    "        o = s(x.dot(self.Wxo) + h.dot(self.Who) + c_new*self.Wco + self.Bo)\n",
    "        h_new = o * self.hidden_activation(c_new)\n",
    "        \n",
    "        return h_new, c_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# we encode be input and output as a 1-of-N vector, thus every element of the input and output is a 1x10 vector with one '1'.\n",
    "\n",
    "def class_encoding(n, num_classes = 10):\n",
    "    ret = zeros(shape=(num_classes))\n",
    "    ret[n] = 1\n",
    "    return ret\n",
    "\n",
    "def class_decoding(v):\n",
    "    return argmax(v)\n",
    "\n",
    "# This should be rewritten without the use of loops\n",
    "\n",
    "def decode_matrix(matrix):\n",
    "    decoded_matrix = zeros(shape = (matrix.shape[0], matrix.shape[1]))\n",
    "    for i in range(matrix.shape[0]):\n",
    "        for j in range(matrix.shape[1]):\n",
    "            decoded_matrix[i][j] = class_decoding(matrix[i][j])\n",
    "    return decoded_matrix\n",
    "\n",
    "def encode_matrix(matrix, num_classes = 10):\n",
    "    encoded_matrix = zeros(shape = (matrix.shape[0], matrix.shape[1], num_classes))\n",
    "    for i in range(matrix.shape[0]):\n",
    "        for j in range(matrix.shape[1]):\n",
    "            encoded_matrix[i][j] = class_encoding(matrix[i][j])\n",
    "    return encoded_matrix.astype(\"float32\")\n",
    "\n",
    "\n",
    "def gen_copy_example(T, seq_len, batchsize):\n",
    "    rng = numpy.random\n",
    "    \n",
    "    sequence = rng.randint(2, 10, size=(seq_len, batchsize))\n",
    "    \n",
    "    X = np.concatenate((sequence,\n",
    "                         np.zeros(shape=(T - 1, batchsize)),\n",
    "                         np.ones(shape=(1, batchsize)), \n",
    "                         np.zeros(shape=(seq_len, batchsize))))\n",
    "    \n",
    "    Y = np.concatenate((np.zeros(shape=(T + seq_len, batchsize)),\n",
    "                        sequence.reshape(seq_len, batchsize)))\n",
    "    \n",
    "    \n",
    "    return encode_matrix(X), encode_matrix(Y)\n",
    "\n",
    "\n",
    "Xc, Yc = gen_copy_example(4, 4, 2)\n",
    "#print Xc.shape, Yc.shape\n",
    "#print 'X:', Xc[:,0,:], '\\nY:', Yc[:,0,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At each point in time, network receives an input of size 1x10 (1-hot encoding), propagets the signal through a layer of\n",
    "hidden_dim LSTM neurons and produces an output of size 1x10 (also treated as 1-hot encoding). Softmax is applied at\n",
    "the end to get calculated probabilities. Cross-entropy is used as a loss function.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class CopyNet(Chain):\n",
    "    def __init__(self, hidden_dim, num_layers=1,\n",
    "                 **kwargs):\n",
    "        super(CopyNet, self).__init__(**kwargs)\n",
    "        self.recs = []\n",
    "        \n",
    "        for hid in xrange(num_layers):\n",
    "            if hid == 0:\n",
    "                in_dim = 10\n",
    "            else:\n",
    "                in_dim = hidden_dim\n",
    "                \n",
    "            rec = LSTM_RNN(in_dim=in_dim, hidden_dim=hidden_dim)\n",
    "            \n",
    "            self.recs.append(rec)\n",
    "            self.children.append(rec)\n",
    "        \n",
    "        self.merge = MergeInputHiddens(\n",
    "            in_dim=10, hidden_dim=hidden_dim,\n",
    "            out_dim=10\n",
    "            )\n",
    "        self.children.append(self.merge)\n",
    "        \n",
    "        self.X = TT.tensor3('X')\n",
    "        self.Y = TT.tensor3('Y')\n",
    "        \n",
    "        self.inputs = [self.X, self.Y]\n",
    "    \n",
    "    def apply(self, X):\n",
    "        H = X\n",
    "        for rec in self.recs:\n",
    "            H = rec.apply(H)\n",
    "            H = H[0] # we don't use cell contents\n",
    "        O = self.merge.apply(X, H)\n",
    "        \n",
    "        return softmax(O)\n",
    "    \n",
    "    # We use cross-entropy to measure the efficiency of the network. You can also a\n",
    "    def get_loss(self):\n",
    "        copy_net_output = self.apply(self.X)\n",
    "        # return ((copy_net_output - self.Y)**2).mean() # MSE\n",
    "        return -(self.Y * TT.log(copy_net_output)).mean() # cross entropy\n",
    "    \n",
    "    def get_output(self):\n",
    "        copy_net_output = self.apply(self.X)\n",
    "        return copy_net_output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24, 1, 10) (24, 1, 10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/pio/os/anaconda/lib/python2.7/site-packages/theano/scan_module/scan_perform_ext.py:133: RuntimeWarning: numpy.ndarray size changed, may indicate binary incompatibility\n",
      "  from scan_perform.scan_perform import *\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(0.25090810656547546, dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xc, Yc = gen_copy_example(4, 10, 1)\n",
    "print Xc.shape, Yc.shape\n",
    "theano.config.compute_test_value='off'\n",
    "theano.config.print_test_value=True\n",
    "debug = False\n",
    "\n",
    "# Set the nubmer of hidden cells here. Note that intuitively you need more cells if you want to copy longer sequences. \n",
    "# The ratio 4 neurons per max seqence lenght seems to be fine.\n",
    "copy_net = CopyNet(hidden_dim=20)\n",
    "copy_net.initialize()\n",
    "copy_net_loss = copy_net.get_loss()\n",
    "copy_net_output = copy_net.get_output()\n",
    "\n",
    "copy_test_function = theano.function(copy_net.inputs, \n",
    "                                       copy_net_loss)\n",
    "copy_check_output = theano.function([copy_net.X], \n",
    "                                       copy_net_output)\n",
    "\n",
    "copy_test_function(Xc, Yc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "copy_trainer = Trainer(copy_net_loss, copy_net.parameters, copy_net.inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem we are solving is described in the following paper, section 5.1: http://arxiv.org/pdf/1511.06464v2.pdf.\n",
    "The input is a seqeunce of size 2 * n + T and each element of the input is of one of the 10 classes, which will name by the numbers from 0 to 9. First n elements is an arbitrary sequence of numbers 2...9. Then, T - 1 '0's follow. The next element is a single '1' and the last n elements are '0's. The desired output is a sequence of the same length, where the first T + n elemtnts are '0's followed by sequence from the beginnig of the input. The kth element of the output should be produced after the net sees the kth element of the input.\n",
    "\n",
    "The difficulty of the task is the fact that a network has to put elements in the memory, store them for a long time and then reproduce them. \n",
    "\n",
    "In the cited paper authors used n = 10 (length of the sequence to reproduce) and 40 neurons. Their network was only a little bit better than baseline (a program that outputs T + n '0's and n random numbers) when T = 100 and did not learn at all for larger T.\n",
    "\n",
    "It tunrs out that this task is in fact easy with the use of curriculum. My network was able to learn to perfectly reproduce the sequence with the time lag of 400.\n",
    "\n",
    "The curriculum is used both for the n (seq_len) as well as T (lag). In every consecutive batch the lag is of random size. As the newtork get better, we increase both the maximium allowed lag, as well as the sequnece lenght. After the training, the net should be able to copy a sequence of any length over any time lag not greater than the learnt ones. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [array(0.26855704188346863, dtype=float32), array(0.26883089542388916, dtype=float32), array(0.019072508439421654, dtype=float32)]\n",
      "0.258574187756\n",
      "[[ 5.]\n",
      " [ 2.]]\n",
      "2000 [array(0.003358281683176756, dtype=float32), array(0.003752228105440736, dtype=float32), array(2.151214903278742e-05, dtype=float32)]\n",
      "0.00338579667732\n",
      "[[ 3.]\n",
      " [ 3.]]\n",
      "2043 Increasing lag to:  5 seq_len is  1\n",
      "2060 Increasing lag to:  6 seq_len is  1\n",
      "2236 Increasing lag to:  7 seq_len is  1\n",
      "2331 Increasing lag to:  8 seq_len is  1\n",
      "2451 Increasing lag to:  9 seq_len is  1\n",
      "2517 Increasing lag to:  10 seq_len is  2\n",
      "4000 [array(0.0008926283335313201, dtype=float32), array(0.0013324925675988197, dtype=float32), array(4.871470446232706e-05, dtype=float32)]\n",
      "0.0140950428322\n",
      "[[ 5.  4.]\n",
      " [ 5.  5.]]\n",
      "6000 [array(0.0008799178758636117, dtype=float32), array(0.0013573311734944582, dtype=float32), array(1.2832471838919446e-05, dtype=float32)]\n",
      "0.0123029416427\n",
      "[[ 9.  4.]\n",
      " [ 9.  9.]]\n",
      "8000 [array(0.010725733824074268, dtype=float32), array(0.011244920082390308, dtype=float32), array(0.0008732179412618279, dtype=float32)]\n",
      "0.00924018118531\n",
      "[[ 4.  7.]\n",
      " [ 4.  7.]]\n",
      "10000 [array(0.006065590772777796, dtype=float32), array(0.006625293754041195, dtype=float32), array(0.003871331922709942, dtype=float32)]\n",
      "0.00602537952363\n",
      "[[ 9.  4.]\n",
      " [ 9.  4.]]\n",
      "11183 Increasing lag to:  11 seq_len is  2\n",
      "11317 Increasing lag to:  12 seq_len is  2\n",
      "11814 Increasing lag to:  13 seq_len is  2\n",
      "12000 [array(0.0048907664604485035, dtype=float32), array(0.005487803835421801, dtype=float32), array(0.004619830287992954, dtype=float32)]\n",
      "0.00604574661702\n",
      "[[ 7.  2.]\n",
      " [ 2.  2.]]\n",
      "12042 Increasing lag to:  14 seq_len is  2\n",
      "12115 Increasing lag to:  15 seq_len is  2\n",
      "12451 Increasing lag to:  16 seq_len is  2\n",
      "12540 Increasing lag to:  17 seq_len is  2\n",
      "12598 Increasing lag to:  18 seq_len is  2\n",
      "12663 Increasing lag to:  19 seq_len is  2\n",
      "12689 Increasing lag to:  20 seq_len is  3\n",
      "14000 [array(0.0001342402829322964, dtype=float32), array(0.0007456273306161165, dtype=float32), array(1.7301154002780095e-05, dtype=float32)]\n",
      "0.0126600014046\n",
      "[[ 6.  5.  2.]\n",
      " [ 6.  5.  5.]]\n",
      "16000 [array(0.002675545634701848, dtype=float32), array(0.0033061576541513205, dtype=float32), array(8.109772898023948e-05, dtype=float32)]\n",
      "0.0118157546967\n",
      "[[ 2.  7.  6.]\n",
      " [ 2.  7.  7.]]\n",
      "18000 [array(0.012050161138176918, dtype=float32), array(0.012701107189059258, dtype=float32), array(0.005657623056322336, dtype=float32)]\n",
      "0.00990619696677\n",
      "[[ 7.  2.  4.]\n",
      " [ 7.  2.  7.]]\n",
      "20000 [array(0.011834274046123028, dtype=float32), array(0.01250513456761837, dtype=float32), array(0.0020550827030092478, dtype=float32)]\n",
      "0.0104062659666\n",
      "[[ 3.  3.  9.]\n",
      " [ 3.  3.  9.]]\n",
      "22000 [array(0.0008311202982440591, dtype=float32), array(0.0015211115824058652, dtype=float32), array(3.9636750443605706e-05, dtype=float32)]\n",
      "0.00723117869347\n",
      "[[ 7.  5.  2.]\n",
      " [ 7.  5.  5.]]\n",
      "24000 [array(0.0008726064115762711, dtype=float32), array(0.0015804265858605504, dtype=float32), array(0.0001321852469118312, dtype=float32)]\n",
      "0.00700797140598\n",
      "[[ 6.  9.  6.]\n",
      " [ 6.  9.  9.]]\n",
      "26000 [array(0.01019175536930561, dtype=float32), array(0.010915788821876049, dtype=float32), array(0.0006292293546721339, dtype=float32)]\n",
      "0.00747559126467\n",
      "[[ 5.  6.  8.]\n",
      " [ 5.  6.  8.]]\n",
      "28000 [array(0.006514755543321371, dtype=float32), array(0.0072535499930381775, dtype=float32), array(0.0009409033227711916, dtype=float32)]\n",
      "0.00638688262552\n",
      "[[ 3.  9.  8.]\n",
      " [ 3.  9.  9.]]\n",
      "30000 [array(0.005110571626573801, dtype=float32), array(0.005864730104804039, dtype=float32), array(0.0003891587257385254, dtype=float32)]\n",
      "0.00595774780959\n",
      "[[ 9.  3.  3.]\n",
      " [ 9.  3.  3.]]\n",
      "32000 [array(0.008054332807660103, dtype=float32), array(0.008822733536362648, dtype=float32), array(0.0022554579190909863, dtype=float32)]\n",
      "0.0067065032199\n",
      "[[ 4.  4.  8.]\n",
      " [ 4.  4.  4.]]\n",
      "34000 [array(3.1026836950331926e-05, dtype=float32), array(0.0008142184815369546, dtype=float32), array(2.3505233315290752e-08, dtype=float32)]\n",
      "0.00454328209162\n",
      "[[ 6.  4.  3.]\n",
      " [ 6.  4.  3.]]\n",
      "34244 Increasing lag to:  21 seq_len is  3\n",
      "35820 Increasing lag to:  22 seq_len is  3\n",
      "36000 [array(0.004330561030656099, dtype=float32), array(0.0051261261105537415, dtype=float32), array(0.00179935316555202, dtype=float32)]\n",
      "0.00452583935112\n",
      "[[ 2.  6.  8.]\n",
      " [ 2.  2.  8.]]\n",
      "36223 Increasing lag to:  23 seq_len is  3\n",
      "36992 Increasing lag to:  24 seq_len is  3\n",
      "37285 Increasing lag to:  25 seq_len is  3\n",
      "37867 Increasing lag to:  26 seq_len is  3\n",
      "37956 Increasing lag to:  27 seq_len is  3\n",
      "38000 [array(0.0005671027465723455, dtype=float32), array(0.001373814302496612, dtype=float32), array(1.2315874300838914e-05, dtype=float32)]\n",
      "0.00251786154695\n",
      "[[ 8.  3.  5.]\n",
      " [ 8.  3.  5.]]\n",
      "38039 Increasing lag to:  28 seq_len is  3\n",
      "38075 Increasing lag to:  29 seq_len is  3\n",
      "38661 Increasing lag to:  30 seq_len is  4\n",
      "40000 [array(0.010456716641783714, dtype=float32), array(0.011267635971307755, dtype=float32), array(0.0016966006951406598, dtype=float32)]\n",
      "0.00955140031874\n",
      "[[ 7.  8.  7.  4.]\n",
      " [ 7.  7.  7.  3.]]\n",
      "42000 [array(2.0524234059848823e-05, dtype=float32), array(0.000837934494484216, dtype=float32), array(3.324825215145211e-08, dtype=float32)]\n",
      "0.00821412354708\n",
      "[[ 4.  5.  5.  9.]\n",
      " [ 4.  5.  9.  2.]]\n",
      "44000 [array(0.00028801843291148543, dtype=float32), array(0.0011140895076096058, dtype=float32), array(3.7998827338014962e-06, dtype=float32)]\n",
      "0.00860440544784\n",
      "[[ 7.  2.  6.  5.]\n",
      " [ 6.  6.  6.  6.]]\n",
      "46000 [array(6.5129846916534e-05, dtype=float32), array(0.0008991801296360791, dtype=float32), array(1.224023407075947e-07, dtype=float32)]\n",
      "0.00842381175607\n",
      "[[ 6.  8.  9.  8.]\n",
      " [ 6.  8.  9.  9.]]\n",
      "48000 [array(0.018726471811532974, dtype=float32), array(0.01956993155181408, dtype=float32), array(0.005815517622977495, dtype=float32)]\n",
      "0.009466053918\n",
      "[[ 7.  4.  2.  4.]\n",
      " [ 7.  4.  4.  4.]]\n",
      "50000 [array(2.4915934773162007e-05, dtype=float32), array(0.0008766012033447623, dtype=float32), array(2.2631571283682206e-08, dtype=float32)]\n",
      "0.0074449358508\n",
      "[[ 7.  5.  7.  5.]\n",
      " [ 7.  5.  7.  7.]]\n",
      "52000 [array(0.007997779175639153, dtype=float32), array(0.008857824839651585, dtype=float32), array(0.004030036274343729, dtype=float32)]\n",
      "0.00686931889504\n",
      "[[ 4.  5.  6.  9.]\n",
      " [ 4.  5.  6.  9.]]\n",
      "54000 [array(0.00022846378851681948, dtype=float32), array(0.0010973657481372356, dtype=float32), array(6.773567292839289e-06, dtype=float32)]\n",
      "0.00860529951751\n",
      "[[ 6.  5.  6.  5.]\n",
      " [ 6.  5.  6.  6.]]\n",
      "56000 [array(0.016320813447237015, dtype=float32), array(0.01719772256910801, dtype=float32), array(0.0023702809121459723, dtype=float32)]\n",
      "0.00685756886378\n",
      "[[ 8.  3.  4.  6.]\n",
      " [ 4.  3.  4.  6.]]\n",
      "58000 [array(0.007651269435882568, dtype=float32), array(0.008536497130990028, dtype=float32), array(0.0165280532091856, dtype=float32)]\n",
      "0.00840116664767\n",
      "[[ 5.  3.  5.  5.]\n",
      " [ 5.  3.  3.  3.]]\n",
      "60000 [array(0.00834597647190094, dtype=float32), array(0.009238479658961296, dtype=float32), array(0.0016096218023449183, dtype=float32)]\n",
      "0.00729162013158\n",
      "[[ 3.  5.  9.  5.]\n",
      " [ 3.  5.  9.  2.]]\n",
      "62000 [array(0.003978088963776827, dtype=float32), array(0.004878561478108168, dtype=float32), array(0.00037179450737312436, dtype=float32)]\n",
      "0.00584345357493\n",
      "[[ 9.  6.  3.  8.]\n",
      " [ 9.  6.  3.  3.]]\n",
      "64000 [array(0.0019338600104674697, dtype=float32), array(0.002841227687895298, dtype=float32), array(5.270175461191684e-05, dtype=float32)]\n",
      "0.00684004742652\n",
      "[[ 9.  8.  9.  4.]\n",
      " [ 9.  8.  9.  4.]]\n",
      "66000 [array(7.12467372068204e-05, dtype=float32), array(0.0009869181085377932, dtype=float32), array(2.0599532035703305e-07, dtype=float32)]\n",
      "0.00504539581016\n",
      "[[ 8.  4.  6.  6.]\n",
      " [ 8.  4.  6.  6.]]\n",
      "68000 [array(0.0004383549967315048, dtype=float32), array(0.0013605259591713548, dtype=float32), array(1.1592793271120172e-05, dtype=float32)]\n",
      "0.00544577371329\n",
      "[[ 4.  8.  5.  5.]\n",
      " [ 4.  8.  5.  5.]]\n",
      "70000 [array(0.007133690174669027, dtype=float32), array(0.008062336593866348, dtype=float32), array(0.0015190079575404525, dtype=float32)]\n",
      "0.00472475867718\n",
      "[[ 9.  5.  2.  9.]\n",
      " [ 9.  5.  2.  9.]]\n",
      "72000 [array(0.005365111865103245, dtype=float32), array(0.0063009969890117645, dtype=float32), array(0.004494634922593832, dtype=float32)]\n",
      "0.00520607409999\n",
      "[[ 7.  7.  6.  2.]\n",
      " [ 7.  7.  2.  6.]]\n",
      "74000 [array(0.0001883159129647538, dtype=float32), array(0.0011307831155136228, dtype=float32), array(3.6929136513208505e-06, dtype=float32)]\n",
      "0.00445811683312\n",
      "[[ 8.  2.  7.  6.]\n",
      " [ 8.  2.  7.  6.]]\n",
      "76000 [array(0.007543710060417652, dtype=float32), array(0.008492745459079742, dtype=float32), array(0.0008135163225233555, dtype=float32)]\n",
      "0.00571092357859\n",
      "[[ 7.  2.  7.  4.]\n",
      " [ 7.  2.  7.  7.]]\n",
      "78000 [array(0.0018437424441799521, dtype=float32), array(0.00279934611171484, dtype=float32), array(8.161244477378204e-05, dtype=float32)]\n",
      "0.0044650468044\n",
      "[[ 8.  7.  5.  5.]\n",
      " [ 8.  7.  5.  4.]]\n",
      "80000 [array(0.0006056667189113796, dtype=float32), array(0.0015669534914195538, dtype=float32), array(1.4008023754286114e-05, dtype=float32)]\n",
      "0.0044381567277\n",
      "[[ 5.  9.  4.  6.]\n",
      " [ 5.  9.  9.  6.]]\n",
      "81551 Increasing lag to:  31 seq_len is  4\n",
      "82000 [array(0.00036947656190022826, dtype=float32), array(0.0013364808401092887, dtype=float32), array(3.4633596897037933e-06, dtype=float32)]\n",
      "0.00397089030594\n",
      "[[ 4.  4.  2.  8.]\n",
      " [ 4.  4.  2.  8.]]\n",
      "82842 Increasing lag to:  32 seq_len is  4\n",
      "84000 [array(0.006899117026478052, dtype=float32), array(0.007872386835515499, dtype=float32), array(0.004083126783370972, dtype=float32)]\n",
      "0.00471298117191\n",
      "[[ 7.  4.  2.  9.]\n",
      " [ 7.  2.  2.  9.]]\n",
      "84512 Increasing lag to:  33 seq_len is  4\n",
      "85128 Increasing lag to:  34 seq_len is  4\n",
      "85577 Increasing lag to:  35 seq_len is  4\n",
      "85819 Increasing lag to:  36 seq_len is  4\n",
      "86000 [array(0.00045317289186641574, dtype=float32), array(0.0014316081069409847, dtype=float32), array(7.943083801364992e-06, dtype=float32)]\n",
      "0.00441683549434\n",
      "[[ 9.  3.  8.  6.]\n",
      " [ 9.  3.  9.  6.]]\n",
      "86112 Increasing lag to:  37 seq_len is  4\n",
      "86284 Increasing lag to:  38 seq_len is  4\n",
      "86532 Increasing lag to:  39 seq_len is  4\n",
      "87128 Increasing lag to:  40 seq_len is  5\n",
      "88000 [array(0.0013082834193482995, dtype=float32), array(0.002287816721946001, dtype=float32), array(0.0002585628826636821, dtype=float32)]\n",
      "0.00712023908272\n",
      "[[ 4.  3.  3.  7.  6.]\n",
      " [ 4.  3.  3.  3.  6.]]\n",
      "90000 [array(0.004212460480630398, dtype=float32), array(0.005193361081182957, dtype=float32), array(0.0004191004845779389, dtype=float32)]\n",
      "0.0078350789845\n",
      "[[ 8.  9.  8.  8.  3.]\n",
      " [ 8.  9.  8.  8.  8.]]\n",
      "92000 [array(0.00014893918705638498, dtype=float32), array(0.0011356992181390524, dtype=float32), array(6.491738986369455e-06, dtype=float32)]\n",
      "0.00832099281251\n",
      "[[ 6.  9.  4.  6.  5.]\n",
      " [ 6.  9.  7.  7.  5.]]\n",
      "94000 [array(0.00013698471593670547, dtype=float32), array(0.0011289629619568586, dtype=float32), array(1.113270172936609e-06, dtype=float32)]\n",
      "0.00807333365083\n",
      "[[ 3.  2.  3.  9.  9.]\n",
      " [ 3.  2.  9.  3.  9.]]\n",
      "96000 [array(1.813108792703133e-05, dtype=float32), array(0.001015778398141265, dtype=float32), array(1.797591231422757e-08, dtype=float32)]\n",
      "0.0064879222773\n",
      "[[ 5.  3.  4.  4.  9.]\n",
      " [ 5.  3.  4.  4.  4.]]\n",
      "98000 [array(0.007607548031955957, dtype=float32), array(0.008610205724835396, dtype=float32), array(0.000776460045017302, dtype=float32)]\n",
      "0.006442040205\n",
      "[[ 3.  5.  3.  9.  8.]\n",
      " [ 3.  5.  3.  9.  9.]]\n",
      "100000 [array(0.0025282665155828, dtype=float32), array(0.003536039497703314, dtype=float32), array(0.00018655270105227828, dtype=float32)]\n",
      "0.00576427672058\n",
      "[[ 6.  6.  3.  7.  9.]\n",
      " [ 6.  6.  3.  7.  7.]]\n",
      "102000 [array(0.0001126216011471115, dtype=float32), array(0.0011259907623752952, dtype=float32), array(2.53694133789395e-06, dtype=float32)]\n",
      "0.00633251294494\n",
      "[[ 4.  3.  4.  3.  2.]\n",
      " [ 4.  3.  4.  4.  2.]]\n",
      "104000 [array(0.001133658573962748, dtype=float32), array(0.002152875065803528, dtype=float32), array(4.140346572967246e-05, dtype=float32)]\n",
      "0.00708878040314\n",
      "[[ 5.  5.  8.  8.  2.]\n",
      " [ 5.  5.  8.  8.  8.]]\n",
      "106000 [array(0.007006995379924774, dtype=float32), array(0.0080318134278059, dtype=float32), array(0.0005279332399368286, dtype=float32)]\n",
      "0.00572004634887\n",
      "[[ 4.  9.  6.  3.  9.]\n",
      " [ 4.  9.  6.  3.  3.]]\n",
      "108000 [array(0.0009418816189281642, dtype=float32), array(0.0019716424867510796, dtype=float32), array(0.00013039697660133243, dtype=float32)]\n",
      "0.00552354939282\n",
      "[[ 6.  7.  5.  5.  3.]\n",
      " [ 6.  7.  5.  5.  3.]]\n",
      "110000 [array(0.0013159632217139006, dtype=float32), array(0.0023509752936661243, dtype=float32), array(6.29550195299089e-05, dtype=float32)]\n",
      "0.00499475561082\n",
      "[[ 4.  5.  9.  7.  6.]\n",
      " [ 4.  5.  9.  7.  6.]]\n",
      "112000 [array(0.0001413137069903314, dtype=float32), array(0.0011809214483946562, dtype=float32), array(1.7854624729807256e-06, dtype=float32)]\n",
      "0.00421290379018\n",
      "[[ 4.  2.  3.  2.  4.]\n",
      " [ 4.  2.  3.  4.  4.]]\n",
      "114000 [array(0.00240091304294765, dtype=float32), array(0.003445669775828719, dtype=float32), array(0.00019957753829658031, dtype=float32)]\n",
      "0.00616960320622\n",
      "[[ 9.  6.  8.  3.  2.]\n",
      " [ 9.  6.  8.  3.  9.]]\n",
      "116000 [array(5.6648452300578356e-05, dtype=float32), array(0.0011067376472055912, dtype=float32), array(5.358519956644159e-07, dtype=float32)]\n",
      "0.00505960732698\n",
      "[[ 3.  2.  9.  4.  8.]\n",
      " [ 3.  2.  9.  2.  2.]]\n",
      "118000 [array(0.01769474521279335, dtype=float32), array(0.018748672679066658, dtype=float32), array(0.004283882211893797, dtype=float32)]\n",
      "0.00721731549129\n",
      "[[ 4.  5.  9.  6.  3.]\n",
      " [ 4.  5.  3.  6.  6.]]\n",
      "120000 [array(0.002630706178024411, dtype=float32), array(0.003689228091388941, dtype=float32), array(0.00017133985238615423, dtype=float32)]\n",
      "0.00472250347957\n",
      "[[ 8.  5.  4.  9.  9.]\n",
      " [ 8.  5.  9.  9.  9.]]\n",
      "122000 [array(6.958509766263887e-05, dtype=float32), array(0.0011319757904857397, dtype=float32), array(1.5478315162908984e-07, dtype=float32)]\n",
      "0.00452026771381\n",
      "[[ 5.  6.  7.  5.  4.]\n",
      " [ 5.  6.  7.  7.  4.]]\n",
      "124000 [array(0.00440898397937417, dtype=float32), array(0.00547486636787653, dtype=float32), array(0.00016856401634868234, dtype=float32)]\n",
      "0.00469521945342\n",
      "[[ 8.  2.  3.  6.  9.]\n",
      " [ 8.  2.  3.  2.  9.]]\n",
      "124006 Increasing lag to:  41 seq_len is  5\n",
      "126000 [array(0.00832501333206892, dtype=float32), array(0.009393995627760887, dtype=float32), array(0.0016164990374818444, dtype=float32)]\n",
      "0.00519815180451\n",
      "[[ 3.  9.  4.  5.  3.]\n",
      " [ 3.  9.  3.  5.  4.]]\n",
      "128000 [array(0.0025313557125627995, dtype=float32), array(0.003604119410738349, dtype=float32), array(0.00017290218966081738, dtype=float32)]\n",
      "0.00371651048772\n",
      "[[ 2.  7.  8.  8.  7.]\n",
      " [ 2.  8.  8.  8.  8.]]\n",
      "130000 [array(1.354407868348062e-05, dtype=float32), array(0.0010888391407206655, dtype=float32), array(1.3825308009529635e-08, dtype=float32)]\n",
      "0.00497469911352\n",
      "[[ 9.  4.  4.  2.  5.]\n",
      " [ 9.  4.  4.  2.  5.]]\n",
      "132000 [array(0.00019182729010935873, dtype=float32), array(0.0012708271387964487, dtype=float32), array(6.452637080656132e-06, dtype=float32)]\n",
      "0.00492285750806\n",
      "[[ 7.  6.  2.  7.  4.]\n",
      " [ 7.  6.  7.  4.  4.]]\n",
      "134000 [array(0.00011744166113203391, dtype=float32), array(0.0011990313651040196, dtype=float32), array(1.991795443245792e-06, dtype=float32)]\n",
      "0.00493108481169\n",
      "[[ 7.  3.  7.  9.  2.]\n",
      " [ 7.  3.  9.  9.  2.]]\n",
      "136000 [array(0.013014143332839012, dtype=float32), array(0.014098295010626316, dtype=float32), array(0.0033745048567652702, dtype=float32)]\n",
      "0.00568453129381\n",
      "[[ 5.  9.  8.  4.  8.]\n",
      " [ 5.  9.  8.  8.  8.]]\n",
      "138000 [array(7.100895891198888e-05, dtype=float32), array(0.0011580209247767925, dtype=float32), array(9.87483417702606e-07, dtype=float32)]\n",
      "0.00589425861835\n",
      "[[ 7.  8.  7.  3.  9.]\n",
      " [ 7.  8.  8.  8.  8.]]\n",
      "140000 [array(3.0894843803253025e-05, dtype=float32), array(0.0011200465960428119, dtype=float32), array(4.2445982728622766e-08, dtype=float32)]\n",
      "0.00429626740515\n",
      "[[ 7.  4.  6.  2.  7.]\n",
      " [ 7.  4.  6.  7.  6.]]\n",
      "142000 [array(0.002086901105940342, dtype=float32), array(0.003178463550284505, dtype=float32), array(9.123892232310027e-05, dtype=float32)]\n",
      "0.00385794043541\n",
      "[[ 2.  6.  3.  4.  6.]\n",
      " [ 2.  6.  3.  4.  4.]]\n",
      "144000 [array(0.00011054785863962024, dtype=float32), array(0.0012035593390464783, dtype=float32), array(8.638743906885793e-07, dtype=float32)]\n",
      "0.00462109176442\n",
      "[[ 6.  3.  9.  2.  4.]\n",
      " [ 6.  3.  9.  9.  2.]]\n",
      "146000 [array(0.0007173905614763498, dtype=float32), array(0.0018125047208741307, dtype=float32), array(2.3109783796826378e-05, dtype=float32)]\n",
      "0.00354905729182\n",
      "[[ 2.  6.  5.  4.  5.]\n",
      " [ 2.  6.  5.  4.  4.]]\n",
      "148000 [array(1.3541031876229681e-05, dtype=float32), array(0.0011103646829724312, dtype=float32), array(1.9594356359675658e-08, dtype=float32)]\n",
      "0.00469180382788\n",
      "[[ 3.  4.  3.  2.  4.]\n",
      " [ 3.  4.  3.  2.  4.]]\n",
      "150000 [array(0.004530065227299929, dtype=float32), array(0.005628582090139389, dtype=float32), array(0.0003965493815485388, dtype=float32)]\n",
      "0.00469826115295\n",
      "[[ 7.  5.  6.  6.  2.]\n",
      " [ 7.  5.  6.  6.  6.]]\n",
      "152000 [array(0.0013353859540075064, dtype=float32), array(0.002435084665194154, dtype=float32), array(0.00015337580407503992, dtype=float32)]\n",
      "0.00449994020164\n",
      "[[ 2.  5.  6.  3.  5.]\n",
      " [ 2.  5.  5.  3.  5.]]\n",
      "154000 [array(0.00042643825872801244, dtype=float32), array(0.0015278728678822517, dtype=float32), array(1.828336826292798e-05, dtype=float32)]\n",
      "0.00404688809067\n",
      "[[ 5.  6.  2.  3.  4.]\n",
      " [ 5.  6.  4.  3.  9.]]\n",
      "156000 [array(0.00012463281746022403, dtype=float32), array(0.0012268262216821313, dtype=float32), array(1.3114828334437334e-06, dtype=float32)]\n",
      "0.00339347822592\n",
      "[[ 4.  4.  3.  2.  3.]\n",
      " [ 4.  4.  3.  2.  3.]]\n",
      "158000 [array(2.1692529116990045e-05, dtype=float32), array(0.0011251996038481593, dtype=float32), array(9.434120329387952e-08, dtype=float32)]\n",
      "0.00362577103078\n",
      "[[ 8.  8.  7.  5.  4.]\n",
      " [ 8.  8.  7.  5.  5.]]\n",
      "160000 [array(0.00308420741930604, dtype=float32), array(0.004189276602119207, dtype=float32), array(0.000508825876750052, dtype=float32)]\n",
      "0.00401736889035\n",
      "[[ 7.  4.  9.  9.  3.]\n",
      " [ 7.  4.  9.  9.  9.]]\n",
      "161019 Increasing lag to:  42 seq_len is  5\n",
      "162000 [array(0.0003063400508835912, dtype=float32), array(0.0014119549887254834, dtype=float32), array(7.417414053634275e-06, dtype=float32)]\n",
      "0.00420669326559\n",
      "[[ 7.  3.  8.  4.  6.]\n",
      " [ 7.  3.  8.  8.  6.]]\n",
      "164000 [array(2.4053735614870675e-05, dtype=float32), array(0.0011307436507195234, dtype=float32), array(3.243584600909344e-08, dtype=float32)]\n",
      "0.0042054974474\n",
      "[[ 2.  2.  7.  2.  8.]\n",
      " [ 2.  2.  2.  7.  8.]]\n",
      "165429 Increasing lag to:  43 seq_len is  5\n",
      "166000 [array(0.00043368543265387416, dtype=float32), array(0.0015414443332701921, dtype=float32), array(6.439511707867496e-06, dtype=float32)]\n",
      "0.00376763823442\n",
      "[[ 5.  9.  7.  5.  7.]\n",
      " [ 5.  9.  7.  7.  7.]]\n",
      "166766 Increasing lag to:  44 seq_len is  5\n",
      "168000 [array(0.00014399916108231992, dtype=float32), array(0.0012531197862699628, dtype=float32), array(2.001831262532505e-06, dtype=float32)]\n",
      "0.00316193001345\n",
      "[[ 4.  6.  4.  6.  7.]\n",
      " [ 4.  6.  4.  6.  7.]]\n",
      "168417 Increasing lag to:  45 seq_len is  5\n",
      "168986 Increasing lag to:  46 seq_len is  5\n",
      "169853 Increasing lag to:  47 seq_len is  5\n",
      "170000 [array(4.845451258006506e-05, dtype=float32), array(0.0011580026475712657, dtype=float32), array(1.161303998742369e-06, dtype=float32)]\n",
      "0.00304327672347\n",
      "[[ 8.  9.  7.  8.  9.]\n",
      " [ 8.  9.  7.  8.  8.]]\n",
      "170666 Increasing lag to:  48 seq_len is  5\n",
      "172000 [array(0.006991463713347912, dtype=float32), array(0.00810091383755207, dtype=float32), array(0.0014200606383383274, dtype=float32)]\n",
      "0.00387867959216\n",
      "[[ 3.  5.  5.  3.  5.]\n",
      " [ 3.  5.  5.  3.  3.]]\n",
      "172136 Increasing lag to:  49 seq_len is  5\n",
      "172729 Increasing lag to:  50 seq_len is  5\n",
      "173135 Increasing lag to:  51 seq_len is  5\n",
      "173195 Increasing lag to:  52 seq_len is  5\n",
      "173612 Increasing lag to:  53 seq_len is  5\n",
      "173706 Increasing lag to:  54 seq_len is  5\n",
      "173855 Increasing lag to:  55 seq_len is  5\n",
      "173990 Increasing lag to:  56 seq_len is  5\n",
      "174000 [array(2.450085048621986e-05, dtype=float32), array(0.0011342121288180351, dtype=float32), array(1.5347846726854186e-07, dtype=float32)]\n",
      "0.00391621654853\n",
      "[[ 6.  7.  2.  8.  3.]\n",
      " [ 6.  7.  8.  8.  9.]]\n",
      "174091 Increasing lag to:  57 seq_len is  5\n",
      "174191 Increasing lag to:  58 seq_len is  5\n",
      "174240 Increasing lag to:  59 seq_len is  5\n",
      "174258 Increasing lag to:  60 seq_len is  5\n",
      "174482 Increasing lag to:  61 seq_len is  5\n",
      "174535 Increasing lag to:  62 seq_len is  5\n",
      "174639 Increasing lag to:  63 seq_len is  5\n",
      "174692 Increasing lag to:  64 seq_len is  5\n",
      "174870 Increasing lag to:  65 seq_len is  5\n",
      "174944 Increasing lag to:  66 seq_len is  5\n",
      "175152 Increasing lag to:  67 seq_len is  5\n",
      "175159 Increasing lag to:  68 seq_len is  5\n",
      "175210 Increasing lag to:  69 seq_len is  5\n",
      "175338 Increasing lag to:  70 seq_len is  5\n",
      "175396 Increasing lag to:  71 seq_len is  5\n",
      "175419 Increasing lag to:  72 seq_len is  5\n",
      "175425 Increasing lag to:  73 seq_len is  5\n",
      "175443 Increasing lag to:  74 seq_len is  5\n",
      "175662 Increasing lag to:  75 seq_len is  5\n",
      "175707 Increasing lag to:  76 seq_len is  5\n",
      "175745 Increasing lag to:  77 seq_len is  5\n",
      "175803 Increasing lag to:  78 seq_len is  5\n",
      "175810 Increasing lag to:  79 seq_len is  5\n",
      "175881 Increasing lag to:  80 seq_len is  5\n",
      "175932 Increasing lag to:  81 seq_len is  5\n",
      "175969 Increasing lag to:  82 seq_len is  5\n",
      "176000 [array(6.021792796673253e-05, dtype=float32), array(0.0011696366127580404, dtype=float32), array(2.894047099744057e-07, dtype=float32)]\n",
      "0.00269525265321\n",
      "[[ 5.  9.  6.  4.  8.]\n",
      " [ 5.  9.  6.  7.  5.]]\n",
      "176011 Increasing lag to:  83 seq_len is  5\n",
      "176038 Increasing lag to:  84 seq_len is  5\n",
      "176077 Increasing lag to:  85 seq_len is  5\n",
      "176104 Increasing lag to:  86 seq_len is  5\n",
      "176122 Increasing lag to:  87 seq_len is  5\n",
      "176145 Increasing lag to:  88 seq_len is  5\n",
      "176199 Increasing lag to:  89 seq_len is  5\n",
      "176213 Increasing lag to:  90 seq_len is  5\n",
      "176310 Increasing lag to:  91 seq_len is  5\n",
      "176405 Increasing lag to:  92 seq_len is  5\n",
      "176440 Increasing lag to:  93 seq_len is  5\n",
      "176551 Increasing lag to:  94 seq_len is  5\n",
      "176589 Increasing lag to:  95 seq_len is  5\n",
      "176654 Increasing lag to:  96 seq_len is  5\n",
      "176713 Increasing lag to:  97 seq_len is  5\n",
      "176787 Increasing lag to:  98 seq_len is  5\n",
      "176846 Increasing lag to:  99 seq_len is  5\n",
      "176912 Increasing lag to:  100 seq_len is  5\n",
      "176924 Increasing lag to:  101 seq_len is  5\n",
      "176930 Increasing lag to:  102 seq_len is  5\n",
      "176945 Increasing lag to:  103 seq_len is  5\n",
      "176985 Increasing lag to:  104 seq_len is  5\n",
      "176994 Increasing lag to:  105 seq_len is  5\n",
      "177020 Increasing lag to:  106 seq_len is  5\n",
      "177047 Increasing lag to:  107 seq_len is  5\n",
      "177075 Increasing lag to:  108 seq_len is  5\n",
      "177238 Increasing lag to:  109 seq_len is  5\n",
      "177261 Increasing lag to:  110 seq_len is  5\n",
      "177335 Increasing lag to:  111 seq_len is  5\n",
      "177360 Increasing lag to:  112 seq_len is  5\n",
      "177504 Increasing lag to:  113 seq_len is  5\n",
      "177540 Increasing lag to:  114 seq_len is  5\n",
      "177564 Increasing lag to:  115 seq_len is  5\n",
      "177595 Increasing lag to:  116 seq_len is  5\n",
      "177597 Increasing lag to:  117 seq_len is  5\n",
      "177635 Increasing lag to:  118 seq_len is  5\n",
      "177645 Increasing lag to:  119 seq_len is  5\n",
      "177655 Increasing lag to:  120 seq_len is  5\n",
      "177671 Increasing lag to:  121 seq_len is  5\n",
      "177732 Increasing lag to:  122 seq_len is  5\n",
      "177823 Increasing lag to:  123 seq_len is  5\n",
      "177844 Increasing lag to:  124 seq_len is  5\n",
      "177858 Increasing lag to:  125 seq_len is  5\n",
      "177870 Increasing lag to:  126 seq_len is  5\n",
      "177954 Increasing lag to:  127 seq_len is  5\n",
      "177999 Increasing lag to:  128 seq_len is  5\n",
      "178000 [array(0.00023194067762233317, dtype=float32), array(0.001339535927399993, dtype=float32), array(7.475144684576662e-06, dtype=float32)]\n",
      "0.00186745112296\n",
      "[[ 5.  3.  6.  5.  2.]\n",
      " [ 5.  3.  6.  8.  6.]]\n",
      "178001 Increasing lag to:  129 seq_len is  5\n",
      "178004 Increasing lag to:  130 seq_len is  5\n",
      "178014 Increasing lag to:  131 seq_len is  5\n",
      "178033 Increasing lag to:  132 seq_len is  5\n",
      "178129 Increasing lag to:  133 seq_len is  5\n",
      "178148 Increasing lag to:  134 seq_len is  5\n",
      "178156 Increasing lag to:  135 seq_len is  5\n",
      "178247 Increasing lag to:  136 seq_len is  5\n",
      "178250 Increasing lag to:  137 seq_len is  5\n",
      "178278 Increasing lag to:  138 seq_len is  5\n",
      "178305 Increasing lag to:  139 seq_len is  5\n",
      "178388 Increasing lag to:  140 seq_len is  5\n",
      "178417 Increasing lag to:  141 seq_len is  5\n",
      "178512 Increasing lag to:  142 seq_len is  5\n",
      "178599 Increasing lag to:  143 seq_len is  5\n",
      "178688 Increasing lag to:  144 seq_len is  5\n",
      "178725 Increasing lag to:  145 seq_len is  5\n",
      "178735 Increasing lag to:  146 seq_len is  5\n",
      "178738 Increasing lag to:  147 seq_len is  5\n",
      "178769 Increasing lag to:  148 seq_len is  5\n",
      "178794 Increasing lag to:  149 seq_len is  5\n",
      "178795 Increasing lag to:  150 seq_len is  5\n",
      "178838 Increasing lag to:  151 seq_len is  5\n",
      "178850 Increasing lag to:  152 seq_len is  5\n",
      "178894 Increasing lag to:  153 seq_len is  5\n",
      "178911 Increasing lag to:  154 seq_len is  5\n",
      "178952 Increasing lag to:  155 seq_len is  5\n",
      "178957 Increasing lag to:  156 seq_len is  5\n",
      "178990 Increasing lag to:  157 seq_len is  5\n",
      "179116 Increasing lag to:  158 seq_len is  5\n",
      "179130 Increasing lag to:  159 seq_len is  5\n",
      "179180 Increasing lag to:  160 seq_len is  5\n",
      "179224 Increasing lag to:  161 seq_len is  5\n",
      "179432 Increasing lag to:  162 seq_len is  5\n",
      "179509 Increasing lag to:  163 seq_len is  5\n",
      "179555 Increasing lag to:  164 seq_len is  5\n",
      "179561 Increasing lag to:  165 seq_len is  5\n",
      "179573 Increasing lag to:  166 seq_len is  5\n",
      "179712 Increasing lag to:  167 seq_len is  5\n",
      "179778 Increasing lag to:  168 seq_len is  5\n",
      "179829 Increasing lag to:  169 seq_len is  5\n",
      "179880 Increasing lag to:  170 seq_len is  5\n",
      "179970 Increasing lag to:  171 seq_len is  5\n",
      "179995 Increasing lag to:  172 seq_len is  5\n",
      "179996 Increasing lag to:  173 seq_len is  5\n",
      "180000 [array(0.0006040200823917985, dtype=float32), array(0.001708036637865007, dtype=float32), array(4.3782274588011205e-05, dtype=float32)]\n",
      "0.00123805110343\n",
      "[[ 5.  2.  7.  3.  7.]\n",
      " [ 5.  2.  7.  3.  7.]]\n",
      "180110 Increasing lag to:  174 seq_len is  5\n",
      "180129 Increasing lag to:  175 seq_len is  5\n",
      "180150 Increasing lag to:  176 seq_len is  5\n",
      "180192 Increasing lag to:  177 seq_len is  5\n",
      "180223 Increasing lag to:  178 seq_len is  5\n",
      "180240 Increasing lag to:  179 seq_len is  5\n",
      "180263 Increasing lag to:  180 seq_len is  5\n",
      "180326 Increasing lag to:  181 seq_len is  5\n",
      "180341 Increasing lag to:  182 seq_len is  5\n",
      "180383 Increasing lag to:  183 seq_len is  5\n",
      "180391 Increasing lag to:  184 seq_len is  5\n",
      "180405 Increasing lag to:  185 seq_len is  5\n",
      "180421 Increasing lag to:  186 seq_len is  5\n",
      "180453 Increasing lag to:  187 seq_len is  5\n",
      "180541 Increasing lag to:  188 seq_len is  5\n",
      "180582 Increasing lag to:  189 seq_len is  5\n",
      "180591 Increasing lag to:  190 seq_len is  5\n",
      "180594 Increasing lag to:  191 seq_len is  5\n",
      "180596 Increasing lag to:  192 seq_len is  5\n",
      "180693 Increasing lag to:  193 seq_len is  5\n",
      "180776 Increasing lag to:  194 seq_len is  5\n",
      "180795 Increasing lag to:  195 seq_len is  5\n",
      "180800 Increasing lag to:  196 seq_len is  5\n",
      "180810 Increasing lag to:  197 seq_len is  5\n",
      "180896 Increasing lag to:  198 seq_len is  5\n",
      "180929 Increasing lag to:  199 seq_len is  5\n",
      "181036 Increasing lag to:  200 seq_len is  5\n",
      "181101 Increasing lag to:  201 seq_len is  5\n",
      "181130 Increasing lag to:  202 seq_len is  5\n",
      "181186 Increasing lag to:  203 seq_len is  5\n",
      "181213 Increasing lag to:  204 seq_len is  5\n",
      "181273 Increasing lag to:  205 seq_len is  5\n",
      "181307 Increasing lag to:  206 seq_len is  5\n",
      "181384 Increasing lag to:  207 seq_len is  5\n",
      "181516 Increasing lag to:  208 seq_len is  5\n",
      "181518 Increasing lag to:  209 seq_len is  5\n",
      "181533 Increasing lag to:  210 seq_len is  5\n",
      "181535 Increasing lag to:  211 seq_len is  5\n",
      "181565 Increasing lag to:  212 seq_len is  5\n",
      "181577 Increasing lag to:  213 seq_len is  5\n",
      "181594 Increasing lag to:  214 seq_len is  5\n",
      "181641 Increasing lag to:  215 seq_len is  5\n",
      "181740 Increasing lag to:  216 seq_len is  5\n",
      "181758 Increasing lag to:  217 seq_len is  5\n",
      "181821 Increasing lag to:  218 seq_len is  5\n",
      "181822 Increasing lag to:  219 seq_len is  5\n",
      "181862 Increasing lag to:  220 seq_len is  5\n",
      "181975 Increasing lag to:  221 seq_len is  5\n",
      "181999 Increasing lag to:  222 seq_len is  5\n",
      "182000 [array(8.838781468512025e-06, dtype=float32), array(0.0011087916791439056, dtype=float32), array(1.7871375490585706e-08, dtype=float32)]\n",
      "0.00127539830282\n",
      "[[ 3.  4.  8.  6.  6.]\n",
      " [ 3.  4.  8.  8.  6.]]\n",
      "182092 Increasing lag to:  223 seq_len is  5\n",
      "182107 Increasing lag to:  224 seq_len is  5\n",
      "182190 Increasing lag to:  225 seq_len is  5\n",
      "182218 Increasing lag to:  226 seq_len is  5\n",
      "182219 Increasing lag to:  227 seq_len is  5\n",
      "182361 Increasing lag to:  228 seq_len is  5\n",
      "182364 Increasing lag to:  229 seq_len is  5\n",
      "182391 Increasing lag to:  230 seq_len is  5\n",
      "182394 Increasing lag to:  231 seq_len is  5\n",
      "182487 Increasing lag to:  232 seq_len is  5\n",
      "182571 Increasing lag to:  233 seq_len is  5\n",
      "182589 Increasing lag to:  234 seq_len is  5\n",
      "182600 Increasing lag to:  235 seq_len is  5\n",
      "182607 Increasing lag to:  236 seq_len is  5\n",
      "182613 Increasing lag to:  237 seq_len is  5\n",
      "182652 Increasing lag to:  238 seq_len is  5\n",
      "182681 Increasing lag to:  239 seq_len is  5\n",
      "182758 Increasing lag to:  240 seq_len is  5\n",
      "182825 Increasing lag to:  241 seq_len is  5\n",
      "182836 Increasing lag to:  242 seq_len is  5\n",
      "183041 Increasing lag to:  243 seq_len is  5\n",
      "183083 Increasing lag to:  244 seq_len is  5\n",
      "183143 Increasing lag to:  245 seq_len is  5\n",
      "183202 Increasing lag to:  246 seq_len is  5\n",
      "183214 Increasing lag to:  247 seq_len is  5\n",
      "183342 Increasing lag to:  248 seq_len is  5\n",
      "183400 Increasing lag to:  249 seq_len is  5\n",
      "183460 Increasing lag to:  250 seq_len is  5\n",
      "183546 Increasing lag to:  251 seq_len is  5\n",
      "183605 Increasing lag to:  252 seq_len is  5\n",
      "183686 Increasing lag to:  253 seq_len is  5\n",
      "183825 Increasing lag to:  254 seq_len is  5\n",
      "183928 Increasing lag to:  255 seq_len is  5\n",
      "183962 Increasing lag to:  256 seq_len is  5\n",
      "184000 [array(0.0021464540623128414, dtype=float32), array(0.0032409336417913437, dtype=float32), array(0.00015707920829299837, dtype=float32)]\n",
      "0.000994323287159\n",
      "[[ 2.  3.  2.  4.  4.]\n",
      " [ 2.  3.  4.  4.  4.]]\n",
      "184062 Increasing lag to:  257 seq_len is  5\n",
      "184327 Increasing lag to:  258 seq_len is  5\n",
      "184451 Increasing lag to:  259 seq_len is  5\n",
      "184468 Increasing lag to:  260 seq_len is  5\n",
      "184583 Increasing lag to:  261 seq_len is  5\n",
      "184597 Increasing lag to:  262 seq_len is  5\n",
      "184619 Increasing lag to:  263 seq_len is  5\n",
      "184663 Increasing lag to:  264 seq_len is  5\n",
      "184711 Increasing lag to:  265 seq_len is  5\n",
      "184719 Increasing lag to:  266 seq_len is  5\n",
      "184816 Increasing lag to:  267 seq_len is  5\n",
      "184821 Increasing lag to:  268 seq_len is  5\n",
      "184825 Increasing lag to:  269 seq_len is  5\n",
      "184907 Increasing lag to:  270 seq_len is  5\n",
      "184966 Increasing lag to:  271 seq_len is  5\n",
      "184967 Increasing lag to:  272 seq_len is  5\n",
      "185040 Increasing lag to:  273 seq_len is  5\n",
      "185116 Increasing lag to:  274 seq_len is  5\n",
      "185120 Increasing lag to:  275 seq_len is  5\n",
      "185196 Increasing lag to:  276 seq_len is  5\n",
      "185252 Increasing lag to:  277 seq_len is  5\n",
      "185263 Increasing lag to:  278 seq_len is  5\n",
      "185343 Increasing lag to:  279 seq_len is  5\n",
      "185344 Increasing lag to:  280 seq_len is  5\n",
      "185414 Increasing lag to:  281 seq_len is  5\n",
      "185558 Increasing lag to:  282 seq_len is  5\n",
      "185684 Increasing lag to:  283 seq_len is  5\n",
      "185708 Increasing lag to:  284 seq_len is  5\n",
      "185744 Increasing lag to:  285 seq_len is  5\n",
      "185786 Increasing lag to:  286 seq_len is  5\n",
      "185790 Increasing lag to:  287 seq_len is  5\n",
      "185825 Increasing lag to:  288 seq_len is  5\n",
      "185837 Increasing lag to:  289 seq_len is  5\n",
      "185848 Increasing lag to:  290 seq_len is  5\n",
      "185852 Increasing lag to:  291 seq_len is  5\n",
      "185862 Increasing lag to:  292 seq_len is  5\n",
      "185929 Increasing lag to:  293 seq_len is  5\n",
      "185983 Increasing lag to:  294 seq_len is  5\n",
      "185987 Increasing lag to:  295 seq_len is  5\n",
      "186000 [array(0.00011635172268142924, dtype=float32), array(0.0012046571355313063, dtype=float32), array(1.804098815227917e-06, dtype=float32)]\n",
      "0.000800140958745\n",
      "[[ 4.  9.  9.  5.  9.]\n",
      " [ 4.  9.  9.  5.  9.]]\n",
      "186017 Increasing lag to:  296 seq_len is  5\n",
      "186041 Increasing lag to:  297 seq_len is  5\n",
      "186046 Increasing lag to:  298 seq_len is  5\n",
      "186048 Increasing lag to:  299 seq_len is  5\n",
      "186051 Increasing lag to:  300 seq_len is  5\n",
      "186080 Increasing lag to:  301 seq_len is  5\n",
      "186095 Increasing lag to:  302 seq_len is  5\n",
      "186181 Increasing lag to:  303 seq_len is  5\n",
      "186229 Increasing lag to:  304 seq_len is  5\n",
      "186267 Increasing lag to:  305 seq_len is  5\n",
      "186494 Increasing lag to:  306 seq_len is  5\n",
      "186525 Increasing lag to:  307 seq_len is  5\n",
      "186533 Increasing lag to:  308 seq_len is  5\n",
      "186546 Increasing lag to:  309 seq_len is  5\n",
      "186565 Increasing lag to:  310 seq_len is  5\n",
      "186622 Increasing lag to:  311 seq_len is  5\n",
      "186630 Increasing lag to:  312 seq_len is  5\n",
      "186723 Increasing lag to:  313 seq_len is  5\n",
      "186729 Increasing lag to:  314 seq_len is  5\n",
      "186744 Increasing lag to:  315 seq_len is  5\n",
      "186843 Increasing lag to:  316 seq_len is  5\n",
      "186942 Increasing lag to:  317 seq_len is  5\n",
      "186947 Increasing lag to:  318 seq_len is  5\n",
      "186997 Increasing lag to:  319 seq_len is  5\n",
      "187012 Increasing lag to:  320 seq_len is  5\n",
      "187092 Increasing lag to:  321 seq_len is  5\n",
      "187222 Increasing lag to:  322 seq_len is  5\n",
      "187269 Increasing lag to:  323 seq_len is  5\n",
      "187270 Increasing lag to:  324 seq_len is  5\n",
      "187330 Increasing lag to:  325 seq_len is  5\n",
      "187433 Increasing lag to:  326 seq_len is  5\n",
      "187502 Increasing lag to:  327 seq_len is  5\n",
      "187551 Increasing lag to:  328 seq_len is  5\n",
      "187660 Increasing lag to:  329 seq_len is  5\n",
      "187674 Increasing lag to:  330 seq_len is  5\n",
      "187735 Increasing lag to:  331 seq_len is  5\n",
      "187747 Increasing lag to:  332 seq_len is  5\n",
      "187861 Increasing lag to:  333 seq_len is  5\n",
      "187873 Increasing lag to:  334 seq_len is  5\n",
      "187874 Increasing lag to:  335 seq_len is  5\n",
      "187937 Increasing lag to:  336 seq_len is  5\n",
      "187999 Increasing lag to:  337 seq_len is  5\n",
      "188000 [array(0.000895104487426579, dtype=float32), array(0.0019768700003623962, dtype=float32), array(0.00033961390727199614, dtype=float32)]\n",
      "0.00100591569208\n",
      "[[ 9.  8.  8.  7.  2.]\n",
      " [ 9.  8.  8.  8.  2.]]\n",
      "188032 Increasing lag to:  338 seq_len is  5\n",
      "188062 Increasing lag to:  339 seq_len is  5\n",
      "188092 Increasing lag to:  340 seq_len is  5\n",
      "188100 Increasing lag to:  341 seq_len is  5\n",
      "188269 Increasing lag to:  342 seq_len is  5\n",
      "188453 Increasing lag to:  343 seq_len is  5\n",
      "188459 Increasing lag to:  344 seq_len is  5\n",
      "188514 Increasing lag to:  345 seq_len is  5\n",
      "188542 Increasing lag to:  346 seq_len is  5\n",
      "188620 Increasing lag to:  347 seq_len is  5\n",
      "188637 Increasing lag to:  348 seq_len is  5\n",
      "188708 Increasing lag to:  349 seq_len is  5\n",
      "188761 Increasing lag to:  350 seq_len is  5\n",
      "188832 Increasing lag to:  351 seq_len is  5\n",
      "188858 Increasing lag to:  352 seq_len is  5\n",
      "188897 Increasing lag to:  353 seq_len is  5\n",
      "189014 Increasing lag to:  354 seq_len is  5\n",
      "189215 Increasing lag to:  355 seq_len is  5\n",
      "189362 Increasing lag to:  356 seq_len is  5\n",
      "189380 Increasing lag to:  357 seq_len is  5\n",
      "189406 Increasing lag to:  358 seq_len is  5\n",
      "189488 Increasing lag to:  359 seq_len is  5\n",
      "189510 Increasing lag to:  360 seq_len is  5\n",
      "189570 Increasing lag to:  361 seq_len is  5\n",
      "189626 Increasing lag to:  362 seq_len is  5\n",
      "189645 Increasing lag to:  363 seq_len is  5\n",
      "189648 Increasing lag to:  364 seq_len is  5\n",
      "189746 Increasing lag to:  365 seq_len is  5\n",
      "189765 Increasing lag to:  366 seq_len is  5\n",
      "189774 Increasing lag to:  367 seq_len is  5\n",
      "189779 Increasing lag to:  368 seq_len is  5\n",
      "189805 Increasing lag to:  369 seq_len is  5\n",
      "189819 Increasing lag to:  370 seq_len is  5\n",
      "189878 Increasing lag to:  371 seq_len is  5\n",
      "189927 Increasing lag to:  372 seq_len is  5\n",
      "190000 [array(0.00035600445698946714, dtype=float32), array(0.0014306269586086273, dtype=float32), array(1.1144574273203034e-05, dtype=float32)]\n",
      "0.000873448443599\n",
      "[[ 5.  8.  8.  3.  6.]\n",
      " [ 5.  8.  8.  3.  6.]]\n",
      "190034 Increasing lag to:  373 seq_len is  5\n",
      "190164 Increasing lag to:  374 seq_len is  5\n",
      "190216 Increasing lag to:  375 seq_len is  5\n",
      "190299 Increasing lag to:  376 seq_len is  5\n",
      "190370 Increasing lag to:  377 seq_len is  5\n",
      "190456 Increasing lag to:  378 seq_len is  5\n",
      "190466 Increasing lag to:  379 seq_len is  5\n",
      "190472 Increasing lag to:  380 seq_len is  5\n",
      "190502 Increasing lag to:  381 seq_len is  5\n",
      "190589 Increasing lag to:  382 seq_len is  5\n",
      "190617 Increasing lag to:  383 seq_len is  5\n",
      "190634 Increasing lag to:  384 seq_len is  5\n",
      "190757 Increasing lag to:  385 seq_len is  5\n",
      "190783 Increasing lag to:  386 seq_len is  5\n",
      "190802 Increasing lag to:  387 seq_len is  5\n",
      "190828 Increasing lag to:  388 seq_len is  5\n",
      "190845 Increasing lag to:  389 seq_len is  5\n",
      "190944 Increasing lag to:  390 seq_len is  5\n",
      "190981 Increasing lag to:  391 seq_len is  5\n",
      "191024 Increasing lag to:  392 seq_len is  5\n",
      "191053 Increasing lag to:  393 seq_len is  5\n",
      "191214 Increasing lag to:  394 seq_len is  5\n",
      "191224 Increasing lag to:  395 seq_len is  5\n",
      "191245 Increasing lag to:  396 seq_len is  5\n",
      "191247 Increasing lag to:  397 seq_len is  5\n",
      "191284 Increasing lag to:  398 seq_len is  5\n",
      "191333 Increasing lag to:  399 seq_len is  5\n",
      "191453 Increasing lag to:  400 seq_len is  5\n",
      "191480 Increasing lag to:  401 seq_len is  5\n",
      "191492 Increasing lag to:  402 seq_len is  5\n",
      "191521 Increasing lag to:  403 seq_len is  5\n",
      "191600 Increasing lag to:  404 seq_len is  5\n",
      "191627 Increasing lag to:  405 seq_len is  5\n",
      "191746 Increasing lag to:  406 seq_len is  5\n",
      "191784 Increasing lag to:  407 seq_len is  5\n",
      "191823 Increasing lag to:  408 seq_len is  5\n",
      "191826 Increasing lag to:  409 seq_len is  5\n",
      "191852 Increasing lag to:  410 seq_len is  5\n",
      "191866 Increasing lag to:  411 seq_len is  5\n",
      "191928 Increasing lag to:  412 seq_len is  5\n",
      "191929 Increasing lag to:  413 seq_len is  5\n",
      "191976 Increasing lag to:  414 seq_len is  5\n",
      "192000 [array(0.0005002155667170882, dtype=float32), array(0.0015679525677114725, dtype=float32), array(0.0001967052521649748, dtype=float32)]\n",
      "0.000865213689394\n",
      "[[ 9.  2.  5.  3.  5.]\n",
      " [ 9.  2.  5.  3.  5.]]\n",
      "192038 Increasing lag to:  415 seq_len is  5\n",
      "192176 Increasing lag to:  416 seq_len is  5\n",
      "192261 Increasing lag to:  417 seq_len is  5\n",
      "192305 Increasing lag to:  418 seq_len is  5\n",
      "192308 Increasing lag to:  419 seq_len is  5\n",
      "192326 Increasing lag to:  420 seq_len is  5\n",
      "192381 Increasing lag to:  421 seq_len is  5\n",
      "192430 Increasing lag to:  422 seq_len is  5\n",
      "192486 Increasing lag to:  423 seq_len is  5\n",
      "192533 Increasing lag to:  424 seq_len is  5\n",
      "192539 Increasing lag to:  425 seq_len is  5\n",
      "192596 Increasing lag to:  426 seq_len is  5\n",
      "192604 Increasing lag to:  427 seq_len is  5\n",
      "192684 Increasing lag to:  428 seq_len is  5\n",
      "192719 Increasing lag to:  429 seq_len is  5\n",
      "192855 Increasing lag to:  430 seq_len is  5\n",
      "192907 Increasing lag to:  431 seq_len is  5\n",
      "192940 Increasing lag to:  432 seq_len is  5\n",
      "192960 Increasing lag to:  433 seq_len is  5\n",
      "192968 Increasing lag to:  434 seq_len is  5\n",
      "193003 Increasing lag to:  435 seq_len is  5\n",
      "193020 Increasing lag to:  436 seq_len is  5\n",
      "193024 Increasing lag to:  437 seq_len is  5\n",
      "193082 Increasing lag to:  438 seq_len is  5\n",
      "193084 Increasing lag to:  439 seq_len is  5\n",
      "193230 Increasing lag to:  440 seq_len is  5\n",
      "193284 Increasing lag to:  441 seq_len is  5\n",
      "193348 Increasing lag to:  442 seq_len is  5\n",
      "193400 Increasing lag to:  443 seq_len is  5\n",
      "193402 Increasing lag to:  444 seq_len is  5\n",
      "193435 Increasing lag to:  445 seq_len is  5\n",
      "193448 Increasing lag to:  446 seq_len is  5\n",
      "193552 Increasing lag to:  447 seq_len is  5\n",
      "193626 Increasing lag to:  448 seq_len is  5\n",
      "193717 Increasing lag to:  449 seq_len is  5\n",
      "193768 Increasing lag to:  450 seq_len is  5\n",
      "193786 Increasing lag to:  451 seq_len is  5\n",
      "193800 Increasing lag to:  452 seq_len is  5\n",
      "193863 Increasing lag to:  453 seq_len is  5\n",
      "193988 Increasing lag to:  454 seq_len is  5\n",
      "194000 [array(2.3370777853415348e-05, dtype=float32), array(0.0010837891604751348, dtype=float32), array(1.7590740526429727e-06, dtype=float32)]\n",
      "0.000691326509696\n",
      "[[ 6.  9.  9.  8.  2.]\n",
      " [ 6.  9.  8.  8.  8.]]\n",
      "194025 Increasing lag to:  455 seq_len is  5\n",
      "194095 Increasing lag to:  456 seq_len is  5\n",
      "194107 Increasing lag to:  457 seq_len is  5\n",
      "194168 Increasing lag to:  458 seq_len is  5\n",
      "194199 Increasing lag to:  459 seq_len is  5\n",
      "194200 Increasing lag to:  460 seq_len is  5\n",
      "194221 Increasing lag to:  461 seq_len is  5\n",
      "194278 Increasing lag to:  462 seq_len is  5\n",
      "194319 Increasing lag to:  463 seq_len is  5\n",
      "194341 Increasing lag to:  464 seq_len is  5\n",
      "194349 Increasing lag to:  465 seq_len is  5\n",
      "194350 Increasing lag to:  466 seq_len is  5\n",
      "194364 Increasing lag to:  467 seq_len is  5\n",
      "194405 Increasing lag to:  468 seq_len is  5\n",
      "194422 Increasing lag to:  469 seq_len is  5\n",
      "194466 Increasing lag to:  470 seq_len is  5\n",
      "194509 Increasing lag to:  471 seq_len is  5\n",
      "194511 Increasing lag to:  472 seq_len is  5\n",
      "194526 Increasing lag to:  473 seq_len is  5\n",
      "194614 Increasing lag to:  474 seq_len is  5\n",
      "194683 Increasing lag to:  475 seq_len is  5\n",
      "194744 Increasing lag to:  476 seq_len is  5\n",
      "194780 Increasing lag to:  477 seq_len is  5\n",
      "194833 Increasing lag to:  478 seq_len is  5\n",
      "194857 Increasing lag to:  479 seq_len is  5\n",
      "194992 Increasing lag to:  480 seq_len is  5\n",
      "195078 Increasing lag to:  481 seq_len is  5\n",
      "195148 Increasing lag to:  482 seq_len is  5\n",
      "195162 Increasing lag to:  483 seq_len is  5\n",
      "195216 Increasing lag to:  484 seq_len is  5\n",
      "195256 Increasing lag to:  485 seq_len is  5\n",
      "195461 Increasing lag to:  486 seq_len is  5\n",
      "195466 Increasing lag to:  487 seq_len is  5\n",
      "195492 Increasing lag to:  488 seq_len is  5\n",
      "195505 Increasing lag to:  489 seq_len is  5\n",
      "195534 Increasing lag to:  490 seq_len is  5\n",
      "195567 Increasing lag to:  491 seq_len is  5\n",
      "195608 Increasing lag to:  492 seq_len is  5\n",
      "195609 Increasing lag to:  493 seq_len is  5\n",
      "195654 Increasing lag to:  494 seq_len is  5\n",
      "195725 Increasing lag to:  495 seq_len is  5\n",
      "195743 Increasing lag to:  496 seq_len is  5\n",
      "195807 Increasing lag to:  497 seq_len is  5\n",
      "195908 Increasing lag to:  498 seq_len is  5\n",
      "195945 Increasing lag to:  499 seq_len is  5\n",
      "196000 [array(8.278570021502674e-06, dtype=float32), array(0.00106148945633322, dtype=float32), array(2.840944191007111e-08, dtype=float32)]\n",
      "0.000414779118728\n",
      "[[ 2.  3.  3.  2.  5.]\n",
      " [ 2.  3.  3.  5.  9.]]\n",
      "196090 Increasing lag to:  500 seq_len is  5\n",
      "196103 Increasing lag to:  500 seq_len is  5\n",
      "196105 Increasing lag to:  500 seq_len is  5\n",
      "196127 Increasing lag to:  500 seq_len is  5\n",
      "196145 Increasing lag to:  500 seq_len is  5\n",
      "196167 Increasing lag to:  500 seq_len is  5\n",
      "196184 Increasing lag to:  500 seq_len is  5\n",
      "196220 Increasing lag to:  500 seq_len is  5\n",
      "196225 Increasing lag to:  500 seq_len is  5\n",
      "196272 Increasing lag to:  500 seq_len is  5\n",
      "196303 Increasing lag to:  500 seq_len is  5\n",
      "196334 Increasing lag to:  500 seq_len is  5\n",
      "196520 Increasing lag to:  500 seq_len is  5\n",
      "196536 Increasing lag to:  500 seq_len is  5\n",
      "196621 Increasing lag to:  500 seq_len is  5\n",
      "196736 Increasing lag to:  500 seq_len is  5\n",
      "196753 Increasing lag to:  500 seq_len is  5\n",
      "196754 Increasing lag to:  500 seq_len is  5\n",
      "196835 Increasing lag to:  500 seq_len is  5\n",
      "196859 Increasing lag to:  500 seq_len is  5\n",
      "196860 Increasing lag to:  500 seq_len is  5\n",
      "196885 Increasing lag to:  500 seq_len is  5\n",
      "196931 Increasing lag to:  500 seq_len is  5\n",
      "196933 Increasing lag to:  500 seq_len is  5\n",
      "197007 Increasing lag to:  500 seq_len is  5\n",
      "197051 Increasing lag to:  500 seq_len is  5\n",
      "197188 Increasing lag to:  500 seq_len is  5\n",
      "197220 Increasing lag to:  500 seq_len is  5\n",
      "197299 Increasing lag to:  500 seq_len is  5\n",
      "197509 Increasing lag to:  500 seq_len is  5\n",
      "197546 Increasing lag to:  500 seq_len is  5\n",
      "197561 Increasing lag to:  500 seq_len is  5\n",
      "197679 Increasing lag to:  500 seq_len is  5\n",
      "197690 Increasing lag to:  500 seq_len is  5\n",
      "197691 Increasing lag to:  500 seq_len is  5\n",
      "197771 Increasing lag to:  500 seq_len is  5\n",
      "197774 Increasing lag to:  500 seq_len is  5\n",
      "197817 Increasing lag to:  500 seq_len is  5\n",
      "197822 Increasing lag to:  500 seq_len is  5\n",
      "197895 Increasing lag to:  500 seq_len is  5\n",
      "197913 Increasing lag to:  500 seq_len is  5\n",
      "197969 Increasing lag to:  500 seq_len is  5\n",
      "198000 [array(0.00020199544087518007, dtype=float32), array(0.0012475487310439348, dtype=float32), array(1.578648698341567e-05, dtype=float32)]\n",
      "0.000496396620292\n",
      "[[ 5.  6.  3.  8.  8.]\n",
      " [ 5.  6.  3.  8.  8.]]\n",
      "198073 Increasing lag to:  500 seq_len is  5\n",
      "198087 Increasing lag to:  500 seq_len is  5\n",
      "198149 Increasing lag to:  500 seq_len is  5\n",
      "198205 Increasing lag to:  500 seq_len is  5\n",
      "198220 Increasing lag to:  500 seq_len is  5\n",
      "198307 Increasing lag to:  500 seq_len is  5\n",
      "198352 Increasing lag to:  500 seq_len is  5\n",
      "198355 Increasing lag to:  500 seq_len is  5\n",
      "198451 Increasing lag to:  500 seq_len is  5\n",
      "198543 Increasing lag to:  500 seq_len is  5\n",
      "198626 Increasing lag to:  500 seq_len is  5\n",
      "198639 Increasing lag to:  500 seq_len is  5\n",
      "198751 Increasing lag to:  500 seq_len is  5\n",
      "198945 Increasing lag to:  500 seq_len is  5\n",
      "198949 Increasing lag to:  500 seq_len is  5\n",
      "198973 Increasing lag to:  500 seq_len is  5\n",
      "199044 Increasing lag to:  500 seq_len is  5\n",
      "199112 Increasing lag to:  500 seq_len is  5\n",
      "199130 Increasing lag to:  500 seq_len is  5\n",
      "199175 Increasing lag to:  500 seq_len is  5\n",
      "199187 Increasing lag to:  500 seq_len is  5\n",
      "199235 Increasing lag to:  500 seq_len is  5\n",
      "199321 Increasing lag to:  500 seq_len is  5\n",
      "199425 Increasing lag to:  500 seq_len is  5\n",
      "199470 Increasing lag to:  500 seq_len is  5\n",
      "199484 Increasing lag to:  500 seq_len is  5\n",
      "199539 Increasing lag to:  500 seq_len is  5\n",
      "199548 Increasing lag to:  500 seq_len is  5\n",
      "199598 Increasing lag to:  500 seq_len is  5\n",
      "199611 Increasing lag to:  500 seq_len is  5\n",
      "199627 Increasing lag to:  500 seq_len is  5\n",
      "199669 Increasing lag to:  500 seq_len is  5\n",
      "199764 Increasing lag to:  500 seq_len is  5\n",
      "199779 Increasing lag to:  500 seq_len is  5\n",
      "199847 Increasing lag to:  500 seq_len is  5\n",
      "199856 Increasing lag to:  500 seq_len is  5\n",
      "199887 Increasing lag to:  500 seq_len is  5\n",
      "199935 Increasing lag to:  500 seq_len is  5\n",
      "199977 Increasing lag to:  500 seq_len is  5\n",
      "199995"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/pio/os/anaconda/lib/python2.7/site-packages/matplotlib/axes/_axes.py:475: UserWarning: No labelled objects found. Use label='...' kwarg on individual plots.\n",
      "  warnings.warn(\"No labelled objects found. \"\n",
      "/pio/os/anaconda/lib/python2.7/site-packages/matplotlib/figure.py:387: UserWarning: matplotlib is currently using a non-GUI backend, so cannot show the figure\n",
      "  \"matplotlib is currently using a non-GUI backend, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Increasing lag to:  500 seq_len is  5\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY8AAAEZCAYAAABvpam5AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa2c1d15590>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEZCAYAAAB4hzlwAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa2b00377d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "copy_net.initialize()\n",
    "\n",
    "# Feel free to tune the learing rate, this one worked the best for me. \n",
    "# We don't use any learing scheuling (lrate is constant), as every time the net starts getting closer to optimum, \n",
    "# we just make the problem harder (increasse the time lag) instead of trying to find the optimum more accurately\n",
    "# I tried using learing rate scheduling, without any noticable iprovement\n",
    "\n",
    "lrate = 5e-3\n",
    "copy_trainer.lrate.set_value(lrate)\n",
    "\n",
    "# Set weight decay, it seems to be helpful\n",
    "copy_trainer.wdec.set_value(1e-6)\n",
    "\n",
    "losses = []\n",
    "\n",
    "# Set the sequence parameters. We start from a trivial task and make it more difficult as we make progress.\n",
    "\n",
    "lag0 = 4\n",
    "lag = lag0\n",
    "seq_len0 = 1\n",
    "seq_len = seq_len0\n",
    "max_seq_len = 5 # Note that you may need more neurons for a longer sequence\n",
    "max_lag = 500\n",
    "max_it = 200000\n",
    "\n",
    "for i in xrange(max_it):\n",
    "    # lrate = \n",
    "    # copy_trainer.lrate.set_value(lrate)\n",
    "    \n",
    "    # We don't want the network to forget a solution for shorter sequences, so the sampled example is of a random size.\n",
    "    # This seems to be important for generalization.\n",
    "    this_lag = randint(lag0, lag + 1)\n",
    "    this_len = randint(seq_len0, seq_len + 1)\n",
    "    \n",
    "    # You can set the batch size here (or leave 32, which seems reasonable)\n",
    "    Xc, Yc = gen_copy_example(this_lag, this_len, 32)\n",
    "    ret = copy_trainer.train_function(Xc, Yc)\n",
    "    \n",
    "    # If the results are good enough, increase the length. \n",
    "    if this_lag > 0.9 * lag and this_len == seq_len and ret[0] < 3e-3:\n",
    "        lag += 1\n",
    "        seq_len = min(max_seq_len, lag / 10 + 1)\n",
    "        if lag > max_lag:\n",
    "            max_it = i + 10000\n",
    "            lag = max_lag\n",
    "        print i, \"Increasing lag to: \", lag , \"seq_len is \", seq_len\n",
    "    if i%2000 == 0:\n",
    "        print i, ret\n",
    "        Xc, Yc = gen_copy_example(lag, seq_len, 32)\n",
    "        ret = copy_test_function(Xc, Yc)\n",
    "        print ret\n",
    "        if i != 0: #ignore noisy start\n",
    "            losses.append((i,) + tuple([ret, lag]))\n",
    "        decoded_input = decode_matrix(Xc)\n",
    "        decoded_output = decode_matrix(copy_check_output(Xc).reshape(Xc.shape))\n",
    "        \n",
    "        # Sanity check\n",
    "        # Note that here we check the net's beahaviour on the max test, whereas most of training examples are shorter.\n",
    "        # We trust that the net outputs T + n '0's at the start, so we don't print it here for the transparency\n",
    "        print vstack([decoded_input[:,0].ravel()[:seq_len], decoded_output[:,0].ravel()[-seq_len:]])\n",
    "\n",
    "        \n",
    "# print charts\n",
    "\n",
    "losses_a = np.array(losses)\n",
    "\n",
    "p1 = plt.figure(1)\n",
    "\n",
    "legend(loc='lower right')\n",
    "title('Training loss')\n",
    "xlabel('iteration')\n",
    "\n",
    "plot(losses_a[:,0], losses_a[:,1], label='rms')\n",
    "\n",
    "savefig('losses_small.png')\n",
    "p1.show()\n",
    "\n",
    "#semilogy(losses_a[:,0], losses_a[:,2], label='rms + wdec')\n",
    "#semilogy(losses_a[:,0], losses_a[:,2], label='rms')\n",
    "#plot(losses_a[:,0], losses_a[:,3], label='grad norm')\n",
    "\n",
    "p2 = plt.figure(2)\n",
    "\n",
    "plot(losses_a[:,0], losses_a[:,2], label='lag')\n",
    "\n",
    "legend(loc='lower right')\n",
    "title('Lag')\n",
    "xlabel('iteration')\n",
    "\n",
    "savefig('lag_small.png')\n",
    "p2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the very rapid decrease in the loss as the lag gets bigger. This is mostly caused by the fact that the loss is the average loss over the whole genereted sequence, so the loss is sum_of_losses/length. The length increases, but sum_of_losses stays more or less the same, as the network is still rather confident that it should output '0's before seeing a '1' on the input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's wee what the trained network can do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00298358732834 0.12 0.00545454545455\n",
      "[[ 8.  3.  2.  7.  3.]\n",
      " [ 8.  3.  2.  3.  3.]]\n",
      "0.000221762951696 0.14 0.00538461538462\n",
      "[[ 7.  9.  7.  7.  5.]\n",
      " [ 7.  9.  7.  7.  5.]]\n",
      "0.000296104495646 0.09 0.003\n",
      "[[ 5.  2.  4.  4.  9.]\n",
      " [ 5.  2.  4.  4.  9.]]\n",
      "0.000297918100841 0.14 0.00411764705882\n",
      "[[ 4.  2.  3.  5.  4.]\n",
      " [ 4.  2.  3.  5.  4.]]\n",
      "9.90223124973e-05 0.15 0.00394736842105\n",
      "[[ 9.  6.  6.  8.  5.]\n",
      " [ 9.  6.  6.  8.  5.]]\n",
      "0.00142532098107 0.14 0.00333333333333\n",
      "[[ 8.  3.  6.  3.  7.]\n",
      " [ 8.  3.  6.  8.  8.]]\n",
      "0.000566864386201 0.12 0.00260869565217\n",
      "[[ 4.  8.  8.  8.  4.]\n",
      " [ 4.  8.  8.  8.  8.]]\n",
      "3.50811023964e-05 0.13 0.0026\n",
      "[[ 4.  7.  6.  6.  5.]\n",
      " [ 4.  7.  6.  6.  5.]]\n",
      "0.00085704337107 0.18 0.00333333333333\n",
      "[[ 5.  4.  3.  9.  3.]\n",
      " [ 5.  4.  3.  3.  3.]]\n",
      "0.000754897890147 0.08 0.00137931034483\n",
      "[[ 8.  6.  8.  3.  4.]\n",
      " [ 8.  6.  8.  3.  8.]]\n",
      "0.000472381711006 0.19 0.00306451612903\n",
      "[[ 7.  9.  6.  8.  3.]\n",
      " [ 7.  9.  6.  8.  8.]]\n",
      "9.879834397e-05 0.17 0.00257575757576\n",
      "[[ 4.  4.  8.  8.  8.]\n",
      " [ 4.  4.  8.  8.  8.]]\n",
      "0.000240392211708 0.2 0.00285714285714\n",
      "[[ 2.  9.  2.  2.  2.]\n",
      " [ 2.  9.  2.  2.  2.]]\n",
      "0.0014171586372 0.19 0.00256756756757\n",
      "[[ 7.  8.  3.  3.  9.]\n",
      " [ 8.  4.  3.  9.  9.]]\n",
      "0.000174127329956 0.18 0.00230769230769\n",
      "[[ 5.  7.  2.  4.  3.]\n",
      " [ 5.  7.  2.  4.  3.]]\n",
      "0.000757626607083 0.23 0.00280487804878\n",
      "[[ 3.  6.  8.  9.  3.]\n",
      " [ 3.  6.  2.  9.  9.]]\n",
      "0.00136655534152 0.18 0.00209302325581\n",
      "[[ 9.  4.  4.  4.  6.]\n",
      " [ 9.  4.  4.  4.  4.]]\n",
      "0.000163019198226 0.1 0.00111111111111\n",
      "[[ 4.  8.  9.  5.  7.]\n",
      " [ 4.  8.  9.  5.  7.]]\n",
      "0.000713721907232 0.21 0.00223404255319\n",
      "[[ 4.  5.  5.  4.  7.]\n",
      " [ 4.  5.  4.  5.  7.]]\n",
      "6.66764535708e-05 0.15 0.0015306122449\n",
      "[[ 6.  7.  7.  3.  7.]\n",
      " [ 6.  7.  7.  3.  7.]]\n",
      "5.16690961376e-05 0.17 0.00166666666667\n",
      "[[ 8.  4.  9.  7.  2.]\n",
      " [ 8.  4.  9.  7.  2.]]\n",
      "0.000207910197787 0.22 0.00207547169811\n",
      "[[ 8.  2.  8.  8.  2.]\n",
      " [ 8.  2.  8.  8.  2.]]\n",
      "0.000905330292881 0.27 0.00245454545455\n",
      "[[ 8.  4.  3.  3.  7.]\n",
      " [ 8.  3.  3.  3.  8.]]\n",
      "0.00106562115252 0.23 0.00201754385965\n",
      "[[ 6.  4.  3.  9.  6.]\n",
      " [ 6.  4.  9.  3.  4.]]\n",
      "0.000226496806135 0.21 0.00177966101695\n",
      "[[ 8.  4.  3.  2.  5.]\n",
      " [ 8.  4.  9.  2.  5.]]\n",
      "0.000299318257021 0.21 0.00172131147541\n",
      "[[ 6.  4.  3.  4.  3.]\n",
      " [ 6.  4.  3.  3.  3.]]\n",
      "0.000398689415306 0.22 0.00174603174603\n",
      "[[ 7.  5.  7.  5.  7.]\n",
      " [ 7.  5.  7.  7.  7.]]\n",
      "0.000212364670006 0.24 0.00184615384615\n",
      "[[ 2.  7.  3.  5.  6.]\n",
      " [ 2.  7.  3.  5.  5.]]\n",
      "0.000800641428214 0.29 0.00216417910448\n",
      "[[ 3.  8.  4.  5.  7.]\n",
      " [ 3.  8.  8.  5.  7.]]\n",
      "0.00038244202733 0.19 0.0013768115942\n",
      "[[ 2.  7.  5.  5.  7.]\n",
      " [ 2.  7.  5.  7.  7.]]\n",
      "0.000492012244649 0.29 0.00204225352113\n",
      "[[ 7.  4.  6.  4.  9.]\n",
      " [ 7.  2.  6.  4.  9.]]\n",
      "0.000508335127961 0.24 0.00164383561644\n",
      "[[ 3.  6.  5.  6.  4.]\n",
      " [ 3.  6.  5.  4.  4.]]\n",
      "0.000489915488288 0.33 0.0022\n",
      "[[ 2.  9.  6.  9.  6.]\n",
      " [ 2.  9.  6.  3.  4.]]\n",
      "0.000813014805317 0.35 0.00227272727273\n",
      "[[ 5.  6.  7.  5.  5.]\n",
      " [ 5.  6.  4.  5.  7.]]\n",
      "0.00233410089277 0.37 0.0023417721519\n",
      "[[ 6.  3.  2.  4.  2.]\n",
      " [ 2.  6.  6.  6.  6.]]\n",
      "0.000733841850888 0.31 0.00191358024691\n",
      "[[ 6.  2.  6.  4.  4.]\n",
      " [ 6.  6.  4.  4.  4.]]\n",
      "0.00017807178665 0.41 0.00246987951807\n",
      "[[ 4.  3.  6.  9.  3.]\n",
      " [ 4.  3.  6.  3.  3.]]\n",
      "0.00109935447108 0.4 0.00235294117647\n",
      "[[ 5.  9.  4.  5.  9.]\n",
      " [ 5.  9.  4.  4.  4.]]\n",
      "0.000608886708505 0.3 0.00172413793103\n",
      "[[ 3.  9.  4.  5.  3.]\n",
      " [ 3.  9.  3.  5.  8.]]\n",
      "0.000526863906998 0.36 0.00202247191011\n",
      "[[ 2.  2.  5.  2.  4.]\n",
      " [ 2.  2.  5.  5.  9.]]\n",
      "0.000695681083016 0.3 0.00164835164835\n",
      "[[ 5.  4.  7.  4.  9.]\n",
      " [ 5.  4.  7.  7.  9.]]\n",
      "6.00136409048e-05 0.37 0.00198924731183\n",
      "[[ 2.  9.  7.  7.  7.]\n",
      " [ 2.  9.  7.  7.  7.]]\n",
      "0.00123018724844 0.36 0.00189473684211\n",
      "[[ 9.  6.  4.  3.  7.]\n",
      " [ 2.  6.  4.  4.  3.]]\n",
      "0.00032366727828 0.4 0.0020618556701\n",
      "[[ 7.  9.  3.  2.  3.]\n",
      " [ 7.  9.  3.  3.  3.]]\n",
      "9.81579214567e-05 0.43 0.00217171717172\n",
      "[[ 9.  5.  9.  2.  2.]\n",
      " [ 9.  5.  9.  2.  2.]]\n",
      "0.000940496916883 0.4 0.0019801980198\n",
      "[[ 9.  6.  3.  8.  9.]\n",
      " [ 2.  6.  9.  9.  9.]]\n",
      "0.000575699901674 0.45 0.00218446601942\n",
      "[[ 2.  9.  6.  9.  2.]\n",
      " [ 2.  9.  6.  6.  4.]]\n",
      "0.00031924477662 0.54 0.008\n",
      "[[ 3.  4.  7.  9.  9.]\n",
      " [ 3.  4.  4.  9.  8.]]\n",
      "0.000749247497879 0.47 0.00219626168224\n",
      "[[ 2.  3.  9.  2.  3.]\n",
      " [ 2.  3.  3.  3.  3.]]\n",
      "0.000302985979943 0.4 0.00183486238532\n",
      "[[ 2.  3.  5.  3.  5.]\n",
      " [ 2.  3.  5.  3.  3.]]\n",
      "0.000914085831027 0.41 0.00184684684685\n",
      "[[ 6.  4.  6.  9.  3.]\n",
      " [ 2.  6.  3.  9.  3.]]\n",
      "0.000473666237667 0.44 0.00194690265487\n",
      "[[ 3.  7.  2.  2.  8.]\n",
      " [ 3.  7.  8.  2.  8.]]\n",
      "0.00125904823653 0.47 0.0124347826087\n",
      "[[ 2.  2.  4.  7.  5.]\n",
      " [ 2.  2.  2.  8.  8.]]\n",
      "0.000497437024023 0.42 0.00179487179487\n",
      "[[ 8.  8.  8.  9.  6.]\n",
      " [ 8.  4.  9.  8.  6.]]\n",
      "0.000922673847526 0.47 0.00197478991597\n",
      "[[ 3.  7.  9.  7.  5.]\n",
      " [ 3.  7.  9.  8.  8.]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXsAAAEZCAYAAAB2AoVaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa2af6db290>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "seq_len = 5\n",
    "\n",
    "err_rates = []\n",
    "\n",
    "for lag in range(100, 1200, 20):\n",
    "    err = 0\n",
    "    err_all = 0\n",
    "    for i in range(20):\n",
    "        Xc, Yc = gen_copy_example(lag, seq_len, 1)\n",
    "        ret = copy_test_function(Xc, Yc)\n",
    "\n",
    "        decoded_input = decode_matrix(Xc)\n",
    "        decoded_output = decode_matrix(copy_check_output(Xc).reshape(Xc.shape))\n",
    "\n",
    "        err += (decoded_input.ravel()[:seq_len] != decoded_output.ravel()[-seq_len:]).mean()\n",
    "        err_all += (np.hstack([np.zeros(seq_len + lag), decoded_input.ravel()[:seq_len] ]) != decoded_output.ravel()).mean()\n",
    "    \n",
    "    err /= 20\n",
    "    err_all /= 20\n",
    "    err_rates.append((lag,) + tuple([ret, err, err_all]))\n",
    "\n",
    "    print ret, err, err_all\n",
    "    print vstack([decoded_input.ravel()[:seq_len], decoded_output.ravel()[-seq_len:]])\n",
    "    #print hstack([decoded_input, decoded_output])\n",
    "    \n",
    "    \n",
    "    \n",
    "errors_a = np.array(err_rates)\n",
    "\n",
    "p1 = plt.figure(3)\n",
    "\n",
    "legend(loc='lower right')\n",
    "title('Error rates on sampled inputs')\n",
    "xlabel('Time lag')\n",
    "\n",
    "plot(errors_a[:,0], errors_a[:,2], label='err_rate')\n",
    "#plot(errors_a[:,0], errors_a[:,2], label='err_rate')\n",
    "#plot(errors_a[:,0], errors_a[:,3], label='err_rate')\n",
    "\n",
    "savefig('errors_small.png')\n",
    "p1.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Error presented above is the fraction of missed predictions. It gets higher as the time lag increases, but the results are still pretty good. For short seqences (<500) the net about 4/5 digits right. For instances twice as big as the ones that the net has ever sen druring the training, it gets about 3/5 digits correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
