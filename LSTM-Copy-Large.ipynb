{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Vendor:  Continuum Analytics, Inc.\n",
      "Package: mkl\n",
      "Message: trial mode expires in 27 days\n",
      "Vendor:  Continuum Analytics, Inc.\n",
      "Package: mkl\n",
      "Message: trial mode expires in 27 days\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "debug = True # global var to control debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import theano\n",
    "\n",
    "import theano.tensor as TT\n",
    "\n",
    "from theano.tensor.shared_randomstreams import RandomStreams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# These are taken from https://github.com/mila-udem/blocks\n",
    "# \n",
    "\n",
    "class Constant():\n",
    "    \"\"\"Initialize parameters to a constant.\n",
    "    The constant may be a scalar or a :class:`~numpy.ndarray` of any shape\n",
    "    that is broadcastable with the requested parameter arrays.\n",
    "    Parameters\n",
    "    ----------\n",
    "    constant : :class:`~numpy.ndarray`\n",
    "        The initialization value to use. Must be a scalar or an ndarray (or\n",
    "        compatible object, such as a nested list) that has a shape that is\n",
    "        broadcastable with any shape requested by `initialize`.\n",
    "    \"\"\"\n",
    "    def __init__(self, constant):\n",
    "        self._constant = numpy.asarray(constant)\n",
    "\n",
    "    def generate(self, rng, shape):\n",
    "        dest = numpy.empty(shape, dtype=np.float32)\n",
    "        dest[...] = self._constant\n",
    "        return dest\n",
    "\n",
    "\n",
    "class IsotropicGaussian():\n",
    "    \"\"\"Initialize parameters from an isotropic Gaussian distribution.\n",
    "    Parameters\n",
    "    ----------\n",
    "    std : float, optional\n",
    "        The standard deviation of the Gaussian distribution. Defaults to 1.\n",
    "    mean : float, optional\n",
    "        The mean of the Gaussian distribution. Defaults to 0\n",
    "    Notes\n",
    "    -----\n",
    "    Be careful: the standard deviation goes first and the mean goes\n",
    "    second!\n",
    "    \"\"\"\n",
    "    def __init__(self, std=1, mean=0):\n",
    "        self._mean = mean\n",
    "        self._std = std\n",
    "\n",
    "    def generate(self, rng, shape):\n",
    "        m = rng.normal(self._mean, self._std, size=shape)\n",
    "        return m.astype(np.float32)\n",
    "\n",
    "\n",
    "class Uniform():\n",
    "    \"\"\"Initialize parameters from a uniform distribution.\n",
    "    Parameters\n",
    "    ----------\n",
    "    mean : float, optional\n",
    "        The mean of the uniform distribution (i.e. the center of mass for\n",
    "        the density function); Defaults to 0.\n",
    "    width : float, optional\n",
    "        One way of specifying the range of the uniform distribution. The\n",
    "        support will be [mean - width/2, mean + width/2]. **Exactly one**\n",
    "        of `width` or `std` must be specified.\n",
    "    std : float, optional\n",
    "        An alternative method of specifying the range of the uniform\n",
    "        distribution. Chooses the width of the uniform such that random\n",
    "        variates will have a desired standard deviation. **Exactly one** of\n",
    "        `width` or `std` must be specified.\n",
    "    \"\"\"\n",
    "    def __init__(self, mean=0., width=None, std=None):\n",
    "        if (width is not None) == (std is not None):\n",
    "            raise ValueError(\"must specify width or std, \"\n",
    "                             \"but not both\")\n",
    "        if std is not None:\n",
    "            # Variance of a uniform is 1/12 * width^2\n",
    "            self._width = numpy.sqrt(12) * std\n",
    "        else:\n",
    "            self._width = width\n",
    "        self._mean = mean\n",
    "\n",
    "    def generate(self, rng, shape):\n",
    "        w = self._width / 2\n",
    "        m = rng.uniform(self._mean - w, self._mean + w, size=shape)\n",
    "        return m.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def default(var, val):\n",
    "    if var is None:\n",
    "        return val\n",
    "    else:\n",
    "        return var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM implementation in Theano\n",
    "\n",
    "Theano fully supports recurrent neural networks. One typically needs only to provide an implementation of a single step of the recurrency.\n",
    "\n",
    "Please read about the scan function which is used to implement the loops: http://deeplearning.net/software/theano/library/scan.html.\n",
    "\n",
    "**Attention**: through the code we will assume that the 0-th axis refers to time and that the 1-st axis refers to individual examples inside a minibatch. (This way in a C-major memory layout individual time steps occupy contiguous regions in memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"\n",
    "    Taken from https://github.com/vitruvianscience/OpenDeep/blob/master/opendeep/utils/activation.py\n",
    "    \n",
    "    See the Theano documentation.\n",
    "    Returns the row-wise softmax function of x.\n",
    "    In the case of 3D input, it returns the scan of softmax applied over the second two dimensions\n",
    "    (loops over first dimension).\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : 2D or 3D tensor\n",
    "        Symbolic 2D or 3D Tensor (or compatible).\n",
    "    Returns\n",
    "    -------\n",
    "    2D or 3D tensor\n",
    "        Row-wise softmax: softmax_{ij}(x) = exp(x_{ij})/sum_k(exp(x_{ik})) applied to `x`. Returns same shape as input.\n",
    "    \"\"\"\n",
    "    if x.ndim == 3:\n",
    "        cost, _ = theano.scan(fn=TT.nnet.softmax, sequences=x)\n",
    "        return cost\n",
    "    return TT.nnet.softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Layer(object):\n",
    "    def __init__(self, rng=None):\n",
    "        if rng is None:\n",
    "            rng = numpy.random\n",
    "        self.rng = rng\n",
    "        self._parameters = []\n",
    "    \n",
    "    @property\n",
    "    def parameters(self):\n",
    "        return self._parameters\n",
    "    \n",
    "    def add_param(self, name, shape, initializer, dtype='float32'):\n",
    "        param = theano.shared(numpy.zeros(\n",
    "            shape, dtype=dtype), name=name)\n",
    "        param.tag.initializer = initializer\n",
    "        self._parameters.append(param)\n",
    "        setattr(self, name, param)\n",
    "        \n",
    "    def initialize(self):\n",
    "        for p in self.parameters:\n",
    "            p.set_value(p.tag.initializer.generate(self.rng, \n",
    "                                                   p.get_value().shape))\n",
    "\n",
    "\n",
    "class RecurrentLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(RecurrentLayer, self).__init__(**kwargs)\n",
    "        self.initial_states = []\n",
    "    \n",
    "    def apply(self, X, **kwargs):\n",
    "        batch_size = X.shape[1]\n",
    "        outputs_info = []\n",
    "        for h in self.initial_states:\n",
    "            h0 = TT.repeat(h, batch_size, axis=0)\n",
    "            outputs_info.append(dict(initial=h0))\n",
    "        \n",
    "        #\n",
    "        # Scan in theano takes a function which performs a single step of the\n",
    "        # recurrent computation. Subclasses just need to provide the\n",
    "        # self.transition function.\n",
    "        #\n",
    "        scan_result, scan_updates = theano.scan(\n",
    "            self.transition,\n",
    "            sequences=X,\n",
    "            outputs_info=outputs_info,\n",
    "            **kwargs\n",
    "            )\n",
    "        # Note: this in general will not be the case and we will need to\n",
    "        # make sure that the updates are given to theano.function\n",
    "        assert not scan_updates\n",
    "        return scan_result\n",
    "\n",
    "\n",
    "class MergeInputHiddens(Layer):\n",
    "    \"\"\"\n",
    "    Merge two sequences - inputs and hidden states and produce an output.\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, in_dim, hidden_dim, out_dim,\n",
    "                 weight_init=None, bias_init=None, \n",
    "                 **kwargs):\n",
    "        super(MergeInputHiddens, self).__init__(**kwargs)\n",
    "        weight_init = default(\n",
    "            weight_init, IsotropicGaussian(1.0/sqrt(in_dim)))\n",
    "        bias_init = default(\n",
    "            weight_init, Constant(0.))\n",
    "        \n",
    "        # Input to output\n",
    "        self.add_param('Wxo', (in_dim, out_dim), \n",
    "                       weight_init)\n",
    "        \n",
    "        # Hidden to output\n",
    "        self.add_param('Who', (hidden_dim, out_dim), \n",
    "                       weight_init)\n",
    "        \n",
    "        # Output bias\n",
    "        self.add_param('Bo', (out_dim,), \n",
    "                       bias_init)\n",
    "        \n",
    "    def apply(self, X, H):\n",
    "        # Get the shape\n",
    "        nsteps, bs, nin = X.shape\n",
    "        nhid = H.shape[2]\n",
    "        \n",
    "        # Note - we flatten the steps and batch size\n",
    "        # as the computation of outputs can be performed in \n",
    "        # parallel for all time steps.\n",
    "        \n",
    "        O = (X.reshape((nsteps*bs, nin)).dot(self.Wxo) + \n",
    "             H.reshape((nsteps*bs, nhid)).dot(self.Who) +\n",
    "             self.Bo)\n",
    "        return O.reshape((nsteps, bs, O.shape[1]))\n",
    "    \n",
    "class Chain(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Chain, self).__init__(**kwargs)\n",
    "        self.children = []\n",
    "        \n",
    "    @property\n",
    "    def parameters(self):\n",
    "        ret = list(self._parameters)\n",
    "        for c in self.children:\n",
    "            ret.extend(c.parameters)\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Trainer(object):\n",
    "    \"\"\"\n",
    "    This class computes the updates to parameters using the RMSProp learning rule and adding weight decay.\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, net_loss, parameters, inputs):\n",
    "        self.RMSProp_dec_rate = \\\n",
    "            theano.shared(np.array(0.9, dtype='float32'))\n",
    "        self.RMSProp_epsilon = \\\n",
    "            theano.shared(np.array(1e-5, dtype='float32'))\n",
    "        self.lrate = \\\n",
    "            theano.shared(np.array(1e-2, dtype='float32'))\n",
    "        self.max_grad_norm = \\\n",
    "            theano.shared(np.array(1., dtype='float32'))\n",
    "        self.wdec = \\\n",
    "            theano.shared(np.array(0., dtype='float32'))\n",
    "\n",
    "        theano.config.compute_test_value='off' # Turn off for gradient computation\n",
    "        \n",
    "        wdec_loss = 0\n",
    "        for p in parameters:\n",
    "            if p.name[0]=='W':\n",
    "                wdec_loss = wdec_loss + (p**2).sum()*self.wdec\n",
    "        \n",
    "        grads = theano.grad(net_loss + wdec_loss, parameters)\n",
    "        updates = []\n",
    "\n",
    "        grad_norm = 0.\n",
    "\n",
    "        for g in grads:\n",
    "            grad_norm = grad_norm + (g**2).sum()\n",
    "        \n",
    "\n",
    "        for g,p in zip(grads, parameters):\n",
    "            step = g\n",
    "            step = g / TT.maximum(1.0, grad_norm/self.max_grad_norm)\n",
    "            if 1:\n",
    "                g2 = theano.shared(p.get_value() * 0.,\n",
    "                                   name=p.name + '_g2')\n",
    "                g2_new = (self.RMSProp_dec_rate * g2 + \n",
    "                          (1.0 - self.RMSProp_dec_rate) * g**2)\n",
    "                updates.append((g2, g2_new))\n",
    "                step = step / TT.sqrt(g2_new + self.RMSProp_epsilon)\n",
    "\n",
    "            step = self.lrate * step\n",
    "            updates.append((p, p - step))\n",
    "\n",
    "        self.train_function = theano.function(inputs, \n",
    "                                              [net_loss, net_loss + wdec_loss, grad_norm], \n",
    "                                              updates=updates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LSTM_RNN(RecurrentLayer):\n",
    "    \"\"\"\n",
    "    Implementation follows Alex Graves, Abdel-rahman Mohamed and Geoffrey Hinton\n",
    "    \"SPEECH RECOGNITION WITH DEEP RECURRENT NEURAL NETWORKS\"\n",
    "    http://www.cs.toronto.edu/~fritz/absps/RNN13.pdf\n",
    "    \"\"\"\n",
    "    def __init__(self, in_dim, hidden_dim,\n",
    "                 hidden_activation=TT.tanh,\n",
    "                 rec_weight_init=None,\n",
    "                 weight_init=None, bias_init=None, \n",
    "                 forget_bias_init=None, \n",
    "                 **kwargs):\n",
    "        super(LSTM_RNN, self).__init__(**kwargs)\n",
    "        rec_weight_init = default(\n",
    "            rec_weight_init, IsotropicGaussian(1.0/sqrt(hidden_dim)))\n",
    "        weight_init = default(\n",
    "            weight_init, IsotropicGaussian(1.0/sqrt(in_dim)))\n",
    "        bias_init = default(\n",
    "            weight_init, Constant(0.))\n",
    "        forget_bias_init = default(\n",
    "            weight_init, Constant(1.))\n",
    "        self.hidden_activation = hidden_activation\n",
    "        \n",
    "        #\n",
    "        # Gates\n",
    "        #\n",
    "        \n",
    "        for gate in 'ifo':\n",
    "            self.add_param('Wx' + gate, (in_dim, hidden_dim), \n",
    "                           weight_init)\n",
    "            self.add_param('Wh' + gate, (hidden_dim, hidden_dim), \n",
    "                           weight_init)\n",
    "            # Note: a cell is only connected to its own gates\n",
    "            # Wc... are diagonal - so we allocate only a vector\n",
    "            # for them\n",
    "            self.add_param('Wc' + gate, (hidden_dim,), \n",
    "                           weight_init)\n",
    "            self.add_param('B' + gate, (hidden_dim,), \n",
    "                           bias_init)\n",
    "        \n",
    "        #\n",
    "        # Note - forget gate bias has a different initializer, because\n",
    "        # we often want to initialize it to 1\n",
    "        #\n",
    "        self.Bf.tag.initializer = forget_bias_init\n",
    "        \n",
    "        # Cell\n",
    "        self.add_param('Wxc', (in_dim, hidden_dim), \n",
    "                       weight_init)\n",
    "        self.add_param('Whc', (hidden_dim, hidden_dim), \n",
    "                       weight_init)\n",
    "        self.add_param('Bc', (hidden_dim,), \n",
    "                       bias_init)\n",
    "        \n",
    "        # Initial states\n",
    "        self.add_param('h0', (1, hidden_dim), \n",
    "                       bias_init)\n",
    "        self.initial_states.append(self.h0)\n",
    "        self.add_param('c0', (1, hidden_dim), \n",
    "                       bias_init)\n",
    "        self.initial_states.append(self.c0)\n",
    "        \n",
    "        \n",
    "    def transition(self, x, h, c):\n",
    "        \"\"\"\n",
    "        One step of LSTM transition.\n",
    "        \n",
    "        x is the previous input\n",
    "        h is the previous hidden state\n",
    "        c is the previous memory cell content\n",
    "        \"\"\"\n",
    "        \n",
    "        #\n",
    "        # Please note:\n",
    "        # The implementation below is not speed-optimal\n",
    "        # usually, it pays off to group similar matrix multiplications\n",
    "        # by grouping gates.\n",
    "        #\n",
    "        # Also, input-related computations should be moved out of scan since\n",
    "        # they can be done for all steps in parallel.\n",
    "        #\n",
    "        \n",
    "        \n",
    "        s = TT.nnet.sigmoid\n",
    "\n",
    "        # Note: for cells we do element0wise multiplication which \n",
    "        # is equvalent to a matrix multiplication with a diagonal matrix!\n",
    "        i = s(x.dot(self.Wxi) + h.dot(self.Whi) + c*self.Wci + self.Bi)\n",
    "        f = s(x.dot(self.Wxf) + h.dot(self.Whf) + c*self.Wcf + self.Bf)\n",
    "        \n",
    "        c_new = f*c + i*self.hidden_activation(x.dot(self.Wxc) + h.dot(self.Whc) + self.Bc)\n",
    "        o = s(x.dot(self.Wxo) + h.dot(self.Who) + c_new*self.Wco + self.Bo)\n",
    "        h_new = o * self.hidden_activation(c_new)\n",
    "        \n",
    "        return h_new, c_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# we encode be input and output as a 1-of-N vector, thus every element of the input and output is a 1x10 vector with one '1'.\n",
    "\n",
    "def class_encoding(n, num_classes = 10):\n",
    "    ret = zeros(shape=(num_classes))\n",
    "    ret[n] = 1\n",
    "    return ret\n",
    "\n",
    "def class_decoding(v):\n",
    "    return argmax(v)\n",
    "\n",
    "# This should be rewritten without the use of loops\n",
    "\n",
    "def decode_matrix(matrix):\n",
    "    decoded_matrix = zeros(shape = (matrix.shape[0], matrix.shape[1]))\n",
    "    for i in range(matrix.shape[0]):\n",
    "        for j in range(matrix.shape[1]):\n",
    "            decoded_matrix[i][j] = class_decoding(matrix[i][j])\n",
    "    return decoded_matrix\n",
    "\n",
    "def encode_matrix(matrix, num_classes = 10):\n",
    "    encoded_matrix = zeros(shape = (matrix.shape[0], matrix.shape[1], num_classes))\n",
    "    for i in range(matrix.shape[0]):\n",
    "        for j in range(matrix.shape[1]):\n",
    "            encoded_matrix[i][j] = class_encoding(matrix[i][j])\n",
    "    return encoded_matrix.astype(\"float32\")\n",
    "\n",
    "\n",
    "def gen_copy_example(T, seq_len, batchsize):\n",
    "    rng = numpy.random\n",
    "    \n",
    "    sequence = rng.randint(2, 10, size=(seq_len, batchsize))\n",
    "    \n",
    "    X = np.concatenate((sequence,\n",
    "                         np.zeros(shape=(T - 1, batchsize)),\n",
    "                         np.ones(shape=(1, batchsize)), \n",
    "                         np.zeros(shape=(seq_len, batchsize))))\n",
    "    \n",
    "    Y = np.concatenate((np.zeros(shape=(T + seq_len, batchsize)),\n",
    "                        sequence.reshape(seq_len, batchsize)))\n",
    "    \n",
    "    \n",
    "    return encode_matrix(X), encode_matrix(Y)\n",
    "\n",
    "\n",
    "Xc, Yc = gen_copy_example(4, 4, 2)\n",
    "#print Xc.shape, Yc.shape\n",
    "#print 'X:', Xc[:,0,:], '\\nY:', Yc[:,0,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At each point in time, network receives an input of size 1x10 (1-hot encoding), propagets the signal through a layer of\n",
    "hidden_dim LSTM neurons and produces an output of size 1x10 (also treated as 1-hot encoding). Softmax is applied at\n",
    "the end to get calculated probabilities. Cross-entropy is used as a loss function.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class CopyNet(Chain):\n",
    "    def __init__(self, hidden_dim, num_layers=1,\n",
    "                 **kwargs):\n",
    "        super(CopyNet, self).__init__(**kwargs)\n",
    "        self.recs = []\n",
    "        \n",
    "        for hid in xrange(num_layers):\n",
    "            if hid == 0:\n",
    "                in_dim = 10\n",
    "            else:\n",
    "                in_dim = hidden_dim\n",
    "                \n",
    "            rec = LSTM_RNN(in_dim=in_dim, hidden_dim=hidden_dim)\n",
    "            \n",
    "            self.recs.append(rec)\n",
    "            self.children.append(rec)\n",
    "        \n",
    "        self.merge = MergeInputHiddens(\n",
    "            in_dim=10, hidden_dim=hidden_dim,\n",
    "            out_dim=10\n",
    "            )\n",
    "        self.children.append(self.merge)\n",
    "        \n",
    "        self.X = TT.tensor3('X')\n",
    "        self.Y = TT.tensor3('Y')\n",
    "        \n",
    "        self.inputs = [self.X, self.Y]\n",
    "    \n",
    "    def apply(self, X):\n",
    "        H = X\n",
    "        for rec in self.recs:\n",
    "            H = rec.apply(H)\n",
    "            H = H[0] # we don't use cell contents\n",
    "        O = self.merge.apply(X, H)\n",
    "        \n",
    "        return softmax(O)\n",
    "    \n",
    "    # We use cross-entropy to measure the efficiency of the network. You can also a\n",
    "    def get_loss(self):\n",
    "        copy_net_output = self.apply(self.X)\n",
    "        # return ((copy_net_output - self.Y)**2).mean() # MSE\n",
    "        return -(self.Y * TT.log(copy_net_output)).mean() # cross entropy\n",
    "    \n",
    "    def get_output(self):\n",
    "        copy_net_output = self.apply(self.X)\n",
    "        return copy_net_output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24, 1, 10) (24, 1, 10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/pio/os/anaconda/lib/python2.7/site-packages/theano/scan_module/scan_perform_ext.py:133: RuntimeWarning: numpy.ndarray size changed, may indicate binary incompatibility\n",
      "  from scan_perform.scan_perform import *\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(0.2049994170665741, dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xc, Yc = gen_copy_example(4, 10, 1)\n",
    "print Xc.shape, Yc.shape\n",
    "theano.config.compute_test_value='off'\n",
    "theano.config.print_test_value=True\n",
    "debug = False\n",
    "\n",
    "# Set the nubmer of hidden cells here. Note that intuitively you need more cells if you want to copy longer sequences. \n",
    "# The ratio 4 neurons per max seqence lenght seems to be fine.\n",
    "copy_net = CopyNet(hidden_dim=40)\n",
    "copy_net.initialize()\n",
    "copy_net_loss = copy_net.get_loss()\n",
    "copy_net_output = copy_net.get_output()\n",
    "\n",
    "copy_test_function = theano.function(copy_net.inputs, \n",
    "                                       copy_net_loss)\n",
    "copy_check_output = theano.function([copy_net.X], \n",
    "                                       copy_net_output)\n",
    "\n",
    "copy_test_function(Xc, Yc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "copy_trainer = Trainer(copy_net_loss, copy_net.parameters, copy_net.inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem we are solving is described in the following paper, section 5.1: http://arxiv.org/pdf/1511.06464v2.pdf.\n",
    "The input is a seqeunce of size 2 * n + T and each element of the input is of one of the 10 classes, which will name by the numbers from 0 to 9. First n elements is an arbitrary sequence of numbers 2...9. Then, T - 1 '0's follow. The next element is a single '1' and the last n elements are '0's. The desired output is a sequence of the same length, where the first T + n elemtnts are '0's followed by sequence from the beginnig of the input. The kth element of the output should be produced after the net sees the kth element of the input.\n",
    "\n",
    "The difficulty of the task is the fact that a network has to put elements in the memory, store them for a long time and then reproduce them. \n",
    "\n",
    "In the cited paper authors used n = 10 (length of the sequence to reproduce) and 40 neurons. Their network was only a little bit better than baseline (a program that outputs T + n '0's and n random numbers) when T = 100 and did not learn at all for larger T.\n",
    "\n",
    "It tunrs out that this task is in fact easy with the use of curriculum. My network was able to learn to perfectly reproduce the sequence with the time lag of 400.\n",
    "\n",
    "The curriculum is used both for the n (seq_len) as well as T (lag). In every consecutive batch the lag is of random size. As the newtork get better, we increase both the maximium allowed lag, as well as the sequnece lenght. After the training, the net should be able to copy a sequence of any length over any time lag not greater than the learnt ones. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [array(0.26487624645233154, dtype=float32), array(0.2657579183578491, dtype=float32), array(0.024268051609396935, dtype=float32)]\n",
      "0.24447825551\n",
      "[[ 3.]\n",
      " [ 7.]]\n",
      "738 Increasing lag to:  5 seq_len is  1\n",
      "780 Increasing lag to:  6 seq_len is  1\n",
      "907 Increasing lag to:  7 seq_len is  1\n",
      "981 Increasing lag to:  8 seq_len is  1\n",
      "1007 Increasing lag to:  9 seq_len is  1\n",
      "1032 Increasing lag to:  10 seq_len is  2\n",
      "2000 [array(0.020747285336256027, dtype=float32), array(0.021738654002547264, dtype=float32), array(0.0015170926926657557, dtype=float32)]\n",
      "0.0138400243595\n",
      "[[ 2.  7.]\n",
      " [ 2.  2.]]\n",
      "4000 [array(0.00013780887820757926, dtype=float32), array(0.001173645374365151, dtype=float32), array(4.337234713602811e-07, dtype=float32)]\n",
      "0.00874095596373\n",
      "[[ 9.  5.]\n",
      " [ 9.  5.]]\n",
      "4341 Increasing lag to:  11 seq_len is  2\n",
      "4500 Increasing lag to:  12 seq_len is  2\n",
      "4598 Increasing lag to:  13 seq_len is  2\n",
      "4613 Increasing lag to:  14 seq_len is  2\n",
      "4777 Increasing lag to:  15 seq_len is  2\n",
      "4847 Increasing lag to:  16 seq_len is  2\n",
      "4959 Increasing lag to:  17 seq_len is  2\n",
      "4999 Increasing lag to:  18 seq_len is  2\n",
      "5181 Increasing lag to:  19 seq_len is  2\n",
      "5276 Increasing lag to:  20 seq_len is  3\n",
      "6000 [array(0.033451057970523834, dtype=float32), array(0.03451882302761078, dtype=float32), array(0.008686655201017857, dtype=float32)]\n",
      "0.0166419036686\n",
      "[[ 7.  6.  3.]\n",
      " [ 7.  6.  6.]]\n",
      "8000 [array(0.0026425134856253862, dtype=float32), array(0.003732494544237852, dtype=float32), array(0.003350307699292898, dtype=float32)]\n",
      "0.0108370985836\n",
      "[[ 3.  9.  4.]\n",
      " [ 3.  9.  4.]]\n",
      "10000 [array(0.01720581017434597, dtype=float32), array(0.01831725798547268, dtype=float32), array(0.00934070535004139, dtype=float32)]\n",
      "0.00981583166867\n",
      "[[ 2.  7.  8.]\n",
      " [ 2.  7.  8.]]\n",
      "11765 Increasing lag to:  21 seq_len is  3\n",
      "12000 [array(0.009717232547700405, dtype=float32), array(0.010845139622688293, dtype=float32), array(0.002886006375774741, dtype=float32)]\n",
      "0.00980322621763\n",
      "[[ 9.  6.  8.]\n",
      " [ 9.  6.  6.]]\n",
      "13978 Increasing lag to:  22 seq_len is  3\n",
      "14000 [array(0.006962194107472897, dtype=float32), array(0.00810522586107254, dtype=float32), array(0.0037843126337975264, dtype=float32)]\n",
      "0.00995826907456\n",
      "[[ 2.  6.  7.]\n",
      " [ 2.  6.  4.]]\n",
      "14336 Increasing lag to:  23 seq_len is  3\n",
      "14362 Increasing lag to:  24 seq_len is  3\n",
      "14802 Increasing lag to:  25 seq_len is  3\n",
      "14942 Increasing lag to:  26 seq_len is  3\n",
      "14963 Increasing lag to:  27 seq_len is  3\n",
      "14976 Increasing lag to:  28 seq_len is  3\n",
      "15095 Increasing lag to:  29 seq_len is  3\n",
      "15115 Increasing lag to:  30 seq_len is  4\n",
      "16000 [array(0.01755312830209732, dtype=float32), array(0.018703514710068703, dtype=float32), array(0.003775332821533084, dtype=float32)]\n",
      "0.0118469586596\n",
      "[[ 2.  6.  3.  8.]\n",
      " [ 2.  8.  6.  8.]]\n",
      "18000 [array(5.1115617679897696e-05, dtype=float32), array(0.0012092795222997665, dtype=float32), array(3.102421999301441e-07, dtype=float32)]\n",
      "0.00988744478673\n",
      "[[ 5.  8.  3.  8.]\n",
      " [ 5.  8.  8.  8.]]\n",
      "20000 [array(0.005862566642463207, dtype=float32), array(0.00702981511130929, dtype=float32), array(0.0028622823301702738, dtype=float32)]\n",
      "0.00913317780942\n",
      "[[ 3.  6.  7.  3.]\n",
      " [ 3.  6.  7.  5.]]\n",
      "22000 [array(0.02031770348548889, dtype=float32), array(0.021494656801223755, dtype=float32), array(0.004001246765255928, dtype=float32)]\n",
      "0.00980805326253\n",
      "[[ 6.  6.  4.  8.]\n",
      " [ 6.  6.  4.  8.]]\n",
      "24000 [array(0.0003859340213239193, dtype=float32), array(0.001570434309542179, dtype=float32), array(3.3171443647006527e-05, dtype=float32)]\n",
      "0.00834745634347\n",
      "[[ 8.  2.  2.  9.]\n",
      " [ 8.  2.  2.  2.]]\n",
      "26000 [array(2.8478030799305998e-05, dtype=float32), array(0.0012217763578519225, dtype=float32), array(2.981944646762713e-08, dtype=float32)]\n",
      "0.00741778034717\n",
      "[[ 5.  5.  7.  6.]\n",
      " [ 5.  5.  7.  5.]]\n",
      "26704 Increasing lag to:  31 seq_len is  4\n",
      "27166 Increasing lag to:  32 seq_len is  4\n",
      "27809 Increasing lag to:  33 seq_len is  4\n",
      "27911 Increasing lag to:  34 seq_len is  4\n",
      "27923 Increasing lag to:  35 seq_len is  4\n",
      "28000 [array(0.006674700416624546, dtype=float32), array(0.007876325398683548, dtype=float32), array(0.0024841176345944405, dtype=float32)]\n",
      "0.00672578206286\n",
      "[[ 9.  8.  8.  5.]\n",
      " [ 9.  8.  8.  8.]]\n",
      "28024 Increasing lag to:  36 seq_len is  4\n",
      "28434 Increasing lag to:  37 seq_len is  4\n",
      "28648 Increasing lag to:  38 seq_len is  4\n",
      "28825 Increasing lag to:  39 seq_len is  4\n",
      "28885 Increasing lag to:  40 seq_len is  5\n",
      "30000 [array(0.009286071173846722, dtype=float32), array(0.010493136942386627, dtype=float32), array(0.0010810710955411196, dtype=float32)]\n",
      "0.00982395932078\n",
      "[[ 8.  3.  5.  9.  4.]\n",
      " [ 8.  3.  9.  5.  7.]]\n",
      "32000 [array(0.0018226144602522254, dtype=float32), array(0.00303571205586195, dtype=float32), array(0.0009623034275136888, dtype=float32)]\n",
      "0.00928954407573\n",
      "[[ 6.  6.  9.  2.  8.]\n",
      " [ 6.  2.  2.  2.  2.]]\n",
      "34000 [array(0.00012414703087415546, dtype=float32), array(0.0013445894001051784, dtype=float32), array(2.1238085992081324e-06, dtype=float32)]\n",
      "0.00836742576212\n",
      "[[ 3.  6.  7.  5.  7.]\n",
      " [ 3.  6.  7.  5.  5.]]\n",
      "36000 [array(2.003132613026537e-05, dtype=float32), array(0.0012461007572710514, dtype=float32), array(1.692804829644956e-08, dtype=float32)]\n",
      "0.00785295758396\n",
      "[[ 3.  8.  8.  6.  2.]\n",
      " [ 3.  8.  6.  8.  4.]]\n",
      "38000 [array(2.8911254048580304e-05, dtype=float32), array(0.0012616219464689493, dtype=float32), array(2.6493122362580834e-08, dtype=float32)]\n",
      "0.00772770214826\n",
      "[[ 5.  3.  2.  3.  6.]\n",
      " [ 5.  3.  2.  6.  2.]]\n",
      "40000 [array(0.00014822249067947268, dtype=float32), array(0.0013857576996088028, dtype=float32), array(1.539301820230321e-06, dtype=float32)]\n",
      "0.00681950617582\n",
      "[[ 5.  4.  4.  8.  9.]\n",
      " [ 5.  4.  4.  8.  8.]]\n",
      "40001 Increasing lag to:  41 seq_len is  5\n",
      "41001 Increasing lag to:  42 seq_len is  5\n",
      "41211 Increasing lag to:  43 seq_len is  5\n",
      "41951 Increasing lag to:  44 seq_len is  5\n",
      "42000 [array(0.007158244960010052, dtype=float32), array(0.008401941508054733, dtype=float32), array(0.0006211405852809548, dtype=float32)]\n",
      "0.0062360027805\n",
      "[[ 4.  7.  3.  3.  8.]\n",
      " [ 4.  3.  3.  4.  8.]]\n",
      "42360 Increasing lag to:  45 seq_len is  5\n",
      "42914 Increasing lag to:  46 seq_len is  5\n",
      "42982 Increasing lag to:  47 seq_len is  5\n",
      "43044 Increasing lag to:  48 seq_len is  5\n",
      "43071 Increasing lag to:  49 seq_len is  5\n",
      "43259 Increasing lag to:  50 seq_len is  6\n",
      "44000 [array(0.019958961755037308, dtype=float32), array(0.021206531673669815, dtype=float32), array(0.002938539255410433, dtype=float32)]\n",
      "0.00938920490444\n",
      "[[ 9.  4.  2.  9.  6.  5.]\n",
      " [ 9.  4.  6.  5.  5.  9.]]\n",
      "46000 [array(0.00012417847756296396, dtype=float32), array(0.0013746782205998898, dtype=float32), array(3.000357310156687e-06, dtype=float32)]\n",
      "0.00858545955271\n",
      "[[ 8.  2.  2.  4.  7.  8.]\n",
      " [ 8.  2.  2.  4.  7.  7.]]\n",
      "48000 [array(0.0019283227156847715, dtype=float32), array(0.0031821993179619312, dtype=float32), array(0.00010322989692213014, dtype=float32)]\n",
      "0.0086967702955\n",
      "[[ 6.  8.  3.  3.  2.  4.]\n",
      " [ 8.  8.  3.  2.  3.  7.]]\n",
      "50000 [array(0.004286819137632847, dtype=float32), array(0.0055449362844228745, dtype=float32), array(0.0016097049228847027, dtype=float32)]\n",
      "0.00865770783275\n",
      "[[ 2.  6.  6.  6.  3.  3.]\n",
      " [ 2.  6.  6.  6.  6.  6.]]\n",
      "50001 Increasing lag to:  51 seq_len is  6\n",
      "51001 Increasing lag to:  52 seq_len is  6\n",
      "52000 [array(0.0004364929045550525, dtype=float32), array(0.001699322834610939, dtype=float32), array(7.95749292592518e-06, dtype=float32)]\n",
      "0.00786231085658\n",
      "[[ 4.  2.  3.  9.  4.  8.]\n",
      " [ 4.  2.  3.  9.  4.  3.]]\n",
      "52001 Increasing lag to:  53 seq_len is  6\n",
      "53001 Increasing lag to:  54 seq_len is  6\n",
      "53304 Increasing lag to:  55 seq_len is  6\n",
      "54000 [array(1.5069662367750425e-05, dtype=float32), array(0.0012819789117202163, dtype=float32), array(1.342473776588804e-08, dtype=float32)]\n",
      "0.00769590958953\n",
      "[[ 3.  9.  2.  7.  7.  6.]\n",
      " [ 3.  9.  3.  7.  7.  7.]]\n",
      "55001 Increasing lag to:  56 seq_len is  6\n",
      "56000 [array(0.0015367388259619474, dtype=float32), array(0.0028064651414752007, dtype=float32), array(5.846292697242461e-05, dtype=float32)]\n",
      "0.00765090156347\n",
      "[[ 9.  9.  6.  6.  3.  7.]\n",
      " [ 9.  9.  6.  6.  3.  5.]]\n",
      "56001 Increasing lag to:  57 seq_len is  6\n",
      "57001 Increasing lag to:  58 seq_len is  6\n",
      "58000 [array(6.11794093856588e-05, dtype=float32), array(0.0013334586983546615, dtype=float32), array(4.5127666226107976e-07, dtype=float32)]\n",
      "0.00744016701356\n",
      "[[ 3.  2.  6.  4.  7.  2.]\n",
      " [ 3.  2.  6.  4.  4.  4.]]\n",
      "58001 Increasing lag to:  59 seq_len is  6\n",
      "58479 Increasing lag to:  60 seq_len is  7\n",
      "60000 [array(0.003072644118219614, dtype=float32), array(0.0043475693091750145, dtype=float32), array(0.0012029308127239347, dtype=float32)]\n",
      "0.00925373844802\n",
      "[[ 4.  2.  6.  3.  5.  3.  7.]\n",
      " [ 4.  6.  6.  3.  5.  3.  3.]]\n",
      "60001 Increasing lag to:  61 seq_len is  7\n",
      "61001 Increasing lag to:  62 seq_len is  7\n",
      "62000 [array(0.030831515789031982, dtype=float32), array(0.03210904076695442, dtype=float32), array(0.09896524995565414, dtype=float32)]\n",
      "0.0119250155985\n",
      "[[ 2.  8.  6.  8.  5.  8.  7.]\n",
      " [ 2.  8.  8.  5.  5.  5.  8.]]\n",
      "62001 Increasing lag to:  63 seq_len is  7\n",
      "63001 Increasing lag to:  64 seq_len is  7\n",
      "64000 [array(4.1038310882868245e-05, dtype=float32), array(0.001321327406913042, dtype=float32), array(3.010698890193453e-07, dtype=float32)]\n",
      "0.00883847661316\n",
      "[[ 9.  9.  7.  6.  7.  2.  7.]\n",
      " [ 9.  9.  7.  9.  7.  9.  9.]]\n",
      "64001 Increasing lag to:  65 seq_len is  7\n",
      "65001 Increasing lag to:  66 seq_len is  7\n",
      "66000 [array(0.01222815178334713, dtype=float32), array(0.013511596247553825, dtype=float32), array(0.0010908289114013314, dtype=float32)]\n",
      "0.00840279646218\n",
      "[[ 2.  2.  2.  9.  9.  7.  2.]\n",
      " [ 2.  2.  2.  9.  2.  7.  9.]]\n",
      "66001 Increasing lag to:  67 seq_len is  7\n",
      "67001 Increasing lag to:  68 seq_len is  7\n",
      "68000 [array(0.007761457469314337, dtype=float32), array(0.009046666324138641, dtype=float32), array(0.0002560533466748893, dtype=float32)]\n",
      "0.00796242151409\n",
      "[[ 2.  6.  2.  2.  5.  4.  2.]\n",
      " [ 2.  6.  2.  2.  7.  2.  2.]]\n",
      "68001 Increasing lag to:  69 seq_len is  7\n",
      "69001 Increasing lag to:  70 seq_len is  8\n",
      "70000 [array(0.00642196973785758, dtype=float32), array(0.007709255907684565, dtype=float32), array(0.00026734056882560253, dtype=float32)]\n",
      "0.00940844416618\n",
      "[[ 4.  7.  8.  7.  4.  9.  3.  7.]\n",
      " [ 4.  7.  8.  7.  4.  7.  4.  4.]]\n",
      "70001 Increasing lag to:  71 seq_len is  8\n",
      "71001 Increasing lag to:  72 seq_len is  8\n",
      "72000 [array(1.124162463383982e-05, dtype=float32), array(0.0013009447138756514, dtype=float32), array(1.1013791656466765e-08, dtype=float32)]\n",
      "0.0112601546571\n",
      "[[ 4.  3.  6.  7.  5.  3.  7.  4.]\n",
      " [ 4.  3.  4.  7.  5.  5.  5.  3.]]\n",
      "72001 Increasing lag to:  73 seq_len is  8\n",
      "73001 Increasing lag to:  74 seq_len is  8\n",
      "74000 [array(0.00021686172112822533, dtype=float32), array(0.0015099858865141869, dtype=float32), array(2.4749892872932833e-06, dtype=float32)]\n",
      "0.0093986839056\n",
      "[[ 9.  8.  4.  2.  3.  5.  3.  9.]\n",
      " [ 9.  8.  2.  3.  5.  5.  3.  2.]]\n",
      "74001 Increasing lag to:  75 seq_len is  8\n",
      "75001 Increasing lag to:  76 seq_len is  8\n",
      "76000 [array(0.010542734526097775, dtype=float32), array(0.011838645674288273, dtype=float32), array(0.0003936004941351712, dtype=float32)]\n",
      "0.00887993909419\n",
      "[[ 4.  7.  7.  7.  9.  9.  2.  7.]\n",
      " [ 4.  7.  7.  7.  9.  9.  9.  9.]]\n",
      "76001 Increasing lag to:  77 seq_len is  8\n",
      "77001 Increasing lag to:  78 seq_len is  8\n",
      "78000 [array(0.0010996464407071471, dtype=float32), array(0.0023977477103471756, dtype=float32), array(3.319361712783575e-05, dtype=float32)]\n",
      "0.00835447572172\n",
      "[[ 2.  5.  8.  3.  8.  6.  2.  7.]\n",
      " [ 2.  5.  8.  8.  8.  8.  8.  8.]]\n",
      "78001 Increasing lag to:  79 seq_len is  8\n",
      "79001 Increasing lag to:  80 seq_len is  9\n",
      "80000 [array(1.4356572137330659e-05, dtype=float32), array(0.0013159762602299452, dtype=float32), array(2.1921422899140453e-07, dtype=float32)]\n",
      "0.0103643015027\n",
      "[[ 9.  9.  4.  2.  9.  3.  2.  3.  3.]\n",
      " [ 9.  9.  9.  9.  3.  3.  9.  5.  3.]]\n",
      "80001 Increasing lag to:  81 seq_len is  9\n",
      "81001 Increasing lag to:  82 seq_len is  9\n",
      "82000 [array(0.003703885478898883, dtype=float32), array(0.005008566193282604, dtype=float32), array(0.00031125941313803196, dtype=float32)]\n",
      "0.00949848163873\n",
      "[[ 7.  6.  8.  2.  6.  6.  2.  9.  2.]\n",
      " [ 7.  6.  8.  6.  2.  2.  2.  2.  9.]]\n",
      "82001 Increasing lag to:  83 seq_len is  9\n",
      "83001 Increasing lag to:  84 seq_len is  9\n",
      "84000 [array(0.000202485010959208, dtype=float32), array(0.0015104163903743029, dtype=float32), array(2.107553882524371e-06, dtype=float32)]\n",
      "0.0106884362176\n",
      "[[ 3.  4.  9.  6.  8.  7.  6.  3.  7.]\n",
      " [ 3.  4.  9.  6.  6.  6.  5.  6.  6.]]\n",
      "84001 Increasing lag to:  85 seq_len is  9\n",
      "85001 Increasing lag to:  86 seq_len is  9\n",
      "86000 [array(0.010923530906438828, dtype=float32), array(0.012234198860824108, dtype=float32), array(0.0009331385954283178, dtype=float32)]\n",
      "0.00896879471838\n",
      "[[ 6.  7.  9.  9.  6.  5.  4.  8.  4.]\n",
      " [ 6.  7.  9.  6.  9.  5.  4.  6.  6.]]\n",
      "86001 Increasing lag to:  87 seq_len is  9\n",
      "87001 Increasing lag to:  88 seq_len is  9\n",
      "88000 [array(0.011748872697353363, dtype=float32), array(0.013062762096524239, dtype=float32), array(0.0010208323365077376, dtype=float32)]\n",
      "0.00964561849833\n",
      "[[ 7.  9.  9.  8.  9.  4.  8.  7.  8.]\n",
      " [ 2.  9.  9.  8.  8.  8.  8.  8.  5.]]\n",
      "88001 Increasing lag to:  89 seq_len is  9\n",
      "89001 Increasing lag to:  90 seq_len is  10\n",
      "90000 [array(0.00031929981196299195, dtype=float32), array(0.001635758439078927, dtype=float32), array(2.965044586744625e-06, dtype=float32)]\n",
      "0.0101934112608\n",
      "[[ 9.  2.  3.  6.  5.  9.  3.  8.  4.  3.]\n",
      " [ 9.  9.  3.  6.  5.  5.  6.  3.  3.  3.]]\n",
      "90001 Increasing lag to:  91 seq_len is  10\n",
      "91001 Increasing lag to:  92 seq_len is  10\n",
      "92000 [array(0.007443922571837902, dtype=float32), array(0.008761970326304436, dtype=float32), array(0.0008183385361917317, dtype=float32)]\n",
      "0.0095743406564\n",
      "[[ 7.  8.  8.  9.  3.  9.  2.  2.  5.  2.]\n",
      " [ 7.  8.  9.  9.  3.  9.  9.  9.  9.  3.]]\n",
      "92001 Increasing lag to:  93 seq_len is  10\n",
      "93001 Increasing lag to:  94 seq_len is  10\n",
      "94000 [array(0.035916492342948914, dtype=float32), array(0.037236690521240234, dtype=float32), array(0.00248180003836751, dtype=float32)]\n",
      "0.0100797200575\n",
      "[[ 4.  3.  5.  5.  9.  9.  6.  3.  5.  5.]\n",
      " [ 4.  3.  5.  5.  9.  9.  9.  9.  9.  9.]]\n",
      "94001 Increasing lag to:  95 seq_len is  10\n",
      "95001 Increasing lag to:  96 seq_len is  10\n",
      "96000 [array(0.00846423115581274, dtype=float32), array(0.009786305949091911, dtype=float32), array(0.0005750827258452773, dtype=float32)]\n",
      "0.00948428548872\n",
      "[[ 5.  4.  7.  4.  3.  9.  4.  6.  3.  9.]\n",
      " [ 5.  4.  7.  4.  3.  9.  6.  6.  6.  6.]]\n",
      "96001 Increasing lag to:  97 seq_len is  10\n",
      "97001 Increasing lag to:  98 seq_len is  10\n",
      "98000 [array(7.919292329461314e-06, dtype=float32), array(0.0013321572914719582, dtype=float32), array(2.522739173116406e-08, dtype=float32)]\n",
      "0.00970143452287\n",
      "[[ 5.  9.  9.  6.  6.  9.  7.  8.  8.  3.]\n",
      " [ 5.  9.  6.  6.  6.  3.  4.  3.  3.  3.]]\n",
      "98001 Increasing lag to:  99 seq_len is  10\n",
      "99001 Increasing lag to:  100 seq_len is  10\n",
      "100000 [array(0.0005369176506064832, dtype=float32), array(0.0018629035912454128, dtype=float32), array(1.8513279428589158e-05, dtype=float32)]\n",
      "0.00954133551568\n",
      "[[ 2.  9.  4.  7.  8.  2.  9.  5.  6.  8.]\n",
      " [ 2.  9.  4.  8.  8.  8.  8.  5.  5.  6.]]\n",
      "100001 Increasing lag to:  101 seq_len is  10\n",
      "101001 Increasing lag to:  102 seq_len is  10\n",
      "102000 [array(0.0016827064100652933, dtype=float32), array(0.003011574735864997, dtype=float32), array(7.172923506004736e-05, dtype=float32)]\n",
      "0.00851028691977\n",
      "[[ 7.  5.  6.  6.  9.  6.  6.  4.  3.  5.]\n",
      " [ 7.  5.  6.  6.  9.  6.  6.  6.  4.  4.]]\n",
      "102001 Increasing lag to:  103 seq_len is  10\n",
      "103001 Increasing lag to:  104 seq_len is  10\n",
      "104000 [array(7.882952195359394e-05, dtype=float32), array(0.0014093223726376891, dtype=float32), array(2.5620167889428558e-06, dtype=float32)]\n",
      "0.00830494053662\n",
      "[[ 8.  7.  3.  2.  4.  9.  8.  9.  3.  2.]\n",
      " [ 8.  7.  3.  2.  9.  4.  4.  4.  5.  3.]]\n",
      "104001 Increasing lag to:  105 seq_len is  10\n",
      "105001 Increasing lag to:  106 seq_len is  10\n",
      "106000 [array(3.1939860491547734e-05, dtype=float32), array(0.0013642964186146855, dtype=float32), array(8.251891614463602e-08, dtype=float32)]\n",
      "0.00843208283186\n",
      "[[ 9.  7.  2.  6.  6.  8.  8.  4.  3.  7.]\n",
      " [ 9.  7.  2.  6.  6.  6.  8.  8.  5.  5.]]\n",
      "106001 Increasing lag to:  107 seq_len is  10\n",
      "107001 Increasing lag to:  108 seq_len is  10\n",
      "108000 [array(0.011775458231568336, dtype=float32), array(0.013110737316310406, dtype=float32), array(0.0015438460977748036, dtype=float32)]\n",
      "0.0081684505567\n",
      "[[ 2.  2.  9.  9.  2.  6.  4.  5.  5.  2.]\n",
      " [ 2.  9.  9.  6.  9.  6.  6.  5.  5.  6.]]\n",
      "108001 Increasing lag to:  109 seq_len is  10\n",
      "109001 Increasing lag to:  110 seq_len is  10\n",
      "110000 [array(0.008432213217020035, dtype=float32), array(0.009768709540367126, dtype=float32), array(0.0008901780820451677, dtype=float32)]\n",
      "0.00792134273797\n",
      "[[ 8.  9.  9.  3.  9.  4.  8.  4.  9.  7.]\n",
      " [ 8.  9.  9.  3.  9.  4.  9.  3.  3.  3.]]\n",
      "110001 Increasing lag to:  111 seq_len is  10\n",
      "111001 Increasing lag to:  112 seq_len is  10\n",
      "112000 [array(0.0028730256017297506, dtype=float32), array(0.004210847895592451, dtype=float32), array(0.0005839982768520713, dtype=float32)]\n",
      "0.00768555048853\n",
      "[[ 7.  6.  9.  7.  4.  6.  4.  5.  5.  2.]\n",
      " [ 7.  6.  9.  7.  6.  6.  4.  5.  5.  9.]]\n",
      "112001 Increasing lag to:  113 seq_len is  10\n",
      "113001 Increasing lag to:  114 seq_len is  10\n",
      "114000 [array(0.03300352767109871, dtype=float32), array(0.03434276953339577, dtype=float32), array(0.21701014041900635, dtype=float32)]\n",
      "0.0104573341087\n",
      "[[ 9.  5.  3.  8.  6.  6.  3.  4.  3.  4.]\n",
      " [ 9.  5.  3.  6.  6.  6.  4.  5.  5.  9.]]\n",
      "114001 Increasing lag to:  115 seq_len is  10\n",
      "115001 Increasing lag to:  116 seq_len is  10\n",
      "116000 [array(0.011814732104539871, dtype=float32), array(0.013155711814761162, dtype=float32), array(0.0009128000237978995, dtype=float32)]\n",
      "0.00741957407445\n",
      "[[ 6.  6.  3.  2.  9.  5.  9.  9.  6.  4.]\n",
      " [ 6.  6.  3.  2.  9.  5.  9.  9.  4.  9.]]\n",
      "116001 Increasing lag to:  117 seq_len is  10\n",
      "117001 Increasing lag to:  118 seq_len is  10\n",
      "118000 [array(0.000939048535656184, dtype=float32), array(0.002280460437759757, dtype=float32), array(1.0651339835021645e-05, dtype=float32)]\n",
      "0.0071141012013\n",
      "[[ 2.  5.  2.  8.  7.  3.  8.  7.  2.  3.]\n",
      " [ 2.  5.  2.  8.  7.  3.  8.  8.  8.  8.]]\n",
      "118001 Increasing lag to:  119 seq_len is  10\n",
      "119001 Increasing lag to:  120 seq_len is  10\n",
      "120000 [array(0.001621900824829936, dtype=float32), array(0.00296401372179389, dtype=float32), array(0.00014842761447653174, dtype=float32)]\n",
      "0.00691804662347\n",
      "[[ 7.  7.  8.  6.  4.  4.  5.  8.  3.  2.]\n",
      " [ 7.  7.  8.  6.  4.  4.  5.  4.  4.  4.]]\n",
      "120001 Increasing lag to:  121 seq_len is  10\n",
      "121001 Increasing lag to:  122 seq_len is  10\n",
      "122000 [array(0.0009597362950444221, dtype=float32), array(0.0023024831898510456, dtype=float32), array(1.9818324290099554e-05, dtype=float32)]\n",
      "0.00662290211767\n",
      "[[ 2.  3.  8.  6.  4.  6.  4.  3.  5.  7.]\n",
      " [ 2.  3.  8.  6.  4.  6.  4.  4.  4.  5.]]\n",
      "122001 Increasing lag to:  123 seq_len is  10\n",
      "123001 Increasing lag to:  124 seq_len is  10\n",
      "124000 [array(0.0029976903460919857, dtype=float32), array(0.004339980892837048, dtype=float32), array(0.0006138054304756224, dtype=float32)]\n",
      "0.0067096510902\n",
      "[[ 7.  8.  6.  4.  4.  4.  6.  4.  5.  8.]\n",
      " [ 7.  8.  6.  4.  6.  4.  4.  4.  5.  5.]]\n",
      "124001 Increasing lag to:  125 seq_len is  10\n",
      "125001 Increasing lag to:  126 seq_len is  10\n",
      "126000 [array(0.010699791833758354, dtype=float32), array(0.012043051421642303, dtype=float32), array(0.0009325727005489171, dtype=float32)]\n",
      "0.00721906777471\n",
      "[[ 5.  7.  6.  7.  3.  4.  7.  9.  6.  9.]\n",
      " [ 5.  7.  4.  3.  3.  4.  9.  9.  9.  9.]]\n",
      "126001 Increasing lag to:  127 seq_len is  10\n",
      "126706 Increasing lag to:  128 seq_len is  10\n",
      "128000 [array(0.005524118430912495, dtype=float32), array(0.0068675922229886055, dtype=float32), array(0.0006557857850566506, dtype=float32)]\n",
      "0.00604412611574\n",
      "[[ 8.  3.  3.  3.  5.  8.  8.  3.  5.  7.]\n",
      " [ 8.  3.  3.  3.  5.  8.  5.  8.  5.  8.]]\n",
      "128001 Increasing lag to:  129 seq_len is  10\n",
      "129001 Increasing lag to:  130 seq_len is  10\n",
      "129907 Increasing lag to:  131 seq_len is  10\n",
      "129962 Increasing lag to:  132 seq_len is  10\n",
      "130000 [array(0.003986446186900139, dtype=float32), array(0.0053303842432796955, dtype=float32), array(0.0001911470026243478, dtype=float32)]\n",
      "0.00614517275244\n",
      "[[ 8.  2.  9.  4.  3.  6.  7.  6.  6.  8.]\n",
      " [ 8.  2.  6.  4.  3.  6.  4.  3.  3.  3.]]\n",
      "130056 Increasing lag to:  133 seq_len is  10\n",
      "130285 Increasing lag to:  134 seq_len is  10\n",
      "131184 Increasing lag to:  135 seq_len is  10\n",
      "131372 Increasing lag to:  136 seq_len is  10\n",
      "131838 Increasing lag to:  137 seq_len is  10\n",
      "132000 [array(0.006915973965078592, dtype=float32), array(0.008259430527687073, dtype=float32), array(0.0005129940109327435, dtype=float32)]\n",
      "0.00620548333973\n",
      "[[ 6.  4.  5.  7.  2.  6.  6.  9.  9.  7.]\n",
      " [ 6.  4.  5.  2.  7.  6.  9.  6.  2.  2.]]\n",
      "132209 Increasing lag to:  138 seq_len is  10\n",
      "132431 Increasing lag to:  139 seq_len is  10\n",
      "132790 Increasing lag to:  140 seq_len is  10\n",
      "132953 Increasing lag to:  141 seq_len is  10\n",
      "133025 Increasing lag to:  142 seq_len is  10\n",
      "133088 Increasing lag to:  143 seq_len is  10\n",
      "133129 Increasing lag to:  144 seq_len is  10\n",
      "133211 Increasing lag to:  145 seq_len is  10\n",
      "133422 Increasing lag to:  146 seq_len is  10\n",
      "133717 Increasing lag to:  147 seq_len is  10\n",
      "133743 Increasing lag to:  148 seq_len is  10\n",
      "133775 Increasing lag to:  149 seq_len is  10\n",
      "133809 Increasing lag to:  150 seq_len is  10\n",
      "133892 Increasing lag to:  151 seq_len is  10\n",
      "134000 [array(0.006223800592124462, dtype=float32), array(0.007565981708467007, dtype=float32), array(0.0011116445530205965, dtype=float32)]\n",
      "0.00538818817586\n",
      "[[ 6.  8.  6.  3.  6.  5.  2.  5.  9.  8.]\n",
      " [ 6.  8.  6.  3.  2.  5.  2.  2.  2.  2.]]\n",
      "134054 Increasing lag to:  152 seq_len is  10\n",
      "134149 Increasing lag to:  153 seq_len is  10\n",
      "134175 Increasing lag to:  154 seq_len is  10\n",
      "134224 Increasing lag to:  155 seq_len is  10\n",
      "134749 Increasing lag to:  156 seq_len is  10\n",
      "135020 Increasing lag to:  157 seq_len is  10\n",
      "135071 Increasing lag to:  158 seq_len is  10\n",
      "135240 Increasing lag to:  159 seq_len is  10\n",
      "135250 Increasing lag to:  160 seq_len is  10\n",
      "135333 Increasing lag to:  161 seq_len is  10\n",
      "135410 Increasing lag to:  162 seq_len is  10\n",
      "135437 Increasing lag to:  163 seq_len is  10\n",
      "135869 Increasing lag to:  164 seq_len is  10\n",
      "136000 [array(0.001073849038220942, dtype=float32), array(0.0024144724011421204, dtype=float32), array(0.00021940401347819716, dtype=float32)]\n",
      "0.00453982874751\n",
      "[[ 5.  2.  8.  5.  9.  6.  7.  9.  9.  7.]\n",
      " [ 5.  2.  8.  5.  9.  6.  7.  9.  9.  9.]]\n",
      "136001 Increasing lag to:  165 seq_len is  10\n",
      "136082 Increasing lag to:  166 seq_len is  10\n",
      "136115 Increasing lag to:  167 seq_len is  10\n",
      "136351 Increasing lag to:  168 seq_len is  10\n",
      "136637 Increasing lag to:  169 seq_len is  10\n",
      "136668 Increasing lag to:  170 seq_len is  10\n",
      "136727 Increasing lag to:  171 seq_len is  10\n",
      "136744 Increasing lag to:  172 seq_len is  10\n",
      "136833 Increasing lag to:  173 seq_len is  10\n",
      "136985 Increasing lag to:  174 seq_len is  10\n",
      "137217 Increasing lag to:  175 seq_len is  10\n",
      "137218 Increasing lag to:  176 seq_len is  10\n",
      "137343 Increasing lag to:  177 seq_len is  10\n",
      "137356 Increasing lag to:  178 seq_len is  10\n",
      "137479 Increasing lag to:  179 seq_len is  10\n",
      "137482 Increasing lag to:  180 seq_len is  10\n",
      "137603 Increasing lag to:  181 seq_len is  10\n",
      "137804 Increasing lag to:  182 seq_len is  10\n",
      "138000 [array(0.0031498975586146116, dtype=float32), array(0.004487467929720879, dtype=float32), array(0.00024407709133811295, dtype=float32)]\n",
      "0.00427276315168\n",
      "[[ 6.  8.  8.  7.  7.  3.  5.  6.  3.  8.]\n",
      " [ 6.  8.  8.  7.  7.  7.  5.  8.  8.  8.]]\n",
      "138069 Increasing lag to:  183 seq_len is  10\n",
      "138133 Increasing lag to:  184 seq_len is  10\n",
      "138210 Increasing lag to:  185 seq_len is  10\n",
      "138270 Increasing lag to:  186 seq_len is  10\n",
      "138330 Increasing lag to:  187 seq_len is  10\n",
      "138380 Increasing lag to:  188 seq_len is  10\n",
      "138458 Increasing lag to:  189 seq_len is  10\n",
      "138625 Increasing lag to:  190 seq_len is  10\n",
      "138743 Increasing lag to:  191 seq_len is  10\n",
      "139084 Increasing lag to:  192 seq_len is  10\n",
      "139283 Increasing lag to:  193 seq_len is  10\n",
      "139429 Increasing lag to:  194 seq_len is  10\n",
      "139505 Increasing lag to:  195 seq_len is  10\n",
      "139535 Increasing lag to:  196 seq_len is  10\n",
      "139804 Increasing lag to:  197 seq_len is  10\n",
      "139807 Increasing lag to:  198 seq_len is  10\n",
      "139845 Increasing lag to:  199 seq_len is  10\n",
      "139870 Increasing lag to:  200 seq_len is  10\n",
      "139911 Increasing lag to:  201 seq_len is  10\n",
      "139949 Increasing lag to:  202 seq_len is  10\n",
      "139968 Increasing lag to:  203 seq_len is  10\n",
      "139984 Increasing lag to:  204 seq_len is  10\n",
      "140000 [array(0.001429362571798265, dtype=float32), array(0.0027639316394925117, dtype=float32), array(6.185832899063826e-05, dtype=float32)]\n",
      "0.00389906810597\n",
      "[[ 5.  7.  6.  7.  6.  3.  4.  7.  7.  8.]\n",
      " [ 5.  7.  6.  3.  6.  7.  4.  5.  5.  4.]]\n",
      "140140 Increasing lag to:  205 seq_len is  10\n",
      "140166 Increasing lag to:  206 seq_len is  10\n",
      "140305 Increasing lag to:  207 seq_len is  10\n",
      "140400 Increasing lag to:  208 seq_len is  10\n",
      "140697 Increasing lag to:  209 seq_len is  10\n",
      "141089 Increasing lag to:  210 seq_len is  10\n",
      "141198 Increasing lag to:  211 seq_len is  10\n",
      "141260 Increasing lag to:  212 seq_len is  10\n",
      "141306 Increasing lag to:  213 seq_len is  10\n",
      "141309 Increasing lag to:  214 seq_len is  10\n",
      "141335 Increasing lag to:  215 seq_len is  10\n",
      "141464 Increasing lag to:  216 seq_len is  10\n",
      "141513 Increasing lag to:  217 seq_len is  10\n",
      "142000 [array(0.003513519885018468, dtype=float32), array(0.004844945389777422, dtype=float32), array(0.00015289802104234695, dtype=float32)]\n",
      "0.00368811236694\n",
      "[[ 4.  2.  8.  4.  3.  6.  5.  5.  5.  2.]\n",
      " [ 4.  2.  6.  4.  3.  6.  5.  5.  5.  5.]]\n",
      "142055 Increasing lag to:  218 seq_len is  10\n",
      "142162 Increasing lag to:  219 seq_len is  10\n",
      "142164 Increasing lag to:  220 seq_len is  10\n",
      "142338 Increasing lag to:  221 seq_len is  10\n",
      "142491 Increasing lag to:  222 seq_len is  10\n",
      "142639 Increasing lag to:  223 seq_len is  10\n",
      "142645 Increasing lag to:  224 seq_len is  10\n",
      "142776 Increasing lag to:  225 seq_len is  10\n",
      "142816 Increasing lag to:  226 seq_len is  10\n",
      "142866 Increasing lag to:  227 seq_len is  10\n",
      "143054 Increasing lag to:  228 seq_len is  10\n",
      "143118 Increasing lag to:  229 seq_len is  10\n",
      "143216 Increasing lag to:  230 seq_len is  10\n",
      "143241 Increasing lag to:  231 seq_len is  10\n",
      "143432 Increasing lag to:  232 seq_len is  10\n",
      "143469 Increasing lag to:  233 seq_len is  10\n",
      "143569 Increasing lag to:  234 seq_len is  10\n",
      "143752 Increasing lag to:  235 seq_len is  10\n",
      "143855 Increasing lag to:  236 seq_len is  10\n",
      "143921 Increasing lag to:  237 seq_len is  10\n",
      "143998 Increasing lag to:  238 seq_len is  10\n",
      "144000 [array(0.00885717198252678, dtype=float32), array(0.010184886865317822, dtype=float32), array(0.0011686676880344748, dtype=float32)]\n",
      "0.00349083985202\n",
      "[[ 6.  2.  9.  4.  7.  2.  3.  8.  9.  4.]\n",
      " [ 6.  2.  9.  4.  7.  2.  3.  2.  3.  5.]]\n",
      "144022 Increasing lag to:  239 seq_len is  10\n",
      "144236 Increasing lag to:  240 seq_len is  10\n",
      "144352 Increasing lag to:  241 seq_len is  10\n",
      "144354 Increasing lag to:  242 seq_len is  10\n",
      "144461 Increasing lag to:  243 seq_len is  10\n",
      "144487 Increasing lag to:  244 seq_len is  10\n",
      "144495 Increasing lag to:  245 seq_len is  10\n",
      "144585 Increasing lag to:  246 seq_len is  10\n",
      "144632 Increasing lag to:  247 seq_len is  10\n",
      "144750 Increasing lag to:  248 seq_len is  10\n",
      "144852 Increasing lag to:  249 seq_len is  10\n",
      "144984 Increasing lag to:  250 seq_len is  10\n",
      "145229 Increasing lag to:  251 seq_len is  10\n",
      "145292 Increasing lag to:  252 seq_len is  10\n",
      "145299 Increasing lag to:  253 seq_len is  10\n",
      "145307 Increasing lag to:  254 seq_len is  10\n",
      "145399 Increasing lag to:  255 seq_len is  10\n",
      "145510 Increasing lag to:  256 seq_len is  10\n",
      "145547 Increasing lag to:  257 seq_len is  10\n",
      "145672 Increasing lag to:  258 seq_len is  10\n",
      "145717 Increasing lag to:  259 seq_len is  10\n",
      "145771 Increasing lag to:  260 seq_len is  10\n",
      "145870 Increasing lag to:  261 seq_len is  10\n",
      "145996 Increasing lag to:  262 seq_len is  10\n",
      "146000 [array(0.0011858195066452026, dtype=float32), array(0.0025091771967709064, dtype=float32), array(7.587778236484155e-05, dtype=float32)]\n",
      "0.00309889530763\n",
      "[[ 4.  2.  7.  9.  4.  6.  8.  2.  5.  6.]\n",
      " [ 4.  2.  7.  9.  6.  6.  6.  6.  5.  5.]]\n",
      "146211 Increasing lag to:  263 seq_len is  10\n",
      "146241 Increasing lag to:  264 seq_len is  10\n",
      "146281 Increasing lag to:  265 seq_len is  10\n",
      "146322 Increasing lag to:  266 seq_len is  10\n",
      "146352 Increasing lag to:  267 seq_len is  10\n",
      "146432 Increasing lag to:  268 seq_len is  10\n",
      "146486 Increasing lag to:  269 seq_len is  10\n",
      "146531 Increasing lag to:  270 seq_len is  10\n",
      "146559 Increasing lag to:  271 seq_len is  10\n",
      "146932 Increasing lag to:  272 seq_len is  10\n",
      "146974 Increasing lag to:  273 seq_len is  10\n",
      "146994 Increasing lag to:  274 seq_len is  10\n",
      "147032 Increasing lag to:  275 seq_len is  10\n",
      "147056 Increasing lag to:  276 seq_len is  10\n",
      "147154 Increasing lag to:  277 seq_len is  10\n",
      "147226 Increasing lag to:  278 seq_len is  10\n",
      "147387 Increasing lag to:  279 seq_len is  10\n",
      "147401 Increasing lag to:  280 seq_len is  10\n",
      "147440 Increasing lag to:  281 seq_len is  10\n",
      "147485 Increasing lag to:  282 seq_len is  10\n",
      "147493 Increasing lag to:  283 seq_len is  10\n",
      "147597 Increasing lag to:  284 seq_len is  10\n",
      "147609 Increasing lag to:  285 seq_len is  10\n",
      "147691 Increasing lag to:  286 seq_len is  10\n",
      "147765 Increasing lag to:  287 seq_len is  10\n",
      "147836 Increasing lag to:  288 seq_len is  10\n",
      "147899 Increasing lag to:  289 seq_len is  10\n",
      "147998 Increasing lag to:  290 seq_len is  10\n",
      "148000 [array(0.0024787457659840584, dtype=float32), array(0.0037976752500981092, dtype=float32), array(5.4404463298851624e-05, dtype=float32)]\n",
      "0.00291475933045\n",
      "[[ 7.  4.  4.  3.  3.  4.  2.  9.  9.  5.]\n",
      " [ 7.  4.  4.  3.  3.  6.  9.  3.  3.  9.]]\n",
      "148147 Increasing lag to:  291 seq_len is  10\n",
      "148150 Increasing lag to:  292 seq_len is  10\n",
      "148199 Increasing lag to:  293 seq_len is  10\n",
      "148457 Increasing lag to:  294 seq_len is  10\n",
      "148526 Increasing lag to:  295 seq_len is  10\n",
      "148625 Increasing lag to:  296 seq_len is  10\n",
      "148981 Increasing lag to:  297 seq_len is  10\n",
      "149013 Increasing lag to:  298 seq_len is  10\n",
      "149028 Increasing lag to:  299 seq_len is  10\n",
      "149111 Increasing lag to:  300 seq_len is  10\n",
      "149272 Increasing lag to:  301 seq_len is  10\n",
      "149423 Increasing lag to:  302 seq_len is  10\n",
      "149460 Increasing lag to:  303 seq_len is  10\n",
      "149519 Increasing lag to:  304 seq_len is  10\n",
      "149636 Increasing lag to:  305 seq_len is  10\n",
      "149651 Increasing lag to:  306 seq_len is  10\n",
      "149996 Increasing lag to:  307 seq_len is  10\n",
      "150000 [array(0.003130513010546565, dtype=float32), array(0.004444282501935959, dtype=float32), array(0.00015318072109948844, dtype=float32)]\n",
      "0.00255053117871\n",
      "[[ 4.  3.  5.  6.  7.  4.  3.  8.  3.  2.]\n",
      " [ 4.  3.  5.  6.  7.  4.  3.  6.  3.  3.]]\n",
      "150019 Increasing lag to:  308 seq_len is  10\n",
      "150050 Increasing lag to:  309 seq_len is  10\n",
      "150254 Increasing lag to:  310 seq_len is  10\n",
      "150295 Increasing lag to:  311 seq_len is  10\n",
      "150351 Increasing lag to:  312 seq_len is  10\n",
      "150364 Increasing lag to:  313 seq_len is  10\n",
      "150534 Increasing lag to:  314 seq_len is  10\n",
      "150594 Increasing lag to:  315 seq_len is  10\n",
      "150980 Increasing lag to:  316 seq_len is  10\n",
      "151231 Increasing lag to:  317 seq_len is  10\n",
      "151236 Increasing lag to:  318 seq_len is  10\n",
      "151250 Increasing lag to:  319 seq_len is  10\n",
      "151307 Increasing lag to:  320 seq_len is  10\n",
      "151403 Increasing lag to:  321 seq_len is  10\n",
      "151425 Increasing lag to:  322 seq_len is  10\n",
      "151451 Increasing lag to:  323 seq_len is  10\n",
      "151464 Increasing lag to:  324 seq_len is  10\n",
      "151508 Increasing lag to:  325 seq_len is  10\n",
      "151643 Increasing lag to:  326 seq_len is  10\n",
      "151751 Increasing lag to:  327 seq_len is  10\n",
      "151795 Increasing lag to:  328 seq_len is  10\n",
      "151828 Increasing lag to:  329 seq_len is  10\n",
      "151930 Increasing lag to:  330 seq_len is  10\n",
      "152000 [array(0.003915294073522091, dtype=float32), array(0.005223083309829235, dtype=float32), array(0.0001263012527488172, dtype=float32)]\n",
      "0.00247005699202\n",
      "[[ 6.  4.  2.  6.  3.  2.  3.  4.  6.  5.]\n",
      " [ 6.  4.  2.  6.  3.  6.  3.  6.  5.  5.]]\n",
      "152051 Increasing lag to:  331 seq_len is  10\n",
      "152070 Increasing lag to:  332 seq_len is  10\n",
      "152118 Increasing lag to:  333 seq_len is  10\n",
      "152128 Increasing lag to:  334 seq_len is  10\n",
      "152185 Increasing lag to:  335 seq_len is  10\n",
      "152198 Increasing lag to:  336 seq_len is  10\n",
      "152418 Increasing lag to:  337 seq_len is  10\n",
      "152436 Increasing lag to:  338 seq_len is  10\n",
      "152492 Increasing lag to:  339 seq_len is  10\n",
      "152533 Increasing lag to:  340 seq_len is  10\n",
      "152544 Increasing lag to:  341 seq_len is  10\n",
      "152793 Increasing lag to:  342 seq_len is  10\n",
      "153237 Increasing lag to:  343 seq_len is  10\n",
      "153239 Increasing lag to:  344 seq_len is  10\n",
      "153274 Increasing lag to:  345 seq_len is  10\n",
      "153375 Increasing lag to:  346 seq_len is  10\n",
      "153444 Increasing lag to:  347 seq_len is  10\n",
      "153463 Increasing lag to:  348 seq_len is  10\n",
      "153852 Increasing lag to:  349 seq_len is  10\n",
      "153913 Increasing lag to:  350 seq_len is  10\n",
      "153953 Increasing lag to:  351 seq_len is  10\n",
      "154000 [array(0.00012301701644901186, dtype=float32), array(0.0014243504265323281, dtype=float32), array(1.5944273172863177e-06, dtype=float32)]\n",
      "0.0022069294937\n",
      "[[ 3.  6.  8.  5.  2.  3.  9.  2.  8.  4.]\n",
      " [ 3.  6.  8.  5.  2.  2.  3.  2.  7.  2.]]\n",
      "154001 Increasing lag to:  352 seq_len is  10\n",
      "154213 Increasing lag to:  353 seq_len is  10\n",
      "154265 Increasing lag to:  354 seq_len is  10\n",
      "154345 Increasing lag to:  355 seq_len is  10\n",
      "154369 Increasing lag to:  356 seq_len is  10\n",
      "154447 Increasing lag to:  357 seq_len is  10\n",
      "154461 Increasing lag to:  358 seq_len is  10\n",
      "154614 Increasing lag to:  359 seq_len is  10\n",
      "154622 Increasing lag to:  360 seq_len is  10\n",
      "154627 Increasing lag to:  361 seq_len is  10\n",
      "154670 Increasing lag to:  362 seq_len is  10\n",
      "154698 Increasing lag to:  363 seq_len is  10\n",
      "154718 Increasing lag to:  364 seq_len is  10\n",
      "154752 Increasing lag to:  365 seq_len is  10\n",
      "154836 Increasing lag to:  366 seq_len is  10\n",
      "154944 Increasing lag to:  367 seq_len is  10\n",
      "155091 Increasing lag to:  368 seq_len is  10\n",
      "155166 Increasing lag to:  369 seq_len is  10\n",
      "155312 Increasing lag to:  370 seq_len is  10\n",
      "155333 Increasing lag to:  371 seq_len is  10\n",
      "155375 Increasing lag to:  372 seq_len is  10\n",
      "155379 Increasing lag to:  373 seq_len is  10\n",
      "155622 Increasing lag to:  374 seq_len is  10\n",
      "155684 Increasing lag to:  375 seq_len is  10\n",
      "155693 Increasing lag to:  376 seq_len is  10\n",
      "155734 Increasing lag to:  377 seq_len is  10\n",
      "155863 Increasing lag to:  378 seq_len is  10\n",
      "155994 Increasing lag to:  379 seq_len is  10\n",
      "156000 [array(0.000809389806818217, dtype=float32), array(0.0021062251180410385, dtype=float32), array(0.00021004327572882175, dtype=float32)]\n",
      "0.00222737994045\n",
      "[[ 5.  9.  8.  3.  8.  5.  2.  6.  4.  5.]\n",
      " [ 5.  9.  8.  3.  8.  3.  8.  3.  7.  2.]]\n",
      "156251 Increasing lag to:  380 seq_len is  10\n",
      "156327 Increasing lag to:  381 seq_len is  10\n",
      "156569 Increasing lag to:  382 seq_len is  10\n",
      "156575 Increasing lag to:  383 seq_len is  10\n",
      "156640 Increasing lag to:  384 seq_len is  10\n",
      "156754 Increasing lag to:  385 seq_len is  10\n",
      "156905 Increasing lag to:  386 seq_len is  10\n",
      "157057 Increasing lag to:  387 seq_len is  10\n",
      "157184 Increasing lag to:  388 seq_len is  10\n",
      "157210 Increasing lag to:  389 seq_len is  10\n",
      "157299 Increasing lag to:  390 seq_len is  10\n",
      "157305 Increasing lag to:  391 seq_len is  10\n",
      "157389 Increasing lag to:  392 seq_len is  10\n",
      "157525 Increasing lag to:  393 seq_len is  10\n",
      "157778 Increasing lag to:  394 seq_len is  10\n",
      "157840 Increasing lag to:  395 seq_len is  10\n",
      "157858 Increasing lag to:  396 seq_len is  10\n",
      "157865 Increasing lag to:  397 seq_len is  10\n",
      "157950 Increasing lag to:  398 seq_len is  10\n",
      "158000 [array(0.0012636409373953938, dtype=float32), array(0.0025545689277350903, dtype=float32), array(1.98473826458212e-05, dtype=float32)]\n",
      "0.00198988430202\n",
      "[[ 3.  7.  8.  3.  7.  9.  3.  4.  6.  5.]\n",
      " [ 3.  7.  8.  7.  7.  9.  7.  7.  9.  5.]]\n",
      "158187 Increasing lag to:  399 seq_len is  10\n",
      "158324 Increasing lag to:  400 seq_len is  10\n",
      "158342 Increasing lag to:  401 seq_len is  10\n",
      "158360 Increasing lag to:  402 seq_len is  10\n",
      "158544 Increasing lag to:  403 seq_len is  10\n",
      "158823 Increasing lag to:  404 seq_len is  10\n",
      "158855 Increasing lag to:  405 seq_len is  10\n",
      "158891 Increasing lag to:  406 seq_len is  10\n",
      "159005 Increasing lag to:  407 seq_len is  10\n",
      "159176 Increasing lag to:  408 seq_len is  10\n",
      "159292 Increasing lag to:  409 seq_len is  10\n",
      "159304 Increasing lag to:  410 seq_len is  10\n",
      "159352 Increasing lag to:  411 seq_len is  10\n",
      "159454 Increasing lag to:  412 seq_len is  10\n",
      "159506 Increasing lag to:  413 seq_len is  10\n",
      "159536 Increasing lag to:  414 seq_len is  10\n",
      "159596 Increasing lag to:  415 seq_len is  10\n",
      "159697 Increasing lag to:  416 seq_len is  10\n",
      "159868 Increasing lag to:  417 seq_len is  10\n",
      "159944 Increasing lag to:  418 seq_len is  10\n",
      "159955 Increasing lag to:  419 seq_len is  10\n",
      "159958 Increasing lag to:  420 seq_len is  10\n",
      "159962 Increasing lag to:  421 seq_len is  10\n",
      "160000 [array(5.2173803851474077e-05, dtype=float32), array(0.0013371246168389916, dtype=float32), array(3.674318236335239e-07, dtype=float32)]\n",
      "0.00222116429359\n",
      "[[ 4.  9.  9.  5.  7.  6.  6.  8.  3.  4.]\n",
      " [ 9.  9.  9.  5.  7.  6.  6.  8.  8.  3.]]\n",
      "160001 Increasing lag to:  422 seq_len is  10\n",
      "160122 Increasing lag to:  423 seq_len is  10\n",
      "160137 Increasing lag to:  424 seq_len is  10\n",
      "160149 Increasing lag to:  425 seq_len is  10\n",
      "160212 Increasing lag to:  426 seq_len is  10\n",
      "160332 Increasing lag to:  427 seq_len is  10\n",
      "160401 Increasing lag to:  428 seq_len is  10\n",
      "160420 Increasing lag to:  429 seq_len is  10\n",
      "160424 Increasing lag to:  430 seq_len is  10\n",
      "160537 Increasing lag to:  431 seq_len is  10\n",
      "160610 Increasing lag to:  432 seq_len is  10\n",
      "160703 Increasing lag to:  433 seq_len is  10\n",
      "160862 Increasing lag to:  434 seq_len is  10\n",
      "160972 Increasing lag to:  435 seq_len is  10\n",
      "160994 Increasing lag to:  436 seq_len is  10\n",
      "161063 Increasing lag to:  437 seq_len is  10\n",
      "161143 Increasing lag to:  438 seq_len is  10\n",
      "161234 Increasing lag to:  439 seq_len is  10\n",
      "161403 Increasing lag to:  440 seq_len is  10\n",
      "161609 Increasing lag to:  441 seq_len is  10\n",
      "161625 Increasing lag to:  442 seq_len is  10\n",
      "161715 Increasing lag to:  443 seq_len is  10\n",
      "161736 Increasing lag to:  444 seq_len is  10\n",
      "161746 Increasing lag to:  445 seq_len is  10\n",
      "161824 Increasing lag to:  446 seq_len is  10\n",
      "161927 Increasing lag to:  447 seq_len is  10\n",
      "162000 [array(0.002373607363551855, dtype=float32), array(0.003652589861303568, dtype=float32), array(3.2401367207057774e-05, dtype=float32)]\n",
      "0.00191638548858\n",
      "[[ 5.  6.  7.  6.  3.  3.  4.  3.  3.  3.]\n",
      " [ 5.  6.  7.  6.  3.  3.  6.  3.  9.  9.]]\n",
      "162167 Increasing lag to:  448 seq_len is  10\n",
      "162179 Increasing lag to:  449 seq_len is  10\n",
      "162244 Increasing lag to:  450 seq_len is  10\n",
      "162252 Increasing lag to:  451 seq_len is  10\n",
      "162588 Increasing lag to:  452 seq_len is  10\n",
      "162779 Increasing lag to:  453 seq_len is  10\n",
      "162785 Increasing lag to:  454 seq_len is  10\n",
      "162840 Increasing lag to:  455 seq_len is  10\n",
      "163166 Increasing lag to:  456 seq_len is  10\n",
      "163167 Increasing lag to:  457 seq_len is  10\n",
      "163357 Increasing lag to:  458 seq_len is  10\n",
      "163447 Increasing lag to:  459 seq_len is  10\n",
      "163591 Increasing lag to:  460 seq_len is  10\n",
      "163628 Increasing lag to:  461 seq_len is  10\n",
      "163690 Increasing lag to:  462 seq_len is  10\n",
      "163691 Increasing lag to:  463 seq_len is  10\n",
      "163753 Increasing lag to:  464 seq_len is  10\n",
      "163797 Increasing lag to:  465 seq_len is  10\n",
      "163878 Increasing lag to:  466 seq_len is  10\n",
      "163881 Increasing lag to:  467 seq_len is  10\n",
      "164000 [array(0.001144082867540419, dtype=float32), array(0.0024163941852748394, dtype=float32), array(5.1097904361085966e-05, dtype=float32)]\n",
      "0.00186204107013\n",
      "[[ 7.  5.  9.  9.  5.  5.  9.  2.  2.  8.]\n",
      " [ 7.  5.  9.  9.  5.  5.  2.  2.  2.  2.]]\n",
      "164434 Increasing lag to:  468 seq_len is  10\n",
      "164545 Increasing lag to:  469 seq_len is  10\n",
      "164652 Increasing lag to:  470 seq_len is  10\n",
      "164757 Increasing lag to:  471 seq_len is  10\n",
      "164944 Increasing lag to:  472 seq_len is  10\n",
      "164967 Increasing lag to:  473 seq_len is  10\n",
      "164969 Increasing lag to:  474 seq_len is  10\n",
      "165208 Increasing lag to:  475 seq_len is  10\n",
      "165327 Increasing lag to:  476 seq_len is  10\n",
      "165639 Increasing lag to:  477 seq_len is  10\n",
      "165659 Increasing lag to:  478 seq_len is  10\n",
      "165754 Increasing lag to:  479 seq_len is  10\n",
      "165776 Increasing lag to:  480 seq_len is  10\n",
      "165800 Increasing lag to:  481 seq_len is  10\n",
      "165822 Increasing lag to:  482 seq_len is  10\n",
      "165899 Increasing lag to:  483 seq_len is  10\n",
      "165927 Increasing lag to:  484 seq_len is  10\n",
      "166000 [array(0.0007112922612577677, dtype=float32), array(0.001977595267817378, dtype=float32), array(3.191531141055748e-05, dtype=float32)]\n",
      "0.0018500930164\n",
      "[[ 8.  6.  2.  9.  8.  4.  6.  6.  2.  3.]\n",
      " [ 8.  8.  2.  2.  6.  4.  2.  2.  3.  3.]]\n",
      "166094 Increasing lag to:  485 seq_len is  10\n",
      "166200 Increasing lag to:  486 seq_len is  10\n",
      "166338 Increasing lag to:  487 seq_len is  10\n",
      "166353 Increasing lag to:  488 seq_len is  10\n",
      "166394 Increasing lag to:  489 seq_len is  10\n",
      "166462 Increasing lag to:  490 seq_len is  10\n",
      "166519 Increasing lag to:  491 seq_len is  10\n",
      "166660 Increasing lag to:  492 seq_len is  10\n",
      "166772 Increasing lag to:  493 seq_len is  10\n",
      "167093 Increasing lag to:  494 seq_len is  10\n",
      "167129 Increasing lag to:  495 seq_len is  10\n",
      "167193 Increasing lag to:  496 seq_len is  10\n",
      "167248 Increasing lag to:  497 seq_len is  10\n",
      "167308 Increasing lag to:  498 seq_len is  10\n",
      "167327 Increasing lag to:  499 seq_len is  10\n",
      "167409 Increasing lag to:  500 seq_len is  10\n",
      "167475 Increasing lag to:  500 seq_len is  10\n",
      "167640 Increasing lag to:  500 seq_len is  10\n",
      "167842 Increasing lag to:  500 seq_len is  10\n",
      "167862 Increasing lag to:  500 seq_len is  10\n",
      "168000 [array(0.015748508274555206, dtype=float32), array(0.01700841262936592, dtype=float32), array(0.0019131031585857272, dtype=float32)]\n",
      "0.00176342239138\n",
      "[[ 4.  5.  9.  3.  9.  6.  9.  6.  5.  7.]\n",
      " [ 4.  5.  9.  3.  9.  9.  6.  3.  3.  6.]]\n",
      "168147 Increasing lag to:  500 seq_len is  10\n",
      "168511 Increasing lag to:  500 seq_len is  10\n",
      "168614 Increasing lag to:  500 seq_len is  10\n",
      "168697 Increasing lag to:  500 seq_len is  10\n",
      "168731 Increasing lag to:  500 seq_len is  10\n",
      "168906 Increasing lag to:  500 seq_len is  10\n",
      "169117 Increasing lag to:  500 seq_len is  10\n",
      "169136 Increasing lag to:  500 seq_len is  10\n",
      "169138 Increasing lag to:  500 seq_len is  10\n",
      "169233 Increasing lag to:  500 seq_len is  10\n",
      "169277 Increasing lag to:  500 seq_len is  10\n",
      "169285 Increasing lag to:  500 seq_len is  10\n",
      "169341 Increasing lag to:  500 seq_len is  10\n",
      "169347 Increasing lag to:  500 seq_len is  10\n",
      "169386 Increasing lag to:  500 seq_len is  10\n",
      "169464 Increasing lag to:  500 seq_len is  10\n",
      "169646 Increasing lag to:  500 seq_len is  10\n",
      "169679 Increasing lag to:  500 seq_len is  10\n",
      "169833 Increasing lag to:  500 seq_len is  10\n",
      "169865 Increasing lag to:  500 seq_len is  10\n",
      "169871 Increasing lag to:  500 seq_len is  10\n",
      "170000 [array(0.00048011456965468824, dtype=float32), array(0.001733229961246252, dtype=float32), array(5.94008224652498e-06, dtype=float32)]\n",
      "0.00182755454443\n",
      "[[ 8.  4.  5.  4.  9.  7.  3.  5.  3.  7.]\n",
      " [ 8.  4.  5.  9.  9.  7.  3.  9.  9.  9.]]\n",
      "170054 Increasing lag to:  500 seq_len is  10\n",
      "170327 Increasing lag to:  500 seq_len is  10\n",
      "170346 Increasing lag to:  500 seq_len is  10\n",
      "170408 Increasing lag to:  500 seq_len is  10\n",
      "170572 Increasing lag to:  500 seq_len is  10\n",
      "170577 Increasing lag to:  500 seq_len is  10\n",
      "170680 Increasing lag to:  500 seq_len is  10\n",
      "170844 Increasing lag to:  500 seq_len is  10\n",
      "170870 Increasing lag to:  500 seq_len is  10\n",
      "171145 Increasing lag to:  500 seq_len is  10\n",
      "171217 Increasing lag to:  500 seq_len is  10\n",
      "171361 Increasing lag to:  500 seq_len is  10\n",
      "171446 Increasing lag to:  500 seq_len is  10\n",
      "171546 Increasing lag to:  500 seq_len is  10\n",
      "171622 Increasing lag to:  500 seq_len is  10\n",
      "171698 Increasing lag to:  500 seq_len is  10\n",
      "171911 Increasing lag to:  500 seq_len is  10\n",
      "172000 [array(0.00028198555810377, dtype=float32), array(0.0015288980212062597, dtype=float32), array(2.078995066767675e-06, dtype=float32)]\n",
      "0.00167688762303\n",
      "[[ 8.  3.  9.  6.  7.  7.  3.  8.  8.  5.]\n",
      " [ 8.  3.  6.  6.  7.  7.  4.  8.  8.  5.]]\n",
      "172144 Increasing lag to:  500 seq_len is  10\n",
      "172268 Increasing lag to:  500 seq_len is  10\n",
      "172270 Increasing lag to:  500 seq_len is  10\n",
      "172334 Increasing lag to:  500 seq_len is  10\n",
      "172385 Increasing lag to:  500 seq_len is  10\n",
      "172481 Increasing lag to:  500 seq_len is  10\n",
      "172576 Increasing lag to:  500 seq_len is  10\n",
      "172613 Increasing lag to:  500 seq_len is  10\n",
      "172662 Increasing lag to:  500 seq_len is  10\n",
      "172697 Increasing lag to:  500 seq_len is  10\n",
      "172851 Increasing lag to:  500 seq_len is  10\n",
      "172955 Increasing lag to:  500 seq_len is  10\n",
      "172977 Increasing lag to:  500 seq_len is  10\n",
      "173003 Increasing lag to:  500 seq_len is  10\n",
      "173017 Increasing lag to:  500 seq_len is  10\n",
      "173055 Increasing lag to:  500 seq_len is  10\n",
      "173089 Increasing lag to:  500 seq_len is  10\n",
      "173137 Increasing lag to:  500 seq_len is  10\n",
      "173206 Increasing lag to:  500 seq_len is  10\n",
      "173233 Increasing lag to:  500 seq_len is  10\n",
      "173358 Increasing lag to:  500 seq_len is  10\n",
      "173394 Increasing lag to:  500 seq_len is  10\n",
      "173419 Increasing lag to:  500 seq_len is  10\n",
      "173430 Increasing lag to:  500 seq_len is  10\n",
      "173445 Increasing lag to:  500 seq_len is  10\n",
      "173451 Increasing lag to:  500 seq_len is  10\n",
      "173467 Increasing lag to:  500 seq_len is  10\n",
      "173594 Increasing lag to:  500 seq_len is  10\n",
      "173621 Increasing lag to:  500 seq_len is  10\n",
      "173678 Increasing lag to:  500 seq_len is  10\n",
      "173689 Increasing lag to:  500 seq_len is  10\n",
      "173771 Increasing lag to:  500 seq_len is  10\n",
      "173875 Increasing lag to:  500 seq_len is  10\n",
      "173901 Increasing lag to:  500 seq_len is  10\n",
      "173950 Increasing lag to:  500 seq_len is  10\n",
      "174000 [array(0.0010182668920606375, dtype=float32), array(0.002258460968732834, dtype=float32), array(4.340375744504854e-05, dtype=float32)]\n",
      "0.00176442658994\n",
      "[[ 7.  4.  3.  8.  8.  9.  6.  3.  6.  2.]\n",
      " [ 7.  4.  3.  8.  9.  9.  9.  3.  9.  9.]]\n",
      "174066 Increasing lag to:  500 seq_len is  10\n",
      "174109 Increasing lag to:  500 seq_len is  10\n",
      "174217 Increasing lag to:  500 seq_len is  10\n",
      "174554 Increasing lag to:  500 seq_len is  10\n",
      "174599 Increasing lag to:  500 seq_len is  10\n",
      "174799 Increasing lag to:  500 seq_len is  10\n",
      "174944 Increasing lag to:  500 seq_len is  10\n",
      "174968 Increasing lag to:  500 seq_len is  10\n",
      "175059 Increasing lag to:  500 seq_len is  10\n",
      "175586 Increasing lag to:  500 seq_len is  10\n",
      "175668 Increasing lag to:  500 seq_len is  10\n",
      "175675 Increasing lag to:  500 seq_len is  10\n",
      "175865 Increasing lag to:  500 seq_len is  10\n",
      "175890 Increasing lag to:  500 seq_len is  10\n",
      "176000 [array(0.003507991787046194, dtype=float32), array(0.004742366727441549, dtype=float32), array(0.00011854931653942913, dtype=float32)]\n",
      "0.0016885518562\n",
      "[[ 2.  5.  3.  4.  9.  8.  2.  3.  4.  9.]\n",
      " [ 2.  5.  3.  4.  9.  8.  8.  3.  9.  9.]]\n",
      "176115 Increasing lag to:  500 seq_len is  10\n",
      "176135 Increasing lag to:  500 seq_len is  10\n",
      "176247 Increasing lag to:  500 seq_len is  10\n",
      "176261 Increasing lag to:  500 seq_len is  10\n",
      "176413 Increasing lag to:  500 seq_len is  10\n",
      "176482 Increasing lag to:  500 seq_len is  10\n",
      "176697 Increasing lag to:  500 seq_len is  10\n",
      "176868 Increasing lag to:  500 seq_len is  10\n",
      "176936 Increasing lag to:  500 seq_len is  10\n",
      "176993 Increasing lag to:  500 seq_len is  10\n",
      "177070 Increasing lag to:  500 seq_len is  10\n",
      "177285 Increasing lag to:  500 seq_len is  10\n",
      "177335 Increasing lag to:  500 seq_len is  10\n",
      "177430 Increasing lag to:  500 seq_len is  10\n",
      "177452 Increasing lag to:  500 seq_len is  10\n",
      "177566 Increasing lag to:  500 seq_len is  10\n",
      "177732 Increasing lag to:  500 seq_len is  10\n",
      "177755 Increasing lag to:  500 seq_len is  10\n",
      "177828 Increasing lag to:  500 seq_len is  10\n",
      "177829 Increasing lag to:  500 seq_len is  10\n",
      "177835 Increasing lag to:  500 seq_len is  10\n",
      "177862 Increasing lag to:  500 seq_len is  10\n",
      "177865 Increasing lag to:  500 seq_len is  10\n",
      "177914 Increasing lag to:  500 seq_len is  10\n",
      "178000 [array(0.0022872844710946083, dtype=float32), array(0.0035158032551407814, dtype=float32), array(0.0001167871305369772, dtype=float32)]\n",
      "0.00181876344141\n",
      "[[ 8.  2.  6.  2.  3.  6.  9.  6.  8.  7.]\n",
      " [ 8.  2.  6.  6.  3.  6.  6.  6.  7.  7.]]\n",
      "178024 Increasing lag to:  500 seq_len is  10\n",
      "178028 Increasing lag to:  500 seq_len is  10\n",
      "178076 Increasing lag to:  500 seq_len is  10\n",
      "178132 Increasing lag to:  500 seq_len is  10\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-bac694a2a2a6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[1;31m# You can set the batch size here (or leave 32, which seems reasonable)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m     \u001b[0mXc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mYc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgen_copy_example\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mthis_lag\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthis_len\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m     \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcopy_trainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mYc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-8b5e0ee2c55d>\u001b[0m in \u001b[0;36mgen_copy_example\u001b[1;34m(T, seq_len, batchsize)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mencode_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencode_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-8b5e0ee2c55d>\u001b[0m in \u001b[0;36mencode_matrix\u001b[1;34m(matrix, num_classes)\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m             \u001b[0mencoded_matrix\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclass_encoding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mencoded_matrix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"float32\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-8b5e0ee2c55d>\u001b[0m in \u001b[0;36mclass_encoding\u001b[1;34m(n, num_classes)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mclass_encoding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_classes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mret\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "copy_net.initialize()\n",
    "\n",
    "# Feel free to tune the learing rate, this one worked the best for me. \n",
    "# We don't use any learing scheuling (lrate is constant), as every time the net starts getting closer to optimum, \n",
    "# we just make the problem harder (increasse the time lag) instead of trying to find the optimum more accurately\n",
    "# I tried using learing rate scheduling, without any noticable iprovement\n",
    "\n",
    "lrate = 5e-3\n",
    "copy_trainer.lrate.set_value(lrate)\n",
    "\n",
    "# Set weight decay, it seems to be helpful\n",
    "copy_trainer.wdec.set_value(1e-6)\n",
    "\n",
    "losses = []\n",
    "\n",
    "# Set the sequence parameters. We start from a trivial task and make it more difficult as we make progress.\n",
    "\n",
    "lag0 = 4\n",
    "lag = lag0\n",
    "seq_len0 = 1\n",
    "seq_len = seq_len0\n",
    "max_seq_len = 10 # Note that you may need more neurons for a longer sequence\n",
    "max_lag = 500\n",
    "max_it = 200000\n",
    "\n",
    "for i in xrange(max_it):\n",
    "    # lrate = \n",
    "    # copy_trainer.lrate.set_value(lrate)\n",
    "    \n",
    "    # We don't want the network to forget a solution for shorter sequences, so the sampled example is of a random size.\n",
    "    # This seems to be important for generalization.\n",
    "    this_lag = randint(lag0, lag + 1)\n",
    "    this_len = randint(seq_len0, seq_len + 1)\n",
    "    \n",
    "    # You can set the batch size here (or leave 32, which seems reasonable)\n",
    "    Xc, Yc = gen_copy_example(this_lag, this_len, 32)\n",
    "    ret = copy_trainer.train_function(Xc, Yc)\n",
    "    \n",
    "    # If the results are good enough, increase the length. \n",
    "    # For this case we also force the increase frome time to time, even if the results are not so great.\n",
    "    if lag * 1000 < i or this_lag > 0.9 * lag and this_len == seq_len and ret[0] < 6e-3:\n",
    "        lag += 1\n",
    "        seq_len = min(max_seq_len, lag / 10 + 1)\n",
    "        if lag > max_lag:\n",
    "            max_it = min(max_it, i + 10000)\n",
    "            lag = max_lag\n",
    "        print i, \"Increasing lag to: \", lag , \"seq_len is \", seq_len\n",
    "     \n",
    "    if i%2000 == 0:\n",
    "        print i, ret\n",
    "        Xc, Yc = gen_copy_example(lag, seq_len, 32)\n",
    "        ret = copy_test_function(Xc, Yc)\n",
    "        print ret\n",
    "        if i != 0: #ignore noisy start\n",
    "            losses.append((i,) + tuple([ret, lag]))\n",
    "        decoded_input = decode_matrix(Xc)\n",
    "        decoded_output = decode_matrix(copy_check_output(Xc).reshape(Xc.shape))\n",
    "        \n",
    "        # Sanity check\n",
    "        # Note that here we check the net's beahaviour on the max test, whereas most of training examples are shorter.\n",
    "        # We trust that the net outputs T + n '0's at the start, so we don't print it here for the transparency\n",
    "        print vstack([decoded_input[:,0].ravel()[:seq_len], decoded_output[:,0].ravel()[-seq_len:]])\n",
    "\n",
    "        \n",
    "# print charts\n",
    "\n",
    "losses_a = np.array(losses)\n",
    "\n",
    "p1 = plt.figure(1)\n",
    "\n",
    "legend(loc='lower right')\n",
    "title('Training loss')\n",
    "xlabel('iteration')\n",
    "\n",
    "plot(losses_a[:,0], losses_a[:,1], label='rms')\n",
    "\n",
    "savefig('losses_big.png')\n",
    "p1.show()\n",
    "\n",
    "#semilogy(losses_a[:,0], losses_a[:,2], label='rms + wdec')\n",
    "#semilogy(losses_a[:,0], losses_a[:,2], label='rms')\n",
    "#plot(losses_a[:,0], losses_a[:,3], label='grad norm')\n",
    "\n",
    "p2 = plt.figure(2)\n",
    "\n",
    "plot(losses_a[:,0], losses_a[:,2], label='lag')\n",
    "\n",
    "legend(loc='lower right')\n",
    "title('Lag')\n",
    "xlabel('iteration')\n",
    "\n",
    "savefig('lag_big.png')\n",
    "p2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the very rapid decrease in the loss as the lag gets bigger. This is mostly caused by the fact that the loss is the average loss over the whole genereted sequence, so the loss is sum_of_losses/length. The length increases, but sum_of_losses stays more or less the same, as the network is still rather confident that it should output '0's before seeing a '1' on the input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's wee what the trained network can do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0060438523069 0.335 0.0279166666667\n",
      "[[ 6.  5.  4.  3.  6.  2.  8.  3.  3.  6.]\n",
      " [ 6.  5.  4.  3.  6.  6.  8.  3.  2.  2.]]\n",
      "0.0057776873 0.315 0.0225\n",
      "[[ 3.  9.  8.  5.  3.  4.  8.  5.  9.  8.]\n",
      " [ 3.  9.  8.  5.  3.  4.  6.  5.  6.  6.]]\n",
      "0.00518716638908 0.285 0.0178125\n",
      "[[ 8.  8.  6.  9.  2.  7.  3.  2.  4.  2.]\n",
      " [ 8.  6.  6.  2.  2.  7.  3.  2.  2.  9.]]\n",
      "0.00465202238411 0.305 0.0169444444444\n",
      "[[ 5.  8.  2.  8.  4.  3.  2.  9.  2.  7.]\n",
      " [ 5.  8.  2.  2.  4.  3.  2.  2.  2.  2.]]\n",
      "0.0035308059305 0.315 0.01575\n",
      "[[ 7.  2.  9.  5.  9.  4.  6.  9.  4.  8.]\n",
      " [ 7.  2.  9.  5.  9.  4.  6.  6.  6.  6.]]\n",
      "0.00491541996598 0.34 0.0154545454545\n",
      "[[ 8.  9.  8.  9.  6.  6.  8.  4.  9.  7.]\n",
      " [ 8.  8.  9.  6.  6.  6.  6.  4.  6.  6.]]\n",
      "0.00341111049056 0.315 0.013125\n",
      "[[ 2.  4.  5.  4.  3.  8.  6.  4.  6.  7.]\n",
      " [ 2.  4.  5.  4.  3.  6.  4.  4.  4.  4.]]\n",
      "0.00281529827043 0.345 0.0132692307692\n",
      "[[ 9.  9.  3.  7.  9.  9.  6.  7.  3.  3.]\n",
      " [ 9.  9.  3.  7.  9.  9.  9.  3.  9.  9.]]\n",
      "0.00176230201032 0.33 0.0117857142857\n",
      "[[ 2.  3.  6.  4.  5.  8.  4.  5.  8.  3.]\n",
      " [ 2.  3.  6.  4.  5.  8.  4.  5.  8.  5.]]\n",
      "0.00307928258553 0.325 0.0108333333333\n",
      "[[ 5.  9.  3.  7.  5.  5.  8.  9.  6.  4.]\n",
      " [ 5.  9.  3.  7.  5.  5.  8.  8.  8.  8.]]\n",
      "0.00205141655169 0.31 0.0096875\n",
      "[[ 7.  2.  4.  6.  7.  8.  3.  2.  4.  2.]\n",
      " [ 7.  2.  4.  6.  7.  8.  3.  2.  2.  2.]]\n",
      "0.00214825873263 0.33 0.00970588235294\n",
      "[[ 6.  2.  6.  3.  4.  2.  7.  2.  2.  8.]\n",
      " [ 6.  2.  2.  3.  4.  2.  3.  2.  8.  2.]]\n",
      "0.00202569575049 0.325 0.00902777777778\n",
      "[[ 7.  8.  4.  7.  3.  2.  2.  6.  3.  8.]\n",
      " [ 7.  8.  4.  3.  3.  2.  2.  2.  2.  2.]]\n",
      "0.00187079736497 0.35 0.00921052631579\n",
      "[[ 9.  3.  5.  9.  5.  3.  6.  4.  9.  6.]\n",
      " [ 9.  3.  5.  9.  3.  3.  9.  4.  4.  6.]]\n",
      "0.00240117893554 0.32 0.008\n",
      "[[ 9.  8.  7.  2.  8.  7.  8.  9.  3.  3.]\n",
      " [ 9.  8.  7.  8.  8.  7.  8.  3.  3.  3.]]\n",
      "0.0028374704998 0.405 0.00964285714286\n",
      "[[ 5.  6.  5.  3.  5.  3.  4.  3.  6.  3.]\n",
      " [ 5.  6.  3.  3.  3.  6.  7.  9.  9.  9.]]\n",
      "0.00194809329696 0.365 0.00829545454545\n",
      "[[ 6.  6.  3.  9.  6.  2.  8.  6.  2.  3.]\n",
      " [ 6.  6.  3.  9.  6.  6.  6.  8.  6.  5.]]\n",
      "0.00199462706223 0.31 0.00673913043478\n",
      "[[ 9.  4.  2.  8.  4.  6.  6.  3.  8.  4.]\n",
      " [ 9.  4.  8.  8.  6.  6.  6.  3.  6.  7.]]\n",
      "0.00239314045757 0.335 0.00697916666667\n",
      "[[ 4.  9.  8.  5.  9.  5.  5.  7.  6.  3.]\n",
      " [ 4.  9.  9.  5.  8.  5.  5.  7.  9.  9.]]\n",
      "0.00160809326917 0.355 0.0071\n",
      "[[ 7.  3.  6.  7.  7.  7.  4.  3.  2.  2.]\n",
      " [ 7.  3.  6.  7.  7.  7.  4.  7.  7.  5.]]\n",
      "0.00166443455964 0.325 0.00625\n",
      "[[ 2.  4.  9.  7.  4.  8.  2.  3.  6.  7.]\n",
      " [ 2.  4.  9.  7.  4.  8.  8.  3.  3.  3.]]\n",
      "0.00124928948935 0.34 0.0062962962963\n",
      "[[ 9.  5.  3.  3.  9.  9.  4.  7.  4.  9.]\n",
      " [ 9.  5.  3.  3.  9.  9.  4.  5.  9.  9.]]\n",
      "0.00154812238179 0.365 0.00651785714286\n",
      "[[ 4.  3.  2.  4.  3.  9.  6.  8.  3.  2.]\n",
      " [ 4.  3.  2.  4.  3.  9.  6.  3.  6.  3.]]\n",
      "0.00166839477606 0.43 0.00741379310345\n",
      "[[ 6.  6.  6.  8.  6.  2.  7.  8.  4.  9.]\n",
      " [ 6.  6.  6.  8.  6.  4.  7.  6.  2.  5.]]\n",
      "0.00179041153751 0.335 0.00558333333333\n",
      "[[ 7.  8.  4.  9.  9.  2.  4.  9.  7.  2.]\n",
      " [ 7.  8.  4.  9.  9.  9.  9.  7.  9.  9.]]\n",
      "0.0013095473405 0.36 0.0058064516129\n",
      "[[ 7.  8.  9.  5.  8.  2.  5.  9.  5.  9.]\n",
      " [ 7.  8.  8.  5.  9.  9.  5.  9.  9.  9.]]\n",
      "0.00147399737034 0.38 0.0059375\n",
      "[[ 6.  7.  4.  6.  5.  2.  3.  4.  9.  5.]\n",
      " [ 6.  7.  4.  6.  5.  6.  3.  3.  3.  3.]]\n",
      "0.00082500296412 0.305 0.00462121212121\n",
      "[[ 7.  2.  7.  9.  8.  8.  6.  5.  9.  5.]\n",
      " [ 7.  2.  7.  9.  8.  8.  6.  5.  6.  6.]]\n",
      "0.000895734759979 0.37 0.00544117647059\n",
      "[[ 9.  2.  3.  6.  4.  8.  6.  7.  8.  2.]\n",
      " [ 9.  2.  3.  6.  4.  8.  8.  7.  8.  8.]]\n",
      "0.00158458668739 0.395 0.00564285714286\n",
      "[[ 9.  2.  8.  7.  7.  3.  3.  4.  9.  8.]\n",
      " [ 9.  2.  8.  7.  3.  3.  4.  7.  7.  5.]]\n",
      "0.00160119065549 0.345 0.00479166666667\n",
      "[[ 6.  6.  6.  2.  6.  4.  4.  7.  2.  6.]\n",
      " [ 6.  6.  2.  6.  2.  4.  6.  7.  5.  5.]]\n",
      "0.00130394636653 0.42 0.00567567567568\n",
      "[[ 6.  8.  4.  2.  3.  7.  5.  9.  5.  8.]\n",
      " [ 6.  8.  4.  4.  7.  3.  5.  5.  9.  9.]]\n",
      "0.00086808775086 0.44 0.00578947368421\n",
      "[[ 4.  2.  7.  9.  2.  8.  9.  5.  9.  4.]\n",
      " [ 4.  2.  7.  9.  2.  8.  8.  5.  9.  9.]]\n",
      "0.000860818079673 0.405 0.00519230769231\n",
      "[[ 3.  3.  3.  4.  5.  3.  8.  6.  6.  2.]\n",
      " [ 3.  3.  3.  4.  5.  3.  8.  6.  8.  8.]]\n",
      "0.00106030073948 0.42 0.00525\n",
      "[[ 7.  2.  4.  4.  4.  2.  3.  6.  3.  8.]\n",
      " [ 7.  2.  4.  2.  2.  4.  3.  6.  7.  8.]]\n",
      "0.00123832339887 0.42 0.00512195121951\n",
      "[[ 4.  4.  2.  5.  8.  2.  8.  7.  6.  4.]\n",
      " [ 4.  4.  2.  5.  8.  8.  8.  5.  8.  8.]]\n",
      "0.00152778625488 0.38 0.00452380952381\n",
      "[[ 8.  3.  4.  4.  4.  8.  3.  7.  4.  2.]\n",
      " [ 8.  3.  4.  4.  4.  7.  4.  7.  7.  7.]]\n",
      "0.00103022577241 0.35 0.00406976744186\n",
      "[[ 7.  8.  9.  9.  4.  9.  4.  8.  9.  4.]\n",
      " [ 7.  8.  9.  9.  4.  9.  9.  7.  7.  6.]]\n",
      "0.000851804099511 0.44 0.005\n",
      "[[ 4.  4.  9.  4.  4.  5.  3.  5.  8.  6.]\n",
      " [ 4.  4.  9.  4.  4.  5.  7.  5.  4.  4.]]\n",
      "0.000988906132989 0.405 0.0045\n",
      "[[ 5.  2.  5.  8.  7.  5.  3.  4.  2.  8.]\n",
      " [ 5.  2.  5.  8.  7.  3.  5.  8.  8.  8.]]\n",
      "0.00103474524803 0.405 0.00440217391304\n",
      "[[ 7.  6.  8.  7.  5.  8.  5.  2.  8.  3.]\n",
      " [ 7.  6.  8.  7.  5.  2.  2.  5.  5.  5.]]\n",
      "0.000902103434782 0.395 0.00420212765957\n",
      "[[ 8.  2.  4.  5.  4.  3.  6.  2.  9.  6.]\n",
      " [ 8.  2.  4.  5.  4.  3.  6.  3.  6.  6.]]\n",
      "0.00127417000476 0.41 0.00427083333333\n",
      "[[ 5.  4.  5.  3.  9.  8.  9.  8.  3.  8.]\n",
      " [ 5.  4.  3.  9.  9.  8.  9.  3.  9.  9.]]\n",
      "0.00223385845311 0.45 0.00459183673469\n",
      "[[ 7.  7.  7.  7.  9.  4.  3.  2.  3.  4.]\n",
      " [ 7.  7.  7.  9.  3.  2.  4.  7.  9.  5.]]\n",
      "0.00090799323516 0.41 0.0041\n",
      "[[ 2.  2.  5.  2.  9.  9.  3.  5.  2.  6.]\n",
      " [ 2.  2.  5.  2.  9.  3.  3.  3.  3.  6.]]\n",
      "0.00101846281905 0.435 0.00426470588235\n",
      "[[ 7.  5.  2.  4.  9.  9.  9.  4.  7.  4.]\n",
      " [ 7.  5.  5.  4.  9.  9.  4.  7.  5.  6.]]\n",
      "0.00181730068289 0.435 0.00418269230769\n",
      "[[ 7.  5.  2.  9.  8.  6.  5.  2.  6.  2.]\n",
      " [ 7.  5.  9.  9.  6.  6.  6.  5.  5.  6.]]\n",
      "0.000581270433031 0.41 0.0038679245283\n",
      "[[ 4.  5.  5.  4.  6.  9.  6.  6.  7.  8.]\n",
      " [ 4.  5.  5.  4.  6.  9.  6.  6.  6.  7.]]\n",
      "0.000938150740694 0.475 0.00439814814815\n",
      "[[ 2.  4.  7.  2.  6.  5.  4.  7.  7.  5.]\n",
      " [ 2.  4.  7.  6.  4.  5.  6.  5.  6.  6.]]\n",
      "0.00131721107755 0.435 0.00395454545455\n",
      "[[ 5.  4.  9.  7.  7.  3.  8.  5.  2.  2.]\n",
      " [ 5.  4.  9.  5.  3.  6.  3.  8.  8.  8.]]\n",
      "0.00105261907447 0.5 0.00446428571429\n",
      "[[ 3.  6.  4.  9.  2.  7.  4.  9.  2.  6.]\n",
      " [ 3.  2.  4.  9.  7.  7.  9.  4.  7.  4.]]\n",
      "0.00150310317986 0.49 0.00429824561404\n",
      "[[ 4.  6.  8.  2.  6.  5.  7.  8.  9.  4.]\n",
      " [ 4.  6.  8.  6.  5.  5.  4.  8.  5.  7.]]\n",
      "0.00108743808232 0.455 0.0039224137931\n",
      "[[ 3.  4.  7.  4.  8.  5.  5.  5.  5.  8.]\n",
      " [ 3.  4.  7.  2.  9.  5.  3.  8.  7.  5.]]\n",
      "0.00153141724877 0.475 0.00402542372881\n",
      "[[ 4.  7.  8.  2.  3.  6.  9.  6.  2.  4.]\n",
      " [ 4.  7.  7.  6.  6.  3.  6.  3.  3.  9.]]\n",
      "0.00109721231274"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/pio/os/anaconda/lib/python2.7/site-packages/matplotlib/axes/_axes.py:475: UserWarning: No labelled objects found. Use label='...' kwarg on individual plots.\n",
      "  warnings.warn(\"No labelled objects found. \"\n",
      "/pio/os/anaconda/lib/python2.7/site-packages/matplotlib/figure.py:387: UserWarning: matplotlib is currently using a non-GUI backend, so cannot show the figure\n",
      "  \"matplotlib is currently using a non-GUI backend, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0.46 0.00383333333333\n",
      "[[ 7.  3.  6.  4.  4.  6.  6.  7.  7.  3.]\n",
      " [ 7.  3.  6.  4.  6.  6.  3.  6.  8.  7.]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEZCAYAAACaWyIJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nMSEwDMOoc0wIDMMw6hwTAsMwjDrn/wPJ6pqvW+0T8wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f137d68cf50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "seq_len = 10\n",
    "\n",
    "err_rates = []\n",
    "\n",
    "for lag in range(100, 1200, 20):\n",
    "    err = 0\n",
    "    err_all = 0\n",
    "    for i in range(20):\n",
    "        Xc, Yc = gen_copy_example(lag, seq_len, 1)\n",
    "        ret = copy_test_function(Xc, Yc)\n",
    "\n",
    "        decoded_input = decode_matrix(Xc)\n",
    "        decoded_output = decode_matrix(copy_check_output(Xc).reshape(Xc.shape))\n",
    "\n",
    "        err += (decoded_input.ravel()[:seq_len] != decoded_output.ravel()[-seq_len:]).mean()\n",
    "        err_all += (np.hstack([np.zeros(seq_len + lag), decoded_input.ravel()[:seq_len] ]) != decoded_output.ravel()).mean()\n",
    "    \n",
    "    err /= 20\n",
    "    err_all /= 20\n",
    "    err_rates.append((lag,) + tuple([ret, err, err_all]))\n",
    "\n",
    "    print ret, err, err_all\n",
    "    print vstack([decoded_input.ravel()[:seq_len], decoded_output.ravel()[-seq_len:]])\n",
    "    #print hstack([decoded_input, decoded_output])\n",
    "    \n",
    "    \n",
    "    \n",
    "errors_a = np.array(err_rates)\n",
    "\n",
    "p1 = plt.figure(3)\n",
    "\n",
    "legend(loc='lower right')\n",
    "title('Error rates on sampled inputs')\n",
    "xlabel('Time lag')\n",
    "\n",
    "plot(errors_a[:,0], errors_a[:,2], label='err_rate')\n",
    "\n",
    "savefig('errors_big.png')\n",
    "p1.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
